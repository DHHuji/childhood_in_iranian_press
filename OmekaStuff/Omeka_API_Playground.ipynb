{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c358223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persianpress.dh.huji.ac.il/issues api keys\n",
    "\n",
    "KEY_IDENTITY = \"1234578890abcede\"  # Replace with your key identity\n",
    "KEY_CREDENTIAL = \"987654321polika\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd19e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see that you can connect to the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36370cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://persianpress.dh.huji.ac.il/issues/api\"\n",
    "\n",
    "headers = {\n",
    "        'Authorization': f'key_identity={KEY_IDENTITY},key_credential={KEY_CREDENTIAL}'\n",
    "    }\n",
    "\n",
    "response = requests.get(API_URL, headers=headers, verify=False)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully connected to the API.\")\n",
    "    if response.text:\n",
    "        print(\"Raw response:\", response.text)\n",
    "\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://persianpress.dh.huji.ac.il/issues/api/resource_templates\"\n",
    "\n",
    "headers = {\n",
    "        'Authorization': f'key_identity={KEY_IDENTITY},key_credential={KEY_CREDENTIAL}'\n",
    "    }\n",
    "\n",
    "response = requests.get(API_URL, headers=headers, verify=False)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully connected to the API.\")\n",
    "    if response.text:\n",
    "        print(\"Raw response:\", response.text)\n",
    "\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://persianpress.dh.huji.ac.il/issues/api/items/1954\"\n",
    "\n",
    "headers = {\n",
    "        'Authorization': f'key_identity={KEY_IDENTITY},key_credential={KEY_CREDENTIAL}'\n",
    "    }\n",
    "\n",
    "response = requests.get(API_URL, headers=headers, verify=False)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully connected to the API.\")\n",
    "    if response.text:\n",
    "        print(\"Raw response:\", response.text)\n",
    "\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = response.json()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['dcterms:identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new \"page\" item with jpg and xml as the media, and use metadata to fill in properties\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import requests \n",
    "\n",
    "# Sample complex path with folder structure\n",
    "jpg_path = Path(r\"C:\\Users\\User\\PycharmProjects\\PersianChildhood\\Journals Data\\Ḥabl al-matı̄n\\1. Jahrgang (1325-1326hq -- 829hš -- 1907-1908)\\2. Ausgabe (16. Rabīʿ al-awwal 1325 -- 10. Ordibehešt 829 -- 30. April 1907)\\3.jpg\")\n",
    "xml_path = \"3.xml\"\n",
    "\n",
    "API_URL = \"https://persianpress.dh.huji.ac.il/issues/api/items\"\n",
    "#API_URL = \"https://persianpress.dh.huji.ac.il/issues/api\"\n",
    "\n",
    "# Function to extract metadata from the full folder structure\n",
    "def extract_metadata_from_path(file_path: Path):\n",
    "    parts = file_path.parts\n",
    "\n",
    "    # Attempt to extract title and issue from folder names\n",
    "    try:\n",
    "        journal_name = parts[-3]  # e.g., 'Ḥabl al-matı̄n'\n",
    "        issue_folder = parts[-2]  # e.g., '2. Ausgabe (...)'\n",
    "        page_number = file_path.stem  # e.g., '2'\n",
    "\n",
    "        # Extract issue number and label\n",
    "        issue_match = re.match(r\"(\\d+)\\. Ausgabe.*\", issue_folder)\n",
    "        issue_number = issue_match.group(1) if issue_match else \"?\"\n",
    "\n",
    "        return {\n",
    "            \"title\": f\"{journal_name} - Issue {issue_number} - Page {page_number}\",\n",
    "            \"identifier\": f\"Issue_{issue_number}_Page_{page_number}\",\n",
    "            \"issue\": issue_folder,\n",
    "            \"page\": page_number\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"title\": \"Untitled\",\n",
    "            \"identifier\": \"unknown\",\n",
    "            \"issue\": \"unknown\",\n",
    "            \"page\": \"0\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Extract text content from XML file\n",
    "def extract_text_from_xml_safe(xml_file):\n",
    "    xml_file = Path(xml_file)  # ⬅ This line converts string to Path\n",
    "    if xml_file.exists():\n",
    "        try:\n",
    "            return xml_file.read_text(encoding=\"utf-8\")\n",
    "        except Exception as e:\n",
    "            return f\"[Error reading XML: {str(e)}]\"\n",
    "    return \"[No XML file found]\"\n",
    "\n",
    "\n",
    "# Extract metadata\n",
    "metadata = extract_metadata_from_path(jpg_path)\n",
    "extracted_text = extract_text_from_xml_safe(xml_path)\n",
    "\n",
    "# Build base JSON structure\n",
    "item_data = {\n",
    "    \"o:resource_template\": {\"o:id\": 2},\n",
    "    \"o:resource_class\": {\"o:id\": 50},\n",
    "    \"o:is_public\": True,\n",
    "    \"dcterms:title\": [\n",
    "        {\"type\": \"literal\", \"property_id\": 1, \"property_label\": \"Title\", \"is_public\": True, \"@value\": metadata[\"title\"]}\n",
    "    ],\n",
    "    \"dcterms:identifier\": [\n",
    "        {\"type\": \"literal\", \"property_id\": 10, \"property_label\": \"Identifier\", \"is_public\": True, \"@value\": metadata[\"identifier\"]}\n",
    "    ],\n",
    "    \"foaf:page\": [\n",
    "        {\"type\": \"literal\", \"property_id\": 174, \"property_label\": \"page\", \"is_public\": True, \"@value\": metadata[\"page\"]}\n",
    "    ],\n",
    "    \"bibo:issue\": [\n",
    "        {\"type\": \"literal\", \"property_id\": 103, \"property_label\": \"issue\", \"is_public\": True, \"@value\": metadata[\"issue\"]}\n",
    "    ],\n",
    "    \"dcterms:description\": [\n",
    "        {\"type\": \"literal\", \"property_id\": 4, \"property_label\": \"Description\", \"is_public\": True, \"@value\": \"Created automatically via Omeka API\"}\n",
    "    ],\n",
    "#     \"extracttext:extracted_text\": [\n",
    "#         {\"type\": \"literal\", \"property_id\": 185, \"property_label\": \"extracted text\", \"is_public\": True, \"@value\": extracted_text}\n",
    "#     ],\n",
    "    \"o:media\": [\n",
    "        {\n",
    "            \"o:ingester\": \"upload\",\n",
    "            \"file_index\": 0,\n",
    "            \"dcterms:title\": [{\"@value\": \"Scanned Page\"}]\n",
    "        },\n",
    "        {\n",
    "            \"o:ingester\": \"upload\",\n",
    "            \"file_index\": 1,\n",
    "            \"dcterms:title\": [{\"@value\": \"XML OCR\"}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display JSON structure\n",
    "# json.dumps(item_data, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- PREPARE MULTIPART FORM DATA ---\n",
    "files = {\n",
    "    'data': (None, json.dumps(item_data), 'application/json'),\n",
    "    'file[0]': (Path(jpg_path).name, open(jpg_path, 'rb'), 'image/jpeg'),\n",
    "    'file[1]': (Path(xml_path).name, open(xml_path, 'rb'), 'text/xml'),\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'key_identity': KEY_IDENTITY,\n",
    "    'key_credential': KEY_CREDENTIAL\n",
    "}\n",
    "\n",
    "# --- SEND REQUEST ---\n",
    "response = requests.post(API_URL, files=files, params=params, verify=False)\n",
    "\n",
    "# --- HANDLE RESPONSE ---\n",
    "if response.status_code in [200, 201]:\n",
    "    print(\"✅ Item created successfully!\")\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "else:\n",
    "    print(f\"❌ Error {response.status_code}:\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d77fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass pipeline attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is currently the working version\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- API Info ---\n",
    "API_BASE = \"https://persianpress.dh.huji.ac.il/issues/api\"\n",
    "\n",
    "params = {\n",
    "    \"key_identity\": KEY_IDENTITY,\n",
    "    \"key_credential\": KEY_CREDENTIAL\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# --- Template IDs ---\n",
    "TEMPLATE_JOURNAL = 4\n",
    "TEMPLATE_ISSUE = 3\n",
    "TEMPLATE_PAGE = 2\n",
    "\n",
    "# --- Class IDs ---\n",
    "CLASS_JOURNAL = 61  # Bibliographic Resource\n",
    "CLASS_ISSUE = 60    # Periodical Issue\n",
    "CLASS_PAGE = 50     # Page\n",
    "\n",
    "# --- Property IDs ---\n",
    "PROPERTY_ID_TITLE = 1         # dcterms:title\n",
    "PROPERTY_ID_IDENTIFIER = 10   # dcterms:identifier\n",
    "PROPERTY_ID_DATE = 7          # dcterms:date\n",
    "PROPERTY_ID_IS_PART_OF = 33   # dcterms:isPartOf\n",
    "\n",
    "# === Load Metadata ===\n",
    "metadf = pd.read_excel(\"Persian Literature Metadata.xlsx\")\n",
    "# Sample filter for testing:\n",
    "test_df = metadf[metadf[\"journal\"] == \"Ḥabl al-matı̄n\"]\n",
    "\n",
    "# === Utility: check if item exists by identifier ===\n",
    "def get_resource_by_identifier(identifier):\n",
    "    \"\"\"\n",
    "    Return the first Omeka item ID that has dcterms:identifier == identifier,\n",
    "    or None if not found.\n",
    "    \"\"\"\n",
    "    search_params = {\n",
    "        \"key_identity\": KEY_IDENTITY,\n",
    "        \"key_credential\": KEY_CREDENTIAL,\n",
    "        \"property[0][property]\": \"dcterms:identifier\",\n",
    "        \"property[0][type]\": \"eq\",\n",
    "        \"property[0][text]\": identifier\n",
    "    }\n",
    "    r = requests.get(f\"{API_BASE}/items\", params=search_params, verify=False)\n",
    "    if r.status_code == 200:\n",
    "        results = r.json()\n",
    "        if results:\n",
    "            found_id = results[0][\"o:id\"]\n",
    "            print(f\"   Found existing item [ID={found_id}] for identifier='{identifier}'\")\n",
    "            return found_id\n",
    "    return None\n",
    "\n",
    "# === Utility: read in the .xml OCR content if present ===\n",
    "def read_xml_content(xml_path: Path):\n",
    "    try:\n",
    "        return xml_path.read_text(encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        return f\"[XML read error: {str(e)}]\"\n",
    "\n",
    "# === Utility: optionally parse a date from the issue label ===\n",
    "def extract_date_from_issue_label(issue_label):\n",
    "    \"\"\"\n",
    "    Example: parse something like '7. Rabīʿ 1326' from the issue string.\n",
    "    Adjust or remove if not matching your data.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d{1,2}\\.\\s*\\w+\\s+\\d{4})', issue_label)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# --- Main Loop ---\n",
    "for idx, row in test_df.iterrows():\n",
    "    uid = str(row[\"unique_id\"])\n",
    "    journal = str(row[\"journal\"])  # e.g. \"Ḥabl al-matı̄n\"\n",
    "    volume = str(row[\"volume\"])\n",
    "    issue = str(row[\"issue\"])\n",
    "    page = str(row[\"page\"])\n",
    "\n",
    "    print(\"\\n----------------------------------------------------\")\n",
    "    print(f\"Processing row={idx}: Journal={journal}, Issue={issue}, Page={page}\")\n",
    "\n",
    "    # --- Build stable identifiers\n",
    "    journal_identifier = journal                  # Must match EXACTLY\n",
    "    issue_identifier = f\"{journal} - Issue {issue}\"\n",
    "    page_identifier = uid\n",
    "    \n",
    "    # skip creating a page if it already exists\n",
    "    existing_page_id = get_resource_by_identifier(page_identifier)\n",
    "    if existing_page_id:\n",
    "        print(f\"   Page '{page_identifier}' already exists, ID={existing_page_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1) Ensure the JOURNAL exists (attempt to find it first)\n",
    "    # -----------------------------------------------------------\n",
    "    print(f\"Checking if Journal '{journal}' already exists by identifier='{journal_identifier}'\")\n",
    "    journal_id = get_resource_by_identifier(journal_identifier)\n",
    "    if journal_id:\n",
    "        print(f\"   ✅ Re-using existing Journal, ID={journal_id}\")\n",
    "    else:\n",
    "        print(f\"   ❌ Not found. Creating new Journal => '{journal}'\")\n",
    "        # Exactly your snippet's approach:\n",
    "        journal_payload = {\n",
    "            \"o:resource_template\": {\"o:id\": TEMPLATE_JOURNAL},\n",
    "            \"o:resource_class\": {\"o:id\": CLASS_JOURNAL},\n",
    "            \"o:is_public\": True,\n",
    "            \"dcterms:title\": [{\n",
    "                \"type\": \"literal\",\n",
    "                \"property_id\": PROPERTY_ID_TITLE,\n",
    "                \"@value\": journal\n",
    "            }],\n",
    "            \"dcterms:identifier\": [{\n",
    "                \"type\": \"literal\",\n",
    "                \"property_id\": PROPERTY_ID_IDENTIFIER,\n",
    "                \"@value\": journal_identifier\n",
    "            }]\n",
    "        }\n",
    "        journal_response = requests.post(\n",
    "            f\"{API_BASE}/items\",\n",
    "            params=params,\n",
    "            headers=headers,\n",
    "            json=journal_payload,\n",
    "            verify=False\n",
    "        )\n",
    "        # if it worked we'll get a new id\n",
    "        if journal_response.status_code in [200, 201]:\n",
    "            journal_id = journal_response.json()[\"o:id\"]\n",
    "            print(f\"   ✅ Created Journal [ID={journal_id}]\")\n",
    "        else:\n",
    "            print(f\"   ❌ Failed to create journal: {journal_response.status_code}\")\n",
    "            print(journal_response.text)\n",
    "            journal_id = None\n",
    "\n",
    "    if not journal_id:\n",
    "        print(\"   Stopping here — cannot proceed without a valid journal.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2) Ensure the ISSUE exists (attempt to find it first)\n",
    "    # -----------------------------------------------------------\n",
    "    print(f\"Checking if Issue '{issue}' already exists by identifier='{issue_identifier}'\")\n",
    "    issue_id = get_resource_by_identifier(issue_identifier)\n",
    "    if issue_id:\n",
    "        print(f\"   ✅ Re-using existing Issue, ID={issue_id}\")\n",
    "    else:\n",
    "        print(f\"   ❌ Not found. Creating new Issue => '{issue}'\")\n",
    "        # We can parse a date from the issue label if desired:\n",
    "        date_val = extract_date_from_issue_label(issue) or \"1900-01-01\"\n",
    "\n",
    "        issue_payload = {\n",
    "            \"o:resource_template\": {\"o:id\": TEMPLATE_ISSUE},\n",
    "            \"o:resource_class\": {\"o:id\": CLASS_ISSUE},\n",
    "            \"o:is_public\": True,\n",
    "            \"dcterms:title\": [{\n",
    "                \"type\": \"literal\",\n",
    "                \"property_id\": PROPERTY_ID_TITLE,\n",
    "                \"@value\": issue\n",
    "            }],\n",
    "            \"dcterms:identifier\": [{\n",
    "                \"type\": \"literal\",\n",
    "                \"property_id\": PROPERTY_ID_IDENTIFIER,\n",
    "                \"@value\": issue_identifier\n",
    "            }],\n",
    "            \"dcterms:date\": [{\n",
    "                \"type\": \"literal\",\n",
    "                \"property_id\": PROPERTY_ID_DATE,\n",
    "                \"@value\": date_val\n",
    "            }],\n",
    "            \"dcterms:isPartOf\": [{\n",
    "                \"type\": \"resource\",\n",
    "                \"property_id\": PROPERTY_ID_IS_PART_OF,\n",
    "                \"value_resource_id\": journal_id\n",
    "            }]\n",
    "        }\n",
    "\n",
    "        issue_response = requests.post(\n",
    "            f\"{API_BASE}/items\",\n",
    "            params=params,\n",
    "            headers=headers,\n",
    "            json=issue_payload,\n",
    "            verify=False\n",
    "        )\n",
    "        if issue_response.status_code in [200, 201]:\n",
    "            issue_id = issue_response.json()[\"o:id\"]\n",
    "            print(f\"   ✅ Created Issue [ID={issue_id}]\")\n",
    "        else:\n",
    "            print(f\"   ❌ Failed to create issue: {issue_response.status_code}\")\n",
    "            print(issue_response.text)\n",
    "            issue_id = None\n",
    "\n",
    "    if not issue_id:\n",
    "        print(\"   Stopping here — cannot proceed without a valid issue.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3) Create or Update the PAGE\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    jpg_path = Path(f\"./Journals Data/{journal}/{volume}/{issue}/{page}.jpg\")\n",
    "    xml_path = Path(f\"./XMLs/Habl_1_jahrgang_pagexml/{uid}.xml\")\n",
    "    extracted_text = read_xml_content(xml_path)\n",
    "\n",
    "    page_data = {\n",
    "        # Resource Template & Class\n",
    "        \"o:resource_template\": {\"o:id\": TEMPLATE_PAGE},\n",
    "        \"o:resource_class\": {\"o:id\": CLASS_PAGE},\n",
    "        \"o:is_public\": True,\n",
    "\n",
    "        # dcterms:title for the Page\n",
    "        \"dcterms:title\": [{\n",
    "            \"type\": \"literal\",\n",
    "            \"property_id\": PROPERTY_ID_TITLE,\n",
    "            \"@value\": f\"{journal} - Issue {issue} - Page {page}\"\n",
    "        }],\n",
    "        \"dcterms:identifier\": [{\n",
    "            \"type\": \"literal\",\n",
    "            \"property_id\": PROPERTY_ID_IDENTIFIER,\n",
    "            \"@value\": page_identifier\n",
    "        }],\n",
    "\n",
    "        # Example additional properties\n",
    "        \"dcterms:isPartOf\": [{\n",
    "            \"type\": \"resource\",\n",
    "            \"property_id\": PROPERTY_ID_IS_PART_OF,\n",
    "            \"value_resource_id\": issue_id\n",
    "        }],\n",
    "\n",
    "        # If you store the page number, the issue name, or the extracted text\n",
    "        \"foaf:page\": [{\n",
    "            \"type\": \"literal\",\n",
    "            \"property_id\": 174,\n",
    "            \"@value\": page\n",
    "        }],\n",
    "        \"bibo:issue\": [{\n",
    "            \"type\": \"literal\",\n",
    "            \"property_id\": 103,\n",
    "            \"@value\": issue\n",
    "        }],\n",
    "        \"extracttext:extracted_text\": [{\n",
    "            \"type\": \"literal\",\n",
    "            \"property_id\": 185,  \n",
    "            \"@value\": extracted_text\n",
    "        }],\n",
    "\n",
    "        # Attach the scanned image + OCR file\n",
    "        \"o:media\": [\n",
    "            {\"o:ingester\": \"upload\", \"file_index\": 0, \"dcterms:title\": [{\"@value\": \"Scan\"}]},\n",
    "            {\"o:ingester\": \"upload\", \"file_index\": 1, \"dcterms:title\": [{\"@value\": \"OCR\"}]}\n",
    "        ],\n",
    "\n",
    "        # Just a note\n",
    "        \"dcterms:description\": [{\n",
    "            \"type\": \"literal\",\n",
    "            \"property_id\": 4,\n",
    "            \"@value\": \"Created automatically via Omeka API\"\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    # POST with multipart form-data to upload files\n",
    "    files = {\n",
    "        \"data\": (None, json.dumps(page_data), \"application/json\"),\n",
    "        \"file[0]\": (jpg_path.name, open(jpg_path, \"rb\"), \"image/jpeg\"),\n",
    "        \"file[1]\": (xml_path.name, open(xml_path, \"rb\"), \"application/xml\")\n",
    "    }\n",
    "\n",
    "    print(f\"Creating Page => '{page_identifier}'\")\n",
    "    page_response = requests.post(\n",
    "        f\"{API_BASE}/items\",\n",
    "        files=files,\n",
    "        params=params,\n",
    "        verify=False\n",
    "    )\n",
    "\n",
    "    if page_response.status_code in [200, 201]:\n",
    "        new_page_id = page_response.json()[\"o:id\"]\n",
    "        print(f\"   ✅ Created Page [ID={new_page_id}] for {page_identifier}\")\n",
    "    else:\n",
    "        print(f\"   ❌ Failed to create page {page_identifier}: {page_response.status_code}\")\n",
    "        print(page_response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link journals to issues and issues to pages via HasPart\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# ===== CONFIGURE YOUR API & AUTH =====\n",
    "API_BASE = \"https://persianpress.dh.huji.ac.il/issues/api\"\n",
    "\n",
    "PARAMS  = {\"key_identity\": KEY_IDENTITY, \"key_credential\": KEY_CREDENTIAL}\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# ===== RESOURCE CLASSES & PROPERTY IDS (adjust if yours differ) =====\n",
    "CLASS_JOURNAL = 61  # e.g. \"Bibliographic Resource\"\n",
    "CLASS_ISSUE   = 60  # e.g. \"Periodical Issue\"\n",
    "CLASS_PAGE    = 50  # e.g. \"Page\"\n",
    "\n",
    "PROPERTY_ID_IS_PART_OF = 33  # dcterms:isPartOf\n",
    "PROPERTY_ID_HAS_PART   = 34  # dcterms:hasPart\n",
    "\n",
    "def get_items_by_resource_class(class_id):\n",
    "    \"\"\"\n",
    "    Returns a list of item dicts for all items \n",
    "    that have the given resource_class_id.\n",
    "    For large sites, you'd implement pagination or multiple calls.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/items\"\n",
    "    params = {\n",
    "        \"resource_class_id\": class_id,\n",
    "        \"per_page\": 1000,  # or implement pagination if needed\n",
    "        **PARAMS\n",
    "    }\n",
    "    resp = requests.get(url, params=params, verify=False)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"❌ Error retrieving items for class_id={class_id}: {resp.status_code} {resp.text}\")\n",
    "        return []\n",
    "\n",
    "def get_children_by_isPartOf(parent_id):\n",
    "    \"\"\"\n",
    "    Returns all items for which 'dcterms:isPartOf' = <parent_id>.\n",
    "    That means these items are children of parent_id.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/items\"\n",
    "    params = {\n",
    "        \"property[0][property]\": \"dcterms:isPartOf\",\n",
    "        \"property[0][type]\": \"res\",  # 'res' means resource reference\n",
    "        \"property[0][text]\": parent_id,\n",
    "        \"per_page\": 1000,\n",
    "        **PARAMS\n",
    "    }\n",
    "    r = requests.get(url, params=params, verify=False)\n",
    "    if r.status_code == 200:\n",
    "        return r.json()\n",
    "    else:\n",
    "        print(f\"❌ Error retrieving children for parent={parent_id}: {r.status_code} {r.text}\")\n",
    "        return []\n",
    "\n",
    "def get_full_item_data(item_id):\n",
    "    \"\"\"\n",
    "    GET the full JSON for the item, so we can re-PUT or re-PATCH\n",
    "    without losing other metadata (titles, identifiers, etc.).\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/items/{item_id}\"\n",
    "    resp = requests.get(url, params=PARAMS, verify=False)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"❌ Error retrieving item_id={item_id}: {resp.status_code} {resp.text}\")\n",
    "        return None\n",
    "\n",
    "def put_item_data(item_id, item_json):\n",
    "    \"\"\"\n",
    "    PUT the entire JSON to Omeka S for item_id.\n",
    "    This overwrites the existing item but preserves \n",
    "    all fields we leave in item_json.\n",
    "    We must remove read-only fields before PUT.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/items/{item_id}\"\n",
    "    # Remove read-only or unneeded fields\n",
    "    for field in [\n",
    "        \"o:id\", \"o:created\", \"o:modified\", \"o:owner\",\n",
    "        \"o:resource_type\", \"o:thumbnail\", \"o:created\",\n",
    "        \"o:media\", \"o:link\", \"o:item_set\", \"o:is_public\", \n",
    "        \"o:modules\", \"o:site\", \"o:request\", \"o:module\",\n",
    "        \"@type\", \"@context\", \"@id\",\n",
    "    ]:\n",
    "        item_json.pop(field, None)\n",
    "\n",
    "    resp = requests.put(url, params=PARAMS, headers=HEADERS, json=item_json, verify=False)\n",
    "    if resp.status_code in [200, 201]:\n",
    "        print(f\"   ✅ PUT item_id={item_id} succeeded.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"   ❌ PUT item_id={item_id} failed: {resp.status_code} {resp.text}\")\n",
    "        return False\n",
    "\n",
    "def safely_append_hasPart(parent_id, child_id):\n",
    "    \"\"\"\n",
    "    1) GET the full item JSON for parent_id\n",
    "    2) read existing 'dcterms:hasPart'\n",
    "    3) if child_id not in there, append it\n",
    "    4) PUT the entire updated item\n",
    "    \"\"\"\n",
    "    parent_data = get_full_item_data(parent_id)\n",
    "    if not parent_data:\n",
    "        print(f\"   ❌ Could not retrieve parent [ID={parent_id}] to patch hasPart.\")\n",
    "        return\n",
    "\n",
    "    # 1) read existing dcterms:hasPart\n",
    "    if \"dcterms:hasPart\" not in parent_data:\n",
    "        parent_data[\"dcterms:hasPart\"] = []\n",
    "\n",
    "    # 'dcterms:hasPart' is typically a list of objects, e.g.:\n",
    "    # [\n",
    "    #   {\n",
    "    #       \"type\": \"resource\",\n",
    "    #       \"property_id\": 34,\n",
    "    #       \"value_resource_id\": 123\n",
    "    #   },\n",
    "    #   ...\n",
    "    # ]\n",
    "    has_parts = parent_data[\"dcterms:hasPart\"]\n",
    "\n",
    "    # 2) Check if child is already in hasPart\n",
    "    already_linked = any(\n",
    "        (hp.get(\"value_resource_id\") == child_id)\n",
    "        for hp in has_parts\n",
    "    )\n",
    "    if already_linked:\n",
    "        print(f\"   (Skipping) Parent={parent_id} already hasPart => Child={child_id}\")\n",
    "        return\n",
    "\n",
    "    # 3) If not linked, append a new entry\n",
    "    new_entry = {\n",
    "        \"type\": \"resource\",\n",
    "        \"property_id\": PROPERTY_ID_HAS_PART,\n",
    "        \"value_resource_id\": child_id\n",
    "    }\n",
    "    has_parts.append(new_entry)\n",
    "\n",
    "    # 4) PUT the entire updated item back to Omeka\n",
    "    success = put_item_data(parent_id, parent_data)\n",
    "    if success:\n",
    "        print(f\"   🔗 Parent [ID={parent_id}] => hasPart => [ID={child_id}] (APPENDED)\")\n",
    "\n",
    "def main():\n",
    "    # ====================================\n",
    "    # 1) For each Journal, find its Issues\n",
    "    # ====================================\n",
    "    print(\"=== Linking Journals => Issues (Has Part) ===\")\n",
    "    journals = get_items_by_resource_class(CLASS_JOURNAL)\n",
    "    print(f\"Retrieved {len(journals)} Journals (class_id={CLASS_JOURNAL}).\")\n",
    "\n",
    "    for j_data in journals:\n",
    "        journal_id = j_data[\"o:id\"]\n",
    "        print(f\"\\n--- Journal [ID={journal_id}] ---\")\n",
    "        # find child items that have isPartOf = this journal\n",
    "        journal_children = get_children_by_isPartOf(journal_id)\n",
    "        print(f\"   Found {len(journal_children)} items referencing this Journal via isPartOf.\")\n",
    "        for child_item in journal_children:\n",
    "            child_id = child_item[\"o:id\"]\n",
    "            # Append to \"hasPart\" if not present\n",
    "            safely_append_hasPart(journal_id, child_id)\n",
    "\n",
    "    # ==================================\n",
    "    # 2) For each Issue, find its Pages\n",
    "    # ==================================\n",
    "    print(\"\\n=== Linking Issues => Pages (Has Part) ===\")\n",
    "    issues = get_items_by_resource_class(CLASS_ISSUE)\n",
    "    print(f\"Retrieved {len(issues)} Issues (class_id={CLASS_ISSUE}).\")\n",
    "\n",
    "    for iss_data in issues:\n",
    "        issue_id = iss_data[\"o:id\"]\n",
    "        print(f\"\\n--- Issue [ID={issue_id}] ---\")\n",
    "        # find child items that have isPartOf = this issue\n",
    "        children = get_children_by_isPartOf(issue_id)\n",
    "        print(f\"   Found {len(children)} items referencing this Issue via isPartOf.\")\n",
    "        for child_item in children:\n",
    "            child_id = child_item[\"o:id\"]\n",
    "            safely_append_hasPart(issue_id, child_id)\n",
    "\n",
    "    print(\"\\nAll done! Your Journals should now have 'Has Part' for their Issues, \"\n",
    "          \"and Issues should have 'Has Part' for their Pages, without losing titles.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b356c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#associate site\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# ===== CONFIGURE YOUR API & AUTH =====\n",
    "API_BASE = \"https://persianpress.dh.huji.ac.il/issues/api\"\n",
    "KEY_IDENTITY = \"L8ZbWWoFDuENrz6PLkJwcqLcgIFoTw2N\"\n",
    "KEY_CREDENTIAL = \"9ince7m5EtRAQ7oqosEEDWn1siXROG7e\"\n",
    "\n",
    "PARAMS  = {\"key_identity\": KEY_IDENTITY, \"key_credential\": KEY_CREDENTIAL}\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# The site ID for \"Persian Press Pilot\"\n",
    "SITE_ID = 1  # <-- REPLACE with the actual ID of your site\n",
    "\n",
    "def get_all_items():\n",
    "    \"\"\"\n",
    "    Returns a list of all items (max 1000).\n",
    "    If you have more than 1000 items, implement pagination or loop multiple pages.\n",
    "    You can also filter by resource_class_id, etc., if you only want certain items.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/items\"\n",
    "    params = {\n",
    "        \"per_page\": 1000,\n",
    "        **PARAMS\n",
    "    }\n",
    "    resp = requests.get(url, params=params, verify=False)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"❌ Error retrieving items: {resp.status_code} {resp.text}\")\n",
    "        return []\n",
    "\n",
    "def get_item_json(item_id):\n",
    "    \"\"\"\n",
    "    Retrieves the full JSON for the item, \n",
    "    so we can re-PUT it without losing metadata.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/items/{item_id}\"\n",
    "    resp = requests.get(url, params=PARAMS, verify=False)\n",
    "    if resp.status_code == 200:\n",
    "        return resp.json()\n",
    "    else:\n",
    "        print(f\"❌ GET item_id={item_id} error: {resp.status_code} {resp.text}\")\n",
    "        return None\n",
    "\n",
    "def put_item_json(item_id, item_data):\n",
    "    \"\"\"\n",
    "    PUT (overwrite) the entire item with item_data,\n",
    "    but keep all existing metadata we didn't remove from item_data.\n",
    "    Must remove read-only fields first.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/items/{item_id}\"\n",
    "\n",
    "    # Remove read-only fields that cause errors or duplication\n",
    "    for field in [\n",
    "        \"o:id\", \"o:created\", \"o:modified\", \"o:owner\",\n",
    "        \"o:resource_type\", \"o:thumbnail\", \"o:created\",\n",
    "        \"o:media\", \"o:link\", \"o:item_set\", \"o:is_public\",\n",
    "        \"o:modules\", \"o:site\", \"o:request\", \"o:module\",\n",
    "        \"@type\", \"@context\", \"@id\",\n",
    "    ]:\n",
    "        item_data.pop(field, None)\n",
    "\n",
    "    resp = requests.put(url, params=PARAMS, headers=HEADERS, json=item_data, verify=False)\n",
    "    if resp.status_code in [200, 201]:\n",
    "        print(f\"   ✅ PUT item_id={item_id} succeeded.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"   ❌ PUT item_id={item_id} failed: {resp.status_code} {resp.text}\")\n",
    "        return False\n",
    "\n",
    "def safely_append_site(item_id, site_id):\n",
    "    \"\"\"\n",
    "    1) GET the full item JSON\n",
    "    2) Check the 'o:site' array\n",
    "    3) If site_id not found, append it\n",
    "    4) PUT the updated item\n",
    "    \"\"\"\n",
    "    data = get_item_json(item_id)\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    # \"o:site\" is a list of site references, e.g.: \n",
    "    # [ { \"o:id\": <siteID>, \"@id\": \"...\", \"o:slug\": \"myslug\"}, ... ]\n",
    "    if \"o:site\" not in data:\n",
    "        data[\"o:site\"] = []\n",
    "\n",
    "    # Check if site already in the list\n",
    "    already_in_site = any(sref.get(\"o:id\") == site_id for sref in data[\"o:site\"])\n",
    "    if already_in_site:\n",
    "        print(f\"   (Skipping) item_id={item_id} is already in site {site_id}.\")\n",
    "        return\n",
    "\n",
    "    # Append a minimal site reference to the 'o:site' array\n",
    "    new_site_ref = {\n",
    "        \"o:id\": site_id\n",
    "    }\n",
    "    data[\"o:site\"].append(new_site_ref)\n",
    "\n",
    "    # Now PUT the entire item back\n",
    "    success = put_item_json(item_id, data)\n",
    "    if success:\n",
    "        print(f\"   🔗 item_id={item_id} ADDED to site {site_id}.\")\n",
    "\n",
    "def main():\n",
    "    items = get_all_items()\n",
    "    print(f\"Found {len(items)} total items. Adding them to site_id={SITE_ID} if not present.\\n\")\n",
    "\n",
    "    for it in items:\n",
    "        item_id = it[\"o:id\"]\n",
    "        print(f\"--- Processing item_id={item_id} ---\")\n",
    "        safely_append_site(item_id, SITE_ID)\n",
    "\n",
    "    print(\"\\nAll done. Items should now appear in site with ID=\", SITE_ID)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d95086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger an action of a module (refresh extract text)\n",
    "\n",
    "import requests\n",
    "import math\n",
    "import json\n",
    "\n",
    "# ========== CONFIGURE YOUR API / AUTH / CLASSES ===================== #\n",
    "API_BASE = \"https://persianpress.dh.huji.ac.il/issues/api\"\n",
    "\n",
    "PARAMS  = {\"key_identity\": KEY_IDENTITY, \"key_credential\": KEY_CREDENTIAL}\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Resource class ID for \"Page\"\n",
    "CLASS_PAGE = 50\n",
    "\n",
    "# We will process re-extractions in smaller chunks to avoid timeouts\n",
    "CHUNK_SIZE = 50\n",
    "\n",
    "# ==================================================================== #\n",
    "\n",
    "def list_all_pages(page_size=200):\n",
    "    \"\"\"\n",
    "    Retrieve ALL items with resource_class_id=CLASS_PAGE (50),\n",
    "    in chunks of <page_size> from the API. \n",
    "    Returns a combined list of items.\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    current_page = 1\n",
    "\n",
    "    while True:\n",
    "        query_params = {\n",
    "            **PARAMS,\n",
    "            \"resource_class_id\": CLASS_PAGE,\n",
    "            \"per_page\": page_size,\n",
    "            \"page\": current_page\n",
    "        }\n",
    "        url = f\"{API_BASE}/items\"\n",
    "        resp = requests.get(url, params=query_params, verify=False)\n",
    "        if resp.status_code == 200:\n",
    "            items = resp.json()\n",
    "            if not items:\n",
    "                break  # No more results\n",
    "            all_items.extend(items)\n",
    "            print(f\"Fetched page={current_page} with {len(items)} items.\")\n",
    "            current_page += 1\n",
    "        else:\n",
    "            print(f\"❌ Error listing items, status={resp.status_code}\")\n",
    "            print(\"Response:\", resp.text)\n",
    "            break\n",
    "\n",
    "    print(f\"Done! Fetched a total of {len(all_items)} 'Page' items.\")\n",
    "    return all_items\n",
    "\n",
    "def refresh_extract_text_for_item(item_id):\n",
    "    \"\"\"\n",
    "    Sends a PARTIAL PATCH that sets \"extract_text_action=refresh\",\n",
    "    triggering re-extraction for the item.\n",
    "    \"\"\"\n",
    "    patch_url = f\"{API_BASE}/items/{item_id}\"\n",
    "    patch_data = {\n",
    "        # According to your module, \"refresh\" re-runs text extraction if files are local\n",
    "        \"extract_text_action\": \"refresh\"\n",
    "    }\n",
    "    resp = requests.patch(patch_url, params=PARAMS, headers=HEADERS, json=patch_data, verify=False)\n",
    "    if resp.status_code in [200, 204]:\n",
    "        print(f\"   ✅ Re-extraction triggered for item_id={item_id}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"   ❌ Failed to patch item_id={item_id}: {resp.status_code}\")\n",
    "        print(resp.text)\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # 1) List all Page items, in smaller page chunks (200 each)\n",
    "    all_pages = list_all_pages(page_size=200)\n",
    "    print(f\"Total Pages retrieved: {len(all_pages)}\")\n",
    "\n",
    "    if not all_pages:\n",
    "        print(\"No pages found, nothing to refresh.\")\n",
    "        return\n",
    "\n",
    "    # 2) Extract IDs\n",
    "    all_page_ids = [p[\"o:id\"] for p in all_pages]\n",
    "\n",
    "    # 3) Chunk them in groups of CHUNK_SIZE to avoid timeouts in consecutive patches\n",
    "    for i in range(0, len(all_page_ids), CHUNK_SIZE):\n",
    "        subset = all_page_ids[i:i + CHUNK_SIZE]\n",
    "        print(f\"\\n=== Processing items {i+1} through {i+len(subset)} of {len(all_page_ids)} ===\")\n",
    "        for item_id in subset:\n",
    "            refresh_extract_text_for_item(item_id)\n",
    "\n",
    "    print(\"\\nAll done! Each item has been requested to 'refresh' extract text. \"\n",
    "          \"Check your Omeka Jobs or logs to see them re-run, and confirm new text is extracted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb56ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
