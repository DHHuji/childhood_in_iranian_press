{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCN4Fu2EchQn"
   },
   "source": [
    "Click on your name and then profile to arrive your eScriptorium instance. If you are working on the main instance, that is `https://msia.escriptorium.fr/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhBZ3GdTceXd"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhFgWj6-c6lM"
   },
   "source": [
    "Then click on API and copy the alphanumeric code after \"Token\" (after and not including the :). This is your secret \"token\" which you should not share with anyone as it gives access to all data to which you have access whether your own or whether shared with you.\n",
    "\n",
    "If you work on the msia instance replace `abe8934572784dea` with your code in the following cell.\n",
    "\n",
    "If you work on another instance, you need to give it a name instead of `YOUR_SERVERNAME2`, replace YOURTOKEN2 with your token and replace \"URL2\" with the url (until but not including the first slash) of your instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1742386897114,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "C6SU61rMc-6t",
    "outputId": "39ad853d-d2d1-4e26-d6fd-e61f84da3fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing __init__.py\n"
     ]
    }
   ],
   "source": [
    "#@title initialization (seize your token and server and then run it)\n",
    "%%writefile __init__.py\n",
    "import copy\n",
    "serverconnections = [{'servername':'Local_INSTANCE', #choose freely\n",
    "  'headers' : {'Authorization':'Token c85e1c7e3d081ddd86ee70c58f2d3c66d6e9c6da'}, # as in UI\n",
    "  'root_url' : 'https://env-9828609.us.reclaim.cloud'}]\n",
    "  # you can add further instance or account connections if you have them\n",
    "\n",
    "for serverconnection in serverconnections:\n",
    "    serverconnection['headersbrief'] = copy.deepcopy(serverconnection['headers'])\n",
    "    serverconnection['headers']['Content-type'] = 'application/json'\n",
    "    serverconnection['headers']['Accept']= 'application/json'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  print(\"<> Running the module as a script! <>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11102,
     "status": "ok",
     "timestamp": 1742386954271,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "l7uFT1sgw3tQ",
    "outputId": "57b26c22-0128-48e1-b83d-5d1b4a86ecc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Levenshtein\n",
      "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n",
      "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 rapidfuzz-3.12.2\n",
      "Collecting xmltodict\n",
      "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.14.2\n"
     ]
    }
   ],
   "source": [
    "#@title import modules - please run this cell to import functions\n",
    "\n",
    "from __init__ import serverconnections\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import numpy\n",
    "from numpy import rad2deg,deg2rad,array,atleast_2d,cos,sin,arctan2,squeeze,cross,linalg\n",
    "import math\n",
    "from math import sqrt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import networkx as nx\n",
    "import collections\n",
    "from statistics import mean\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely import LineString\n",
    "from shapely import intersection\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import split as poly_split\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "from typing import Any, Tuple, Union, List, Dict, Type, TypeVar\n",
    "from imageio import imread\n",
    "from collections import defaultdict\n",
    "\n",
    "!pip install Levenshtein\n",
    "import Levenshtein\n",
    "#!pip install bidict\n",
    "#import bidict\n",
    "#!pip install xlsxwriter\n",
    "#import xlsxwriter\n",
    "!pip install xmltodict\n",
    "import xmltodict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1742386961502,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "_4IS2IZ7wggh"
   },
   "outputs": [],
   "source": [
    "# @title helper functions\n",
    "\n",
    "def loop_through_itempages_and_get_all(base_url,suffix=''):\n",
    "    result_list=[]\n",
    "    def get_page(page,result_list):\n",
    "        data = False\n",
    "        #url = urljoin(base_url, '?page=%d' % page)\n",
    "        if suffix=='':\n",
    "          url = base_url+'?page='+str(page)\n",
    "        else:\n",
    "          url = base_url+'?page='+str(page)+'&'+suffix\n",
    "        res = requests.get(url, headers=headers)\n",
    "        #print(res.json())\n",
    "        try:\n",
    "            data = res.json()\n",
    "        except json.decoder.JSONDecodeError as e:\n",
    "            print('exception')\n",
    "            print(res)\n",
    "        else:\n",
    "          if data:\n",
    "            #print(data)\n",
    "            result_list += [item for item in data['results']]\n",
    "        if data:\n",
    "          if data['next']:\n",
    "            get_page(page+1,result_list)\n",
    "    get_page(1,result_list)\n",
    "    return(result_list)\n",
    "\n",
    "def get_pks_of_dict_list(list_of_dictionaries):\n",
    "    pks =[item['pk'] for item in list_of_dictionaries]\n",
    "    return pks\n",
    "\n",
    "def get_sthg_from_dict_list(list_of_dictionaries,sthg):\n",
    "  return([item[sthg] for item in list_of_dictionaries])\n",
    "\n",
    "# get_urls--------------------------------------------------------------------------------------------\n",
    "def get_documents_url():\n",
    "  return root_url+'/api/documents/'\n",
    "\n",
    "def get_specific_doc_url(doc_pk):\n",
    "  return get_documents_url()+str(doc_pk)+'/'\n",
    "\n",
    "def get_parts_url(doc_pk):\n",
    "  return get_specific_doc_url(doc_pk)+'parts/'\n",
    "\n",
    "def get_specific_part_url(doc_pk,part_pk):\n",
    "  return get_parts_url(doc_pk)+str(part_pk)+'/'\n",
    "\n",
    "def get_metadata_url(doc_pk,part_pk):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'metadata/'\n",
    "\n",
    "def get_regions_url(doc_pk,part_pk):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'blocks/'\n",
    "\n",
    "def get_specific_region_url(doc_pk,part_pk,region_pk):\n",
    "  return get_regions_url(doc_pk,part_pk)+str(region_pk)+'/'\n",
    "\n",
    "def get_lines_url(doc_pk,part_pk):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'lines/'\n",
    "\n",
    "def get_specific_line_url(doc_pk,part_pk,line_pk):\n",
    "  return get_lines_url(doc_pk,part_pk)+str(line_pk)+'/'\n",
    "\n",
    "def get_doc_transcriptions_url(doc_pk):\n",
    "  return get_specific_doc_url(doc_pk)+'transcriptions/'\n",
    "\n",
    "def get_specific_transcription_level_url(doc_pk,tr_level):\n",
    "  return get_doc_transcriptions_url(doc_pk)+str(tr_level)+'/'\n",
    "\n",
    "def get_part_transcriptions_base_url(doc_pk,part_pk):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'transcriptions/'\n",
    "\n",
    "def get_part_transcriptions_url(doc_pk,part_pk,tr_level):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'transcriptions/?transcription='+str(tr_level)+'&'\n",
    "\n",
    "def get_specific_transcription_url(doc_pk,part_pk,tr_pk):\n",
    "  return get_part_transcriptions_base_url+str(tr_pk)+'/'\n",
    "\n",
    "def get_models_url():\n",
    "  return root_url+'/api/models/'\n",
    "\n",
    "def get_specific_model_url(model_pk):\n",
    "  return get_models_url()+str(model_pk)+'/'\n",
    "\n",
    "#def get_projects_url():\n",
    "#  return root_url+'/api/projects/'\n",
    "\n",
    "#def get_linetypes_url():\n",
    "#  return root_url+'/api/types/lines/'\n",
    "\n",
    "#def get_regiontypes_url():\n",
    "#  return root_url+'/api/types/regions/'\n",
    "\n",
    "#def get_annotationtypes_url():\n",
    "#  return root_url+'/api/types/annotations/'\n",
    "\n",
    "#def get_annotationtypes_url():\n",
    "#  return root_url+'/api/types/part/'\n",
    "\n",
    "\n",
    "def get_segmentation_url(doc_pk):\n",
    "  return get_specific_doc_url(doc_pk)+'segment/'\n",
    "\n",
    "def get_transcribe_url(doc_pk):\n",
    "  return get_specific_doc_url(doc_pk)+'transcribe/'\n",
    "\n",
    "\n",
    "\n",
    "  # search-and-sort-----------------------------------------------------------------------\n",
    "def search(pk, list_of_dictionaries):\n",
    "    result_list=[element for element in list_of_dictionaries if element['pk'] == pk]\n",
    "    if len(result_list)==1:\n",
    "      return result_list  [0]\n",
    "    elif len(result_list)==0:\n",
    "      return None\n",
    "    else:\n",
    "      return result_list\n",
    "\n",
    "def search_any(key, value, list_of_dictionaries):\n",
    "    return [element for element in list_of_dictionaries if element[key] == value]\n",
    "\n",
    "def filter_dict_list(key,key_val_list,list_of_dictionaries):\n",
    "  if (isinstance(key_val_list,str)) or (not(isinstance(key_val_list,list))):\n",
    "    return search_any(key, key_val_list, list_of_dictionaries)\n",
    "  else:\n",
    "    return [row for row in list_of_dictionaries if row.get(key) in key_val_list]\n",
    "\n",
    "def find_substr_in_str(str,substr):\n",
    "  return [m for m in re.finditer(substr, str)]\n",
    "\n",
    "def replace_substr_in_str(str,substr,replacement):\n",
    "  return re.sub(substr, replacement, str)\n",
    "\n",
    "def get_idx(pk, list_of_dictionaries):\n",
    "  result_list=[n for n,element in enumerate(list_of_dictionaries) if element['pk'] == pk]\n",
    "  if len(result_list)==1:\n",
    "    return result_list[0]\n",
    "  elif len(result_list)==0:\n",
    "    return None\n",
    "  else:\n",
    "    return result_list\n",
    "\n",
    "def check_token_contains(key,chars,list_of_dictionaries):\n",
    "  return [n for n,element in enumerate(list_of_dictionaries) if any((c in chars) for c in element[key])]\n",
    "\n",
    "def find_string_in_page_transcription(doc_pk,part_pk,tr_level,query_string):\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "    transcription_txt = get_sthg_from_dict_list(transcriptions,'content')\n",
    "    page_transcription = ' '.join(transcription_txt)\n",
    "    return find_substr_in_str(page_transcription,query_string)\n",
    "\n",
    "\n",
    "#-------------maths-----------------------------------------------------------------------------------------\n",
    "\n",
    "def get_line_beg_point(line,direction='RTL'):\n",
    "  if direction == 'RTL':\n",
    "    return line['baseline'][-1]\n",
    "  else:\n",
    "    return line['baseline'][0]\n",
    "def get_line_end_point(line,direction='RTL'):\n",
    "  if direction == 'RTL':\n",
    "    return line['baseline'][0]\n",
    "  else:\n",
    "    return line['baseline'][-1]\n",
    "def get_line_mid_point(line):\n",
    "  baseline = line['baseline']\n",
    "  line_length=get_baseline_length(baseline)\n",
    "  nu_segments = len(baseline)-1\n",
    "  segment_length_list=list()\n",
    "  p0 = baseline[0]\n",
    "  for p1 in baseline[1:]:\n",
    "    segment = [p0,p1]\n",
    "    segment_length_list.append(get_baseline_length(segment))\n",
    "    p0 = p1\n",
    "  way2go = line_length\n",
    "  i=-1\n",
    "  while way2go > line_length/2:\n",
    "    i += 1\n",
    "    way2go -= segment_length_list[i]\n",
    "  angle = get_angle([baseline[i],baseline[i+1]])\n",
    "  h,w = get_vector(angle,-way2go)\n",
    "  return [baseline[i][0]-h,baseline[i][1]-w]\n",
    "\n",
    "def get_line_beg_angle(line,direction='RTL'):\n",
    "  p1 = get_line_beg_point(line,direction)\n",
    "  if direction == 'RTL':\n",
    "    p2 = line['baseline'][-2]\n",
    "  else:\n",
    "    p2 = line['baseline'][1]\n",
    "  return get_angle([p1,p2])\n",
    "def get_line_end_angle(line,direction='RTL'):\n",
    "  p1 = get_line_end_point(line,direction)\n",
    "  if direction == 'RTL':\n",
    "    p2 = line['baseline'][1]\n",
    "  else:\n",
    "    p2 = line['baseline'][-2]\n",
    "  return get_angle([p2,p1])\n",
    "def get_baseline_length(baseline):\n",
    "  total_length=0\n",
    "  x0,y0 = baseline[0]\n",
    "  for x1,y1 in baseline[1:]:\n",
    "    total_length += dist2points(x0,y0,x1,y1)\n",
    "    x0 = x1\n",
    "    y0 = y1\n",
    "  return round(total_length,1)\n",
    "def get_mean_line_height(line):\n",
    "  mask=line['mask']\n",
    "  line_area=Polygon(mask).area\n",
    "  line_length=get_baseline_length(line['baseline'])\n",
    "  if line_length>0:\n",
    "    return round(line_area/line_length,1)\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def get_average_line_distance(lines):\n",
    "  nu_lines = len(lines)\n",
    "  if nu_lines>1:\n",
    "    distmapsize = (nu_lines-1,nu_lines-1)\n",
    "    distmap = numpy.ones(distmapsize,dtype = int)*1000000\n",
    "    for i1,line1 in enumerate(lines):\n",
    "      linestr1 = LineString(line1['baseline'])\n",
    "      for i2,line2 in enumerate(lines[i1+1:]):\n",
    "        linestr2 = LineString(line2['baseline'])\n",
    "        distmap[i1-1][i1+i2] = int(round(linestr1.distance(linestr2),0))\n",
    "    row_ind, col_ind = linear_sum_assignment(distmap)\n",
    "    dist_sum=0\n",
    "    for r,c in zip(row_ind,col_ind):\n",
    "      dist_sum+=distmap[r][c]\n",
    "    return int(round(dist_sum/nu_lines,0))\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def dist2points(x1,y1,x2,y2):\n",
    "  dist=sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "  return dist\n",
    "\n",
    "def get_centroid(line):\n",
    "  return [int(round((line[0][0]+line[-1][0])/2,0)),int(round((line[0][1]+line[-1][1])/2,0))]\n",
    "\n",
    "def get_angle(line):\n",
    "    return rad2deg(arctan2(line[-1][-1] - line[0][-1], line[-1][0] - line[0][0]))\n",
    "\n",
    "def get_vector(angle,length):\n",
    "  #from numpy import rad2deg,deg2rad,cos,sin\n",
    "  #return rad2deg(arctan2(line[-1][-1] - line[0][-1], line[-1][0] - line[0][0]))\n",
    "  #length=hypothenuse\n",
    "  #ankathete=x\n",
    "  #gegenkathete=y\n",
    "  y=int(round(sin(deg2rad(angle))*length,0))\n",
    "  x=int(round(cos(deg2rad(angle))*length,0))\n",
    "  return x,y\n",
    "\n",
    "def rotateNP(p, origin=(0, 0), degrees=0):\n",
    "    angle = deg2rad(degrees)\n",
    "    R = array([[cos(angle), -sin(angle)],\n",
    "                  [sin(angle),  cos(angle)]])\n",
    "    o = atleast_2d(origin)\n",
    "    p = atleast_2d(p)\n",
    "    return squeeze((R @ (p.T-o.T) + o.T).T)\n",
    "\n",
    "def line_intersection(line1, line2):\n",
    "    xdiff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])\n",
    "    ydiff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1])\n",
    "\n",
    "    def det(a, b):\n",
    "        return a[0] * b[1] - a[1] * b[0]\n",
    "\n",
    "    div = det(xdiff, ydiff)\n",
    "    if div == 0:\n",
    "        print('lines do not intersect')\n",
    "        x = line1[0][0]\n",
    "        y = 100000\n",
    "    else:\n",
    "        d = (det(*line1), det(*line2))\n",
    "        x = det(d, xdiff) / div\n",
    "        y = det(d, ydiff) / div\n",
    "    return x, y\n",
    "\n",
    "def float2int(x):\n",
    "  return int(round(x,0))\n",
    "\n",
    "def str2int(x):\n",
    "  return int(round(float(x),0))\n",
    "\n",
    "def list_of_str_points2int(list_of_points):\n",
    "  upper_int = []\n",
    "  for p in list_of_points:\n",
    "    p_int = list()\n",
    "    for c in p:\n",
    "      p_int.append(str2int(c))\n",
    "    upper_int.append(p_int)\n",
    "  return upper_int\n",
    "\n",
    "def plot_polygon(Polygon):\n",
    "  matplot(*Polygon.exterior.xy)\n",
    "\n",
    "# check_validity--------------------------------------------------------------------------------\n",
    "def check_point_in_image(p,imgHeight,imgWidth,safetydistance):\n",
    "    # deal with x:\n",
    "    p[0]=min(imgWidth-safetydistance,max(p[0],safetydistance))\n",
    "    p[1]=min(imgHeight-safetydistance,max(p[1],safetydistance))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1742386964095,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "LHhTd_vwlwlD"
   },
   "outputs": [],
   "source": [
    "#@title basic\n",
    "import json\n",
    "import requests\n",
    "from numpy import rad2deg,deg2rad,array,atleast_2d,cos,sin,arctan2,squeeze,cross,linalg\n",
    "from shapely import LineString\n",
    "from shapely.ops import split as poly_split\n",
    "\n",
    "\n",
    "#----------------get-info------------------------------\n",
    "def get_serverinfo(servername,serverconnections):\n",
    "  serverconnection=search_any('servername', servername, serverconnections)[0]\n",
    "  print('switching to ',servername)\n",
    "  headers = serverconnection['headers']\n",
    "  headersbrief = serverconnection['headersbrief']\n",
    "  root_url = serverconnection['root_url']\n",
    "  return root_url,headers,headersbrief\n",
    "\n",
    "\n",
    "def get_basic_info(doc_pk):\n",
    "  '''\n",
    "  input: document ID\n",
    "  output:\n",
    "  nu of parts\n",
    "  transcription_level_list (list of different transcription levels)\n",
    "  region_type_list, line_type_list (segmentation ontologies)\n",
    "  '''\n",
    "  print('get document segmentation ontology for document: ',doc_pk)\n",
    "  doc_url = get_documents_url()+str(doc_pk)+'/'\n",
    "  print(doc_url)\n",
    "  document_dict=requests.get(url=doc_url,headers=headers).json()\n",
    "  nu_parts = document_dict['parts_count']\n",
    "  print('Document:', doc_pk,' with ',nu_parts,' parts')\n",
    "  region_type_list=document_dict['valid_block_types']\n",
    "  print('region types:',region_type_list)\n",
    "  line_type_list=document_dict['valid_line_types']\n",
    "  print('line types:',line_type_list)\n",
    "  transcription_level_list=document_dict['transcriptions']\n",
    "  print('transcription_level_list:',transcription_level_list)\n",
    "  return nu_parts,transcription_level_list,region_type_list,line_type_list\n",
    "\n",
    "def get_margin_and_paratext_type_pks(doc_pk):\n",
    "    region_type_list,line_type_list=get_document_segmentation_ontology(doc_pk)\n",
    "    for region_type in region_type_list:\n",
    "        if region_type['name']=='Margin':\n",
    "            margin_type_pk=region_type['pk']\n",
    "        elif region_type['name']=='Paratext':\n",
    "            paratext_type_pk=region_type['pk']\n",
    "    return margin_type_pk,paratext_type_pk\n",
    "\n",
    "def get_default_line_type_pks(doc_pk):\n",
    "  region_types,line_types = get_document_segmentation_ontology(doc_pk)\n",
    "  default_line_types = search_any('name','default',line_types)\n",
    "  default_line_type_pks = [line_type['pk'] for line_type in default_line_types]\n",
    "  return default_line_type_pks\n",
    "\n",
    "#-get-items----------------------------------------------------------------------\n",
    "def get_item(url):\n",
    "  return(requests.get(url,headers=headers).json())\n",
    "\n",
    "def get_all_docs():\n",
    "  docs_url = get_documents_url()\n",
    "  return loop_through_itempages_and_get_all(docs_url)\n",
    "\n",
    "def get_doc(doc_pk):\n",
    "  doc_url = get_specific_doc_url(doc_pk)\n",
    "  return(get_item(doc_url))\n",
    "\n",
    "def get_all_projects():\n",
    "  projects_url = get_projects_url()\n",
    "  return loop_through_itempages_and_get_all(projects_url)\n",
    "\n",
    "def get_projects_url():\n",
    "  return root_url+'/api/projects/'\n",
    "\n",
    "def get_part(doc_pk,part_pk):\n",
    "  part_url = get_specific_part_url(doc_pk,part_pk)\n",
    "  return(get_item(part_url))\n",
    "\n",
    "def get_region(doc_pk,part_pk,region_pk):\n",
    "  region_url = get_specific_region_url(doc_pk,part_pk,region_pk)\n",
    "  return(get_item(region_url))\n",
    "\n",
    "def get_line(doc_pk,part_pk,line_pk):\n",
    "  line_url = get_specific_line_url(doc_pk,part_pk,line_pk)\n",
    "  return(get_item(line_url))\n",
    "\n",
    "def get_all_metadata_of_part(doc_pk,part_pk):\n",
    "  metadata_url = get_metadata_url(doc_pk,part_pk)\n",
    "  return(loop_through_itempages_and_get_all(metadata_url))\n",
    "\n",
    "def get_all_parts(doc_pk):\n",
    "    parts_url = get_parts_url(doc_pk)\n",
    "    return(loop_through_itempages_and_get_all(parts_url))\n",
    "\n",
    "def get_all_regions_of_part(doc_pk,part_pk):\n",
    "    regions_url = get_regions_url(doc_pk,part_pk)\n",
    "    return(loop_through_itempages_and_get_all(regions_url))\n",
    "\n",
    "def get_all_lines_of_part(doc_pk,part_pk):\n",
    "    lines_url = get_lines_url(doc_pk,part_pk)\n",
    "    return(loop_through_itempages_and_get_all(lines_url))\n",
    "\n",
    "def get_page_height_width(doc_pk,part_pk):\n",
    "  part_dict = get_part(doc_pk,part_pk)\n",
    "  pagewidth=part_dict['image']['size'][0]\n",
    "  pageheight=part_dict['image']['size'][1]\n",
    "  return pageheight,pagewidth\n",
    "\n",
    "def get_img_from_url(img_url):\n",
    "  img = imread(img_url)\n",
    "  return img\n",
    "\n",
    "def get_part_img(doc_pk,part):\n",
    "  imgname = part['filename']\n",
    "  img_url = root_url+'/media/documents/'+str(doc_pk)+'/'+imgname\n",
    "  #print(img_url)\n",
    "  img = get_img_from_url(img_url)\n",
    "  return img\n",
    "\n",
    "def write_doc_list_from_instance2file(fname='results.tsv'):\n",
    "  docs = get_all_docs()\n",
    "  f = open(fname,mode='wt')\n",
    "  for doc in docs:\n",
    "    doc_pk=doc['pk']\n",
    "    doc_name=doc['name']\n",
    "    doc_img_nu=doc['parts_count']\n",
    "    f.write(str(doc_pk)+'\\t'+doc_name+'\\t'+str(doc_img_nu)+'\\n')\n",
    "  f.close()\n",
    "\n",
    "def get_part_pk_list(doc_pk):\n",
    "    parts = get_all_parts(doc_pk)\n",
    "    pks = get_pks_of_dict_list(parts)\n",
    "    return pks\n",
    "\n",
    "def get_line_pk_list(doc_pk,part_pk):\n",
    "    lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    pks = get_pks_of_dict_list(lines)\n",
    "    return pks\n",
    "#def get_linetranscription_pk_list(doc_pk, part_pk,tr_pk):\n",
    "#    tr_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'\n",
    "#    return(loop_through_itempages(tr_url,'pk','results'))\n",
    "\n",
    "def get_region_pk_list(doc_pk,part_pk):\n",
    "    regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "    pks = get_pks_of_dict_list(regions)\n",
    "    return pks\n",
    "\n",
    "def get_inhabited_region_pk_list(doc_pk,part_pk):\n",
    "    all_lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    inhabited_regions = list(set([line['region'] for line in all_lines]))\n",
    "    return(inhabited_regions)\n",
    "\n",
    "def get_region_pk_list_of_type(doc_pk,part_pk,region_type):\n",
    "    regions_url = get_regions_url(doc_pk,part_pk)\n",
    "    all_regions=get_all_regions_of_part(doc_pk,part_pk)\n",
    "    regions=[region['pk'] for region in all_regions if region['typology'] == region_type]\n",
    "    return regions\n",
    "\n",
    "def get_regions_and_lines(doc_pk,part_pk):\n",
    "  lines_url=get_lines_url(doc_pk,part_pk)\n",
    "  lines=loop_through_itempages_and_get_all(lines_url)\n",
    "  regions_url=get_regions_url(doc_pk,part_pk)\n",
    "  regions=loop_through_itempages_and_get_all(regions_url)\n",
    "  return regions,lines\n",
    "\n",
    "def get_regions_and_lines_and_transcriptions(doc_pk,part_pk,tr_level):\n",
    "  lines_url=get_lines_url(doc_pk,part_pk)\n",
    "  lines=loop_through_itempages_and_get_all(lines_url)\n",
    "  regions_url=get_regions_url(doc_pk,part_pk)\n",
    "  regions=loop_through_itempages_and_get_all(regions_url)\n",
    "  tr_url = get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "  tr_suffix = 'transcription='+str(tr_level)\n",
    "  transcriptions=loop_through_itempages_and_get_all(tr_url,tr_suffix)\n",
    "  return regions,lines,transcriptions\n",
    "\n",
    "def get_main_region_pk_list(doc_pk,part_pk):\n",
    "    regions = get_region_pk_list_of_type(doc_pk,part_pk,2)\n",
    "    return regions\n",
    "\n",
    "def get_region_box_list(doc_pk,part_pk):\n",
    "    regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "    return(get_sthg_from_dict_list(regions,'box'))\n",
    "\n",
    "def get_all_page_transcriptions(doc_pk,part_pk):\n",
    "    tr_url = get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "    return(loop_through_itempages_and_get_all(tr_url))\n",
    "\n",
    "def get_page_transcription(doc_pk,part_pk,tr_level):\n",
    "    tr_url = get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "    trs =  loop_through_itempages_and_get_all(tr_url)\n",
    "    return [tr for tr in trs if tr['transcription']==tr_level]\n",
    "\n",
    "def get_transcription_level_pk_by_name(doc_pk,tr_level_name):\n",
    "    tr_levels = get_transcription_levels(doc_pk)\n",
    "    tr_levels = search_any('name',tr_level_name,tr_levels)\n",
    "    if not(tr_levels == []):\n",
    "      return tr_levels[0]['pk']\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "def get_page_transcription_tr_pks(doc_pk,part_pk,tr_level):\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "    return(get_sthg_from_dict_list(transcriptions,'pk'))\n",
    "\n",
    "def get_page_transcription_line_pks(doc_pk,part_pk,tr_level):\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "    return(get_sthg_from_dict_list(transcriptions,'line'))\n",
    "\n",
    "def get_all_lines_of_region(doc_pk,part_pk,region_pk):\n",
    "    lines_url = get_lines_url(doc_pk,part_pk)\n",
    "    all_lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    lines = [line for line in all_lines if line['region'] == region_pk]\n",
    "    return lines\n",
    "\n",
    "def get_all_line_pks_of_region(doc_pk,part_pk,region_pk):\n",
    "    lines_url = get_lines_url(doc_pk,part_pk)\n",
    "    all_lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    line_pks = [line['pk'] for line in all_lines if line['region'] == region_pk]\n",
    "    return line_pks\n",
    "\n",
    "def get_document_segmentation_ontology(doc_pk,print_status=False):\n",
    "    doc_url = get_specific_doc_url(doc_pk)\n",
    "    document_dict=get_item(doc_url)\n",
    "    region_type_list=document_dict['valid_block_types']\n",
    "    line_type_list=document_dict['valid_line_types']\n",
    "    if print_status:\n",
    "      print('get document segmentation ontology for document: ',doc_pk)\n",
    "      print(document_dict)\n",
    "      for region_type in region_type_list:\n",
    "          print(region_type)\n",
    "      for line_type in line_type_list:\n",
    "          print(line_type)\n",
    "    return region_type_list,line_type_list\n",
    "\n",
    "def get_transcription_levels(doc_pk):\n",
    "    tr_level_url = get_doc_transcriptions_url(doc_pk)\n",
    "    tr_levels = get_item(tr_level_url)\n",
    "    return tr_levels\n",
    "\n",
    "def get_specific_tr_level_pk(doc_pk,tr_level_name):\n",
    "  all_tr_levels = get_transcription_levels(doc_pk)\n",
    "  tr_levels = search_any('name',tr_level_name,all_tr_levels)\n",
    "  if not(tr_levels) == None:\n",
    "    if len(tr_levels)==1:\n",
    "      return tr_levels[0]['pk']\n",
    "    else:\n",
    "      return [tr_level['pk'] for tr_level in tr_levels]\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def get_set_of_all_characters_in_transcription_level(doc_pk,part_list,tr_level):\n",
    "    print('get char set of document for document: ',doc_pk)\n",
    "    chars = set()\n",
    "    for n,part_pk in enumerate(part_list):\n",
    "        if n % 100 == 0:\n",
    "            print(n,' parts finished')\n",
    "        transcriptions_this_part=get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "        for line in transcriptions_this_part:\n",
    "            chars=chars.union(set(line['content']))\n",
    "    chars=list(chars)\n",
    "    chars.sort()\n",
    "    return chars\n",
    "\n",
    "def get_imgname_list(doc_pk):\n",
    "    parts = get_all_parts(doc_pk)\n",
    "    return(get_sthg_from_dict_list(parts,'filename'))\n",
    "\n",
    "def get_all_models():\n",
    "  models_url = get_models_url()\n",
    "  models = loop_through_itempages_and_get_all(models_url)\n",
    "  simplified_models = [{'pk':model['pk'],'name':model['name'],'file':model['file'],'job':model['job'],'owner':model['owner'],'training':model['training']} for model in models]\n",
    "  unique_models =  [dict(t) for t in {tuple(d.items()) for d in simplified_models}]\n",
    "  return(unique_models)\n",
    "\n",
    "def get_specific_model(model_pk):\n",
    "  model_url = get_specific_model_url(model_pk)\n",
    "  return(get_item(model_url))\n",
    "\n",
    "def get_all_seg_models():\n",
    "  all_models = get_all_models()\n",
    "  return [model for model in all_models if model['job'] == 'Segment']\n",
    "\n",
    "def get_all_trans_models():\n",
    "  all_models = get_all_models()\n",
    "  return [model for model in all_models if model['job'] == 'Recognize']\n",
    "\n",
    "\"\"\"\n",
    "def get_doc_annotation_taxonomies(doc_pk):\n",
    "  tax_url = get_specific_doc_url(doc_pk)+'taxonomies/annotations/'\n",
    "  return loop_through_itempages_and_get_all(tax_url)\n",
    "\"\"\"\n",
    "def get_doc_annotation_components(doc_pk):\n",
    "  comp_url = get_specific_doc_url(doc_pk)+'taxonomies/components/'\n",
    "  return loop_through_itempages_and_get_all(comp_url)\n",
    "\n",
    "def get_doc_txt_annotation_taxonomies(doc_pk):\n",
    "  text_tax_url = get_specific_doc_url(doc_pk)+'taxonomies/annotations/text/'\n",
    "  return loop_through_itempages_and_get_all(text_tax_url)\n",
    "\n",
    "def get_doc_img_annotation_taxonomies(doc_pk):\n",
    "  img_tax_url = get_specific_doc_url(doc_pk)+'taxonomies/annotations/image/'\n",
    "  return loop_through_itempages_and_get_all(img_tax_url)\n",
    "\n",
    "def get_part_txt_annotations(doc_pk,part_pk):\n",
    "  text_tax_url = get_specific_part_url(doc_pk,part_pk)+'annotations/text/'\n",
    "  return loop_through_itempages_and_get_all(text_tax_url)\n",
    "\n",
    "def get_part_img_annotations(doc_pk,part_pk):\n",
    "  img_tax_url = get_specific_part_url(doc_pk,part_pk)+'annotations/image/'\n",
    "  return loop_through_itempages_and_get_all(img_tax_url)\n",
    "\n",
    "def get_specific_part_txt_annotations(doc_pk,part_pk,annot_pk):\n",
    "  text_tax_url = get_specific_part_url(doc_pk,part_pk)+'annotations/text/'+str(annot_pk)+'/'\n",
    "  return get_item(text_tax_url)\n",
    "\n",
    "def get_specific_part_img_annotations(doc_pk,part_pk,annot_pk):\n",
    "  img_tax_url = get_specific_part_url(doc_pk,part_pk)+'annotations/image/'+str(annot_pk)+'/'\n",
    "  return get_item(img_tax_url)\n",
    "\n",
    "def get_all_doc_txt_annotations(doc_pk):\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  all_annots = list()\n",
    "  for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    all_annots += get_part_txt_annotations(doc_pk,part_pk)\n",
    "  return all_annots\n",
    "\n",
    "def get_all_doc_img_annotations(doc_pk):\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  all_annots = list()\n",
    "  for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    all_annots += get_part_img_annotations(doc_pk,part_pk)\n",
    "  return all_annots\n",
    "\n",
    "def get_all_doc_annotations(doc_pk):\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  all_annots = list()\n",
    "  for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    all_annots += get_part_txt_annotations(doc_pk,part_pk)\n",
    "    all_annots += get_part_img_annotations(doc_pk,part_pk)\n",
    "  return all_annots\n",
    "\n",
    "def get_all_img_names(doc_pk):\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  return [(part['pk'], part['filename']) for part in parts]\n",
    "\n",
    "#---------------create-things-------------------------------------------------------------------------------\n",
    "\n",
    "def create_new_document(doc_name,project,script_pk=\"Hebrew\",read_direction=\"rtl\",line_offset=1, show_conf = False):\n",
    "    docs_url = get_documents_url()\n",
    "    data=json.dumps({\"name\": doc_name,\"main_script\": script_pk,\n",
    "                     \"project\": project,\n",
    "                     \"read_direction\":read_direction,\n",
    "                     \"line_offset\": line_offset,\n",
    "                     \"show_confidence_viz\": show_conf,\n",
    "                     \"valid_block_types\": [{\"pk\": 3,\"name\": \"Commentary\"},\n",
    "                     {\"pk\": 4,\"name\": \"Illustration\"},{\"pk\": 2,\"name\": \"Main\"},\n",
    "                     {\"pk\": 1,\"name\": \"Title\"}]})\n",
    "    res = requests.post(docs_url,headers=headers,data=data)\n",
    "    print(res.status_code, res.content)\n",
    "    jsonResponse=json.loads(res.content.decode('utf-8'))\n",
    "    doc_pk=jsonResponse['pk']\n",
    "    return doc_pk\n",
    "\n",
    "def create_transcription_levels(doc_pk,tr_name):\n",
    "    doc_tr_url=get_doc_transcriptions_url(doc_pk)\n",
    "    transcription_level_list = requests.get(doc_tr_url, headers=headers).json()\n",
    "    print('existing transcription levels: ',transcription_level_list)\n",
    "    existing_tr_level_names = [tr_level['name'] for tr_level in transcription_level_list]\n",
    "    if tr_name in existing_tr_level_names:\n",
    "      print(tr_name,'already exists')\n",
    "      break_this_code\n",
    "    else:\n",
    "      datajson={'name':tr_name}\n",
    "      print(datajson)\n",
    "      print(doc_tr_url)\n",
    "      data=json.dumps(datajson) #add check whether tr_level exists\n",
    "      res=requests.post(doc_tr_url,headers=headers,data=data)\n",
    "      print(res)\n",
    "      jsonResponse=json.loads(res.content.decode('utf-8'))\n",
    "      tr_pk=jsonResponse['pk']\n",
    "      return tr_pk\n",
    "\n",
    "\n",
    "def create_part(doc_pk,dirname,fname,part_name_in_UI = '',comments=None):\n",
    "  # part_name is what will appear in the UI left to the image filename. If left empty it is given automatically such as \"Element 1\" according to the position of the part in the document.\n",
    "  # e.g. pk = create_part(2299,r'/content','38774_jpg_000068_C.jpg','trial3','some_comment')\n",
    "  parts_url = get_parts_url(doc_pk)\n",
    "#mydir=(r\"A:\\openITI\\testset\")\n",
    "#myfile='15.png'\n",
    "  file = os.path.join(dirname, fname)\n",
    "  data = dict()\n",
    "  with open(file, 'rb') as fh:\n",
    "    if not(part_name_in_UI == ''):\n",
    "      data['name'] = part_name_in_UI\n",
    "    if not(comments == None):\n",
    "      data['comments'] = comments\n",
    "    res = requests.post(parts_url,data=data, files={'image': fh},headers=headersbrief)\n",
    "  print(res.status_code, res.content)\n",
    "  jsonResponse=json.loads(res.content.decode('utf-8'))\n",
    "  part_pk=jsonResponse['pk']\n",
    "  return part_pk\n",
    "\n",
    "def create_part_metadata(doc_pk,part_pk,eScr_metadata):\n",
    "  metadata_url = get_metadata_url(doc_pk,part_pk)\n",
    "  r = requests.post(url=metadata_url,headers=headers,data=json.dumps(eScr_metadata))\n",
    "  return r\n",
    "\n",
    "def create_region(doc_pk,part_pk,reg_polygon,regiontype_pk = None):\n",
    "  region_url = get_regions_url(doc_pk,part_pk)\n",
    "  data = {'document_part':part_pk,'box':reg_polygon,'typology':regiontype_pk}\n",
    "  r = requests.post(url=region_url,headers=headers,data=json.dumps(data))\n",
    "  return r\n",
    "\n",
    "def create_line(doc_pk,part_pk,baseline,mask = None, repolygonize = False, linetype_pk = None):\n",
    "  line_url = get_lines_url(doc_pk,part_pk)\n",
    "  data = {'document_part':part_pk,'baseline':baseline,'mask' : mask, 'typology':linetype_pk}\n",
    "  r = requests.post(url=line_url,headers=headers,data=json.dumps(data))\n",
    "  line_pk = r.json().get('pk')\n",
    "  if repolygonize:\n",
    "    repolygonize_line(doc_pk,part_pk,line_pk)\n",
    "  return r\n",
    "\n",
    "def bulk_create_lines(doc_pk,part_pk,lines):\n",
    "  bulk_create_url = get_lines_url(doc_pk,part_pk)+'bulk_create/'\n",
    "  data = {'lines' : lines}\n",
    "  r = requests.post(url=bulk_create_url,headers=headers,data=json.dumps(data))\n",
    "  return r\n",
    "\n",
    "def bulk_create_transcriptions(doc_pk,part_pk,trs):\n",
    "  bulk_create_url = get_part_transcriptions_base_url(doc_pk,part_pk)+'bulk_create/'\n",
    "  data = {'lines' : trs}\n",
    "  r = requests.post(url=bulk_create_url,headers=headers,data=json.dumps(data))\n",
    "  return r\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_txt_annotation(doc_pk,part_pk,tr_level_pk,start_line_pk,start_offset,end_line_pk,end_offset,annot_type,comments=''):\n",
    "  url = get_specific_part_url+'annotations/text/'\n",
    "  # integrate comments\n",
    "  data = {'taxonomy':annot_type,'transcription': tr_level_pk,'components':[],'start_line':start_line_pk,'start_offset':start_offset,'end_line':end_line_pk,'end_offset':end_offset}\n",
    "  r = requests.post(url,headers=headers,data = json.dumps(data))\n",
    "  return r\n",
    "\"\"\"\n",
    "\n",
    "def merge_lines(doc_pk,part_pk,lines):\n",
    "  merge_url = get_lines_url(doc_pk,part_pk)+'merge/'\n",
    "  # data should look like: json.dumps({'lines':[pk1,pk2,pk3]})\n",
    "  r = requests.post(merge_url,lines,headers = headers)\n",
    "  return r\n",
    "\n",
    "#-DELETE items-------------------------------------------------------------------\n",
    "def delete_item(delete_url):\n",
    "  r = requests.delete(delete_url,headers=headers)\n",
    "  return r\n",
    "\n",
    "def delete_part(doc_pk,part_pk):\n",
    "  delete_url = get_specific_part_url(doc_pk,part_pk)\n",
    "  r = delete_item(delete_url)\n",
    "  return r\n",
    "\n",
    "def delete_region(doc_pk,part_pk,region_pk):\n",
    "  delete_url = get_specific_region_url(doc_pk,part_pk,region_pk)\n",
    "  r = delete_item(delete_url)\n",
    "  return r\n",
    "\n",
    "def delete_all_regions_of_part(doc_pk,part_pk):\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  for region in regions:\n",
    "    delete_region(part_pk,region['pk'])\n",
    "\n",
    "def delete_line(doc_pk,part_pk,line_pk):\n",
    "  delete_url = get_specific_line_url(doc_pk,part_pk,line_pk)\n",
    "  r = delete_item(delete_url)\n",
    "  return r\n",
    "\n",
    "def bulk_delete_lines(doc_pk,part_pk,line_pk_list):\n",
    "  delete_url = get_lines_url(doc_pk,part_pk)+'bulk_delete/'\n",
    "  bulk_json=json.dumps({'lines': line_pk_list})\n",
    "  r=requests.post(delete_url,headers=headers,data=bulk_json)\n",
    "  return r\n",
    "\n",
    "def delete_all_lines_of_part(doc_pk,part_pk):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  del_lines = [line['pk'] for line in lines]\n",
    "  r = bulk_delete_lines(doc_pk,part_pk,del_lines)\n",
    "  return r\n",
    "\n",
    "def delete_all_lines_of_linetype(doc_pk,part_pk,line_type):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  del_line_pk_list = [line['pk'] for line in lines if line['typology']==line_type]\n",
    "  r = bulk_delete_lines(doc_pk,part_pk,del_line_pk_list)\n",
    "  return r\n",
    "\n",
    "def delete_all_lines_in_region(doc_pk,part_pk,region_pk):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  del_line_pk_list = [line['pk'] for line in lines if line['typology']==region_pk]\n",
    "  r = bulk_delete_lines(doc_pk,part_pk,del_line_pk_list)\n",
    "  return r\n",
    "\n",
    "def delete_all_lines_in_region_types(doc_pk,part_pk,region_type_pk_list):\n",
    "  if isinstance(region_type_pk_list,int):\n",
    "    region_type_pk_list = [region_type_pk_list]\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  del_line_pk_list = [line['pk'] for line in lines if line['region'] in [region['pk'] for region in regions if region['typology'] in region_type_pk_list]]\n",
    "  r = bulk_delete_lines(doc_pk,part_pk,del_line_pk_list)\n",
    "  return r\n",
    "\n",
    "def delete_all_transcriptions_of_tr_levels_in_linetypes(doc_pk,part_pk,tr_level_pk_list,line_type_pk_list):\n",
    "  if isinstance(line_type_pk_list,int):\n",
    "    line_type_pk_list = [line_type_pk_list]\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  transcriptions = delete_all_part_transcriptions(doc_pk,part_pk)\n",
    "  del_transcriptions = [tr for tr in transcriptions if (tr['transcription '] in tr_level_pk_list) and (tr['line'] in [line['pk'] for line in lines if line['typology'] in line_type_pk_list])]\n",
    "  r = bulk_delete_transcriptions(doc_pk,part_pk,del_transcriptions)\n",
    "  return r\n",
    "\n",
    "def delete_all_transcriptions_of_tr_levels_in_regiontypes(doc_pk,part_pk,tr_level_pk_list,region_type_pk_list):\n",
    "  if isinstance(region_type_pk_list,int):\n",
    "    region_type_pk_list = [region_type_pk_list]\n",
    "  if isinstance(tr_level_pk_list,int):\n",
    "    tr_level_pk_list = [tr_level_pk_list]\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  transcriptions = delete_all_part_transcriptions(doc_pk,part_pk)\n",
    "  del_regions = [region['pk'] for region in regions if region['typology'] in region_type_pk_list]\n",
    "  del_lines = [line['pk'] for line in lines if line['region'] in del_regions]\n",
    "  del_transcriptions = [tr for tr in transcriptions if (tr['transcription '] in tr_level_pk_list) and (tr['line'] in del_lines)]\n",
    "  r = bulk_delete_transcriptions(doc_pk,part_pk,del_transcriptions)\n",
    "  return r\n",
    "\n",
    "def bulk_delete_transcriptions(doc_pk,part_pk,transcription_pk_list):\n",
    "  delete_url = get_part_transcriptions_base_url(doc_pk,part_pk)+'bulk_delete/'\n",
    "  bulk_json=json.dumps({'lines': transcription_pk_list})\n",
    "  r=requests.post(delete_url,headers=headers,data=bulk_json)\n",
    "  return r\n",
    "\n",
    "def delete_part_transcription(doc_pk,part_pk,tr_level):\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  tr_pks = [tr['pk'] for tr in transcriptions]\n",
    "  r = bulk_delete_transcriptions(doc_pk,part_pk,tr_pks)\n",
    "  return r\n",
    "\n",
    "def delete_all_part_transcriptions(doc_pk,part_pk):\n",
    "  transcriptions = get_all_page_transcriptions(doc_pk,part_pk)\n",
    "  tr_pks = [tr['pk'] for tr in transcriptions]\n",
    "  r = bulk_delete_transcriptions(doc_pk,part_pk,tr_pks)\n",
    "  return r\n",
    "\n",
    "def delete_all_part_regions_lines_transcriptions(doc_pk,part_pk):\n",
    "  delete_all_part_transcriptions(doc_pk,part_pk)\n",
    "  delete_all_lines_of_part(doc_pk,part_pk)\n",
    "  delete_all_regions_of_part(doc_pk,part_pk)\n",
    "\n",
    "\n",
    "\n",
    "# modify-basic-data-------------------------------------------------------------------------\n",
    "\n",
    "def rename_tr_level(doc_pk,tr_level,new_name):\n",
    "  tr_level_url=get_doc_transcriptions_url(doc_pk)+str(tr_level)+'/'\n",
    "  rjson=get_item(tr_level_url)\n",
    "  rjson['name']=new_name\n",
    "  r=requests.put(tr_level_url,headers=headers,data=json.dumps(rjson))\n",
    "  return r\n",
    "\n",
    "def update_item(update_url, item):\n",
    "  r=requests.put(update_url,headers=headers,data=json.dumps(item))\n",
    "  return r\n",
    "\n",
    "def update_part(doc_pk,part_pk,part):\n",
    "  update_url = get_specific_part_url(doc_pk,part['pk'])\n",
    "  r = update_item(update_url,part)\n",
    "  return r\n",
    "\n",
    "def update_region(doc_pk,part_pk,region):\n",
    "  update_url = get_specific_region_url(doc_pk,part_pk,region['pk'])\n",
    "  r = update_item(update_url, region)\n",
    "  return r\n",
    "\n",
    "def update_line(doc_pk,part_pk,line):\n",
    "  update_url = get_specific_line_url(doc_pk,part_pk,line['pk'])\n",
    "  r = update_item(update_url, line)\n",
    "  return r\n",
    "\n",
    "def bulk_update_lines(doc_pk,part_pk,lines):\n",
    "  bulk_update_url = get_lines_url(doc_pk,part_pk)+'bulk_update/'\n",
    "  bulk_json=json.dumps({'lines': lines})\n",
    "  res_bulk=requests.put(bulk_update_url,headers=headers,data=bulk_json)\n",
    "  return res_bulk\n",
    "\n",
    "def bulk_update_transcriptions(doc_pk,part_pk,transcriptions):\n",
    "  bulk_update_url = get_part_transcriptions_base_url(doc_pk,part_pk)+'bulk_update/'\n",
    "  bulk_json=json.dumps({'lines': transcriptions}) # or should it say lines????\n",
    "  res_bulk=requests.put(bulk_update_url,headers=headers,data=bulk_json)\n",
    "  return res_bulk\n",
    "\n",
    "def modify_part_name(doc_pk,part_pk,part_name_in_UI):\n",
    "  part_url = get_specific_part_url(doc_pk,part_pk)\n",
    "  data={'name':part_name_in_UI}\n",
    "  res = requests.patch(part_url,data=json.dumps(data), headers=headers)\n",
    "  return res.status_code\n",
    "\n",
    "def add_comments2part(doc_pk,part_pk,comments,overwrite = False):\n",
    "  #e.g. sc = add_comments2part(2299,455702,comments='comment4',overwrite = False)\n",
    "  part_url = get_specific_part_url(doc_pk,part_pk)\n",
    "  part = get_item(part_url)\n",
    "  if overwrite:\n",
    "    data={'comments':comments}\n",
    "  else:\n",
    "    data={'comments':part['comments'] + comments}\n",
    "  res = requests.patch(part_url,data=json.dumps(data), headers=headers)\n",
    "  return res.status_code\n",
    "\n",
    "# function-endpoints------------------------------------------------------------------------------\n",
    "\n",
    "def import_xml(doc_pk,dirname,fname,tr_level_name,override = False):\n",
    "  \"\"\"\n",
    "  e.g. import_xml(3221,r'/content/','export_doc3221_trial_alto_202302231124.zip','AT')\n",
    "  \"\"\"\n",
    "  data={'name':tr_level_name,'document':doc_pk, 'task':'import-xml'}\n",
    "  if override:\n",
    "    data['override'] = True\n",
    "  file = os.path.join(dirname, fname)\n",
    "  import_url=get_specific_doc_url(doc_pk)+'imports/'\n",
    "  with open(file,'rb') as fh:\n",
    "    file_handler={'upload_file':fh}\n",
    "    res=requests.post(import_url,headers=headersbrief,data=data,files=file_handler)\n",
    "    return res\n",
    "\n",
    "\n",
    "def export_xml(doc_pk,part_pk_list,tr_level_pk,region_type_pk_list,include_undefined = True, include_orphan = True, file_format = 'alto',include_images = False, print_status = True):\n",
    "  export_url = get_specific_doc_url(doc_pk)+'export/' # e.g. https://escriptorium.openiti.org/api/documents/3221/export/\n",
    "  if include_undefined:\n",
    "    region_type_pk_list += 'Undefined'\n",
    "  if include_orphan:\n",
    "    region_type_pk_list += 'Orphan'\n",
    "  data = {'parts': part_pk_list, 'transcription': tr_level_pk,'task': 'export','region_types':region_type_pk_list,'include_images':include_images,'file_format':file_format}\n",
    "  #e.g. {\"parts\": [755434], \"transcription\": 5631, \"task\": \"export\", \"region_types\": [2,'Undefined','Orphan'], \"include_images\" : False, \"file_format\": \"alto\"}\n",
    "  res = requests.post(export_url,data=data,headers=headersbrief)\n",
    "  if print_status:\n",
    "    print(res.status_code)\n",
    "    if round(res.status_code/100,0) != 2:\n",
    "      print(res.content)\n",
    "  return res\n",
    "\n",
    "def segment(doc_pk,segmodel_pk,part_pk_list,steps = 'both',override = True,print_status = True):\n",
    "  if not(steps in ['lines','both','regions','masks']):\n",
    "    print('steps are not valid')\n",
    "    juststop\n",
    "  segment_url = get_segmentation_url(doc_pk)\n",
    "  data = {'parts':part_pk_list,'model':segmodel_pk,'steps':steps,'override':override}\n",
    "  r = requests.post(segment_url,data = json.dumps(data),headers = headers)\n",
    "  if print_status:\n",
    "    print(r.status_code)\n",
    "  return r\n",
    "\n",
    "def transcribe(doc_pk,trans_model_pk,part_pk_list,tr_level,print_status = False):\n",
    "  trans_url = get_transcribe_url(doc_pk)\n",
    "  # '.../api/documents/{DOC_PK}/transcribe/'\n",
    "  data = {'parts':part_pk_list,'model':trans_model_pk,'transcription':tr_level}\n",
    "  #{parts: [1, 2], model: 1} {headers only with authorization}\n",
    "  r = requests.post(trans_url,data = json.dumps(data),headers = headers)\n",
    "  if print_status:\n",
    "    print(r.status_code)\n",
    "  return r\n",
    "\n",
    "def txt2img_alignment(doc_pk,part_pk_list,trans_model_pk,tr_level):\n",
    "  doc_url = get_documents_url()\n",
    "  align_url = doc_url+str(doc_pk)+'/forced_align/'\n",
    "  if part_pk_list=='':\n",
    "    jsondata = {'model': trans_model_pk, 'transcription' : tr_level}\n",
    "  else:\n",
    "    jsondata = {'parts': part_pk_list, 'model': trans_model_pk, 'transcription' : tr_level}\n",
    "  r = requests.post(url=align_url,headers=headers,data=json.dumps(jsondata))\n",
    "  return r\n",
    "\n",
    "def move_part(doc_pk,part_pk,place):\n",
    "    print('move part_pk '+str(part_pk)+'to place '+str(place))\n",
    "    url=get_specific_part_url(doc_pk,part_pk)+'move/'\n",
    "    data='{\"index\":'+str(place)+'}'\n",
    "    res=requests.post(url,headers=headers,data=data)\n",
    "    return res\n",
    "\n",
    "def crop_part_img(doc_pk,part_pk,top,left,bottom,right,print_status = False):\n",
    "  data = json.dumps({'x1':left, 'y1':top, 'x2' : right, 'y2':bottom})\n",
    "  crop_url=get_specific_part_url(doc_pk,part_pk)+'crop/'\n",
    "  r = requests.post(url = crop_url,headers = headers,data = data)\n",
    "  if print_status:\n",
    "    print(r.status_code)\n",
    "  return r\n",
    "\n",
    "def rotate_img(doc_pk,part_pk,angle,print_status = False):\n",
    "  rotate_url=get_specific_part_url(doc_pk,part_pk)+'rotate/'\n",
    "  data={'angle':angle}\n",
    "  res=requests.post(url=rotate_url,headers=headers,data=json.dumps(data))\n",
    "  if print_status:\n",
    "    print(res.status_code)\n",
    "  return res\n",
    "\n",
    "def repolygonize(doc_pk,part_pk, print_status= False):\n",
    "    print('repolygonize : ',part_pk)\n",
    "    url_repoly=get_specific_part_url(doc_pk,part_pk)+'reset_masks/'\n",
    "    res=requests.post(url_repoly,headers=headers)\n",
    "    if print_status:\n",
    "      print(res.status_code)\n",
    "    return res\n",
    "\n",
    "def repolygonize_line(doc_pk,part_pk,line_pk):\n",
    "    print('repolygonize line : ',line_pk)\n",
    "    url_repoly=get_specific_part_url(doc_pk,part_pk)+'reset_masks/?only='+str(line_pk)\n",
    "    r=requests.post(url_repoly,headers=headers)\n",
    "    return r\n",
    "\n",
    "def reorder(doc_pk,part_pk,print_status = False):\n",
    "    print('reorder ',part_pk)\n",
    "    reorder_url=get_specific_part_url(doc_pk,part_pk)+'recalculate_ordering/'\n",
    "    res = requests.post(reorder_url,headers=headers)\n",
    "    if print_status:\n",
    "      print(res.status_code)\n",
    "    return res\n",
    "\n",
    "def train(doc_pk,part_pk_list,model_name,tr_level_pk,on_top_model_pk,override = False):\n",
    "  train_url = get_specific_doc_url(doc_url)+'train/'\n",
    "  data = {'parts': part_pk_list,'model': on_top_model_pk, 'model_name': model_name,'transcription': tr_level_pk,'override': override  }\n",
    "  r=requests.post(url= train_url,data=json.dumps(data),headers=headers)\n",
    "  return r\n",
    "\n",
    "def segtrain(doc_pk,part_pk_list,model_name,on_top_model_pk,override = False):\n",
    "  segtrain_url = get_specific_doc_url(doc_url)+'segtrain/'\n",
    "  data = {'parts': part_pk_list,'model': on_top_model_pk, 'model_name': model_name,'override': override  }\n",
    "  r=requests.post(url= segtrain_url,data=json.dumps(data),headers=headers)\n",
    "  return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1742386967927,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "CbryrfBIYuN0"
   },
   "outputs": [],
   "source": [
    "#@title complex\n",
    "#%%writefile complex_api_functions.py\n",
    "\n",
    "# find_and_replace_text_or_trim, transfer_txt2level, get_pages_without_transcription_level, get_pages_with_wordstring_in_transcription_level,\n",
    "# delete_linefree_regions, keep_only_biggest_region, associate_lines_with_existing_regions, delete_unlinked_lines, delete_1p_lines,\n",
    "# find_lines_without_mask, extend_lines, create_transcription_table, delete_empty_lines\n",
    "\n",
    "def associate_line_with_existing_region(doc_pk,part_pk,line):\n",
    "  r = None\n",
    "  centroidx,centroidy=get_line_mid_point(line)\n",
    "  p = Point(centroidx, centroidy)\n",
    "\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  for region in regions:\n",
    "    region['poly'] = Polygon(region['box'])\n",
    "    if p.within(region['poly']):\n",
    "      line['region']=region['pk']\n",
    "      r = update_line(doc_pk,part_pk,line)\n",
    "      break\n",
    "  return r\n",
    "\n",
    "def calculate_average_line_distance(lines,pageheight,pagewidth):\n",
    "\n",
    "  average_line_height_list=[]\n",
    "  midpointList = [get_line_mid_point(line) for line in lines]\n",
    "  print(midpointList)\n",
    "  lineBegList = [get_line_beg_point(line) for line in lines]\n",
    "  lineEndList = [get_line_end_point(line) for line in lines]\n",
    "  d_list=[]\n",
    "  for n,midpoint in enumerate(midpointList):\n",
    "    if n>0:\n",
    "      p1=array(lineBegList[n-1])\n",
    "      p2=array(lineEndList[n-1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append(-cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
    "    elif len(midpointList)>1:\n",
    "      p1=array(lineBegList[n+1])\n",
    "      p2=array(lineEndList[n+1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append((cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
    "    average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
    "    average_line_height_list.append(average_line_height)\n",
    "    print(average_line_height)\n",
    "  if average_line_height_list:\n",
    "    return int(round(mean(average_line_height_list)))\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "def get_average_line_distance(lines):\n",
    "  nu_lines = len(lines)\n",
    "  if nu_lines>1:\n",
    "    distmapsize = (nu_lines-1,nu_lines-1)\n",
    "    distmap = numpy.ones(distmapsize,dtype = int)*1000000\n",
    "    for i1,line1 in enumerate(lines):\n",
    "      linestr1 = LineString(line1['baseline'])\n",
    "      for i2,line2 in enumerate(lines[i1+1:]):\n",
    "        linestr2 = LineString(line2['baseline'])\n",
    "        distmap[i1-1][i1+i2] = int(round(linestr1.distance(linestr2),0))\n",
    "    row_ind, col_ind = linear_sum_assignment(distmap)\n",
    "    dist_sum=0\n",
    "    for r,c in zip(row_ind,col_ind):\n",
    "      dist_sum+=distmap[r][c]\n",
    "    return int(round(dist_sum/nu_lines,0))\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "\n",
    "def simplify_line_polygons_on_part(doc_pk,part_pk,simplification=10):\n",
    "  lines_url=get_lines_url(doc_pk,part_pk)\n",
    "  lines=loop_through_itempages_and_get_all(lines_url)\n",
    "  for line in lines:\n",
    "    mask=line['mask']\n",
    "    #print(len(mask),mask)\n",
    "    maskPoly=Polygon(mask)\n",
    "    maskSimple=maskPoly.simplify(10)\n",
    "    #print(maskSimple)\n",
    "    x,y = maskSimple.exterior.coords.xy\n",
    "    new_mask = [list(x) for x in zip(x,y)]\n",
    "    #print(len(new_mask),new_mask)\n",
    "    line_url = lines_url+str(line['pk'])+'/'\n",
    "    line_json={'mask' : new_mask}\n",
    "    res_line = requests.patch(line_url,headers=headers,data=json.dumps(line_json))\n",
    "\n",
    "def calculate_CER(doc_pk,part_pk,master_tr,CER_tr,get_lines):\n",
    "  master_trs = get_page_transcription(doc_pk,part_pk,master_tr)\n",
    "  CER_trs = get_page_transcription(doc_pk,part_pk,CER_tr)\n",
    "\n",
    "  cumul_dist = 0\n",
    "  cumul_len = 0\n",
    "  comparison_table =[]\n",
    "  for mtr in master_trs:\n",
    "    ctr = search_any('line',mtr['line'],CER_trs)\n",
    "    mtr_line_len = len(mtr['content'])\n",
    "    included = False\n",
    "    lev_dist = None\n",
    "    ctr_content = None\n",
    "    ctr_pk = None\n",
    "    if len(ctr)==1:\n",
    "        ctr_content = ctr[0]['content']\n",
    "        ctr_pk = ctr[0]['pk']\n",
    "    if (mtr_line_len>0) and (len(ctr)==1):\n",
    "      included = True\n",
    "      lev_dist = Levenshtein.distance(mtr['content'],ctr_content)\n",
    "      cumul_dist += lev_dist\n",
    "      cumul_len += mtr_line_len\n",
    "    if get_lines:\n",
    "      comparison_table.append({'line_pk':mtr['line'],\n",
    "                               'mtr_pk':mtr['pk'],\n",
    "                               'mtr_content':mtr['content'],\n",
    "                               'ctr_pk':ctr_pk,\n",
    "                               'ctr_content':ctr_content,\n",
    "                               'levenshtein_dist':lev_dist,\n",
    "                               'mtr_line_len':mtr_line_len,\n",
    "                               'included':included})\n",
    "  if get_lines:\n",
    "    for ctr in CER_trs:\n",
    "      mtr_cands = search_any('line_pk',ctr['line'],comparison_table)\n",
    "      if len(ctr)==0:\n",
    "        comparison_table.append({'line_pk':mtr['line'],\n",
    "                                'mtr_pk':None,\n",
    "                                'mtr_content':None,\n",
    "                                'ctr_pk':ctr['pk'],\n",
    "                                'ctr_content':ctr['content'],\n",
    "                                'levenshtein_dist':lev_dist,\n",
    "                                'mtr_line_len':None,\n",
    "                                'included':included})\n",
    "    return cumul_dist,cumul_len,comparison_table\n",
    "  else:\n",
    "    return cumul_dist,cumul_len\n",
    "\n",
    "def find_and_replace_chars(doc_pk,part_pk,tr_level,chars,replacement_char,do_replace):\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  if do_replace:\n",
    "    print('replacing '+chars+' with '+replacement_char)\n",
    "    for m,line_tr in enumerate(transcriptions):\n",
    "      oldstr=line_tr['content']\n",
    "      if (any((c in chars) for c in oldstr)):\n",
    "        content=line_tr['content']\n",
    "        content=content.replace(chars,replacement_char)\n",
    "        if not(oldstr==content):\n",
    "          line_tr['content']=content\n",
    "          tr_url = get_specific_transcription_url(doc_pk,part_pk,line_tr['pk'])\n",
    "          res_patch=requests.patch(tr_url,headers=headers,data=json.dumps(line_tr['content']))\n",
    "    return ''\n",
    "  else:\n",
    "    print('find pages with '+chars)\n",
    "    chars=set(chars)\n",
    "    for m,line_tr in enumerate(transcriptions):\n",
    "      linestr=line_tr['content']\n",
    "      if any((c in chars) for c in linestr):\n",
    "        print('have a look for character in ',part_pk,' line :',m,)\n",
    "    return part_pk\n",
    "\n",
    "def replace_string_or_trim(doc_pk,part_pk,tr_level,search_str,replacement_str,do_replace,do_trim=False):\n",
    "  if do_replace:\n",
    "    print('replacing '+search_str+' with '+replacement_str)\n",
    "  if do_trim:\n",
    "    print('trimming')\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  for line_tr in transcriptions:\n",
    "    oldstr = line_tr['content']\n",
    "    if do_trim:\n",
    "      line_tr['content'] = line_tr['content'].strip()\n",
    "    if do_replace:\n",
    "      line_tr['content'] = replace_substr_in_str(line_tr['content'],search_str,replacement_str)\n",
    "    if not(oldstr == tr['content']):\n",
    "      tr_url = get_specific_transcription_url(doc_pk,part_pk,line_tr['pk'])\n",
    "      res_patch=requests.patch(tr_url,headers=headers,data=json.dumps(line_tr['content']))\n",
    "\n",
    "def transfer_txt2level(doc_pk,part_pk,source_tr_level,target_tr_level,overwrite):\n",
    "  target_tr_url=get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "  source_transcriptions = get_page_transcription(doc_pk,part_pk,source_tr_level)\n",
    "  target_transcriptions = get_page_transcription(doc_pk,part_pk,target_tr_level)\n",
    "  target_tr_line_pks = get_sthg_from_dict_list(target_transcriptions,'line') # create list of existing target transcription line pks.\n",
    "  post_transcriptions = []\n",
    "  for line_tr in source_transcriptions:\n",
    "    line_pk = line_tr['line']\n",
    "    data = {'line': line_pk, 'transcription': target_tr_level, 'content': line_tr['content']}\n",
    "    if line_pk in target_tr_line_pks: # check if exists\n",
    "      if overwrite:\n",
    "        updated_data = search_any('line',line_pk,target_transcriptions)[0]\n",
    "        updated_data['content'] = data['content']\n",
    "        post_transcriptions.append(updated_data)\n",
    "    else:\n",
    "      post_transcriptions.append(data)\n",
    "  bulk_update_transcriptions(doc_pk,part_pk,post_transcriptions)\n",
    "\n",
    "def get_lines_without_transcription_level(doc_pk,part_pk,tr_level,also_empty_lines=False):\n",
    "  print('check pages without transcription in transcription level for pk', part_pk)\n",
    "  line_pks = set(get_line_pk_list(doc_pk,part_pk))\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  if also_empty_lines:\n",
    "    transcriptions = [tr for tr in transcriptions if len(tr['content'].strip())>0]\n",
    "  tr_line_pks = set(get_sthg_from_dict_list(transcriptions,'line')) # create set of existing transcription line pks.\n",
    "  missing=line_pks.difference(tr_line_pks)\n",
    "  return missing\n",
    "\n",
    "def find_or_delete_linefree_regions(doc_pk,part_pk,do_delete=False):\n",
    "  inhabited_region_list=set(get_inhabited_region_pk_list(doc_pk,part_pk))\n",
    "  region_list=set(get_region_pk_list(doc_pk,part_pk))\n",
    "  uninhabited_region_list=region_list.difference(inhabited_region_list)\n",
    "  for region in uninhabited_region_list:\n",
    "    if do_delete:\n",
    "      delete_region(doc_pk,part_pk,region)\n",
    "      print('found and deleted linefree region:',region)\n",
    "    else:\n",
    "      print('found linefree region:',region)\n",
    "  if not(do_delete):\n",
    "    return uninhabited_region_list\n",
    "\n",
    "\n",
    "def keep_only_n_biggest_regions(doc_pk,part_pk,n):\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  for region in regions:\n",
    "    region['area'] = Polygon(region['box']).area\n",
    "  sorted_regions = sorted(regions,key=lambda i: i['area'], reverse = True)\n",
    "  if len(sorted_regions)>n:\n",
    "    for region in sorted_regions[n:]:\n",
    "      print('delete ',region['pk'])\n",
    "      delete_region(doc_pk,part_pk,region['pk'])\n",
    "\n",
    "def associate_lines_with_existing_regions_and_reorder(doc_pk,part_pk):\n",
    "  print(part_pk)\n",
    "  line_url=get_lines_url(doc_pk,part_pk)\n",
    "  region_url=get_regions_url(doc_pk,part_pk)\n",
    "  lines=loop_through_itempages_and_get_all(line_url)\n",
    "  regions=loop_through_itempages_and_get_all(region_url)\n",
    "  nu_regions=len(regions)\n",
    "  region_pk_list = [item['pk'] for item in regions]\n",
    "  updated_lines=list()\n",
    "  for region in regions:\n",
    "    region['poly'] = Polygon(region['box'])\n",
    "  #print('associating')\n",
    "  for n,line in enumerate(lines):\n",
    "    baseline=line['baseline']\n",
    "    centroidx,centroidy=get_centroid(baseline)\n",
    "    p = Point(centroidx, centroidy)\n",
    "    for region in regions:\n",
    "      if p.within(region['poly']) and not(line['region']==region['pk']):\n",
    "        line['region']=region['pk']\n",
    "        updated_lines.append(line)\n",
    "        break\n",
    "  if len(updated_lines)>0:\n",
    "    print('associating lines')\n",
    "    print(updated_lines)\n",
    "    bulk_update_lines(doc_pk,part_pk,updated_lines)\n",
    "    print('reordering')\n",
    "    reorder(doc_pk,part_pk)\n",
    "  else:\n",
    "    print('nothing to associate')\n",
    "\n",
    "def find_or_delete_unlinked_lines(doc_pk,part_pk,do_delete=False):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  lines2delete = list()\n",
    "  for line_n,line in enumerate(lines):\n",
    "    if line['region']==None:\n",
    "      print('found unassociated line in doc',doc_pk,', on part',part_pk,', line:',line_n)\n",
    "      if do_delete:\n",
    "        lines2delete.append(line['pk'])\n",
    "  if len(lines2delete)>0:\n",
    "    bulk_delete_lines(doc_pk,part_pk,lines2delete)\n",
    "    reorder(doc_pk,part_pk)\n",
    "\n",
    "def delete_1p_lines(doc_pk,part_pk):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  lines2delete = list()\n",
    "  for line_n,line in enumerate(lines):\n",
    "    if (len(line['baseline'])<2):\n",
    "      print('need to delete line')\n",
    "      lines2delete.append(line['pk'])\n",
    "  if len(lines2delete)>0:\n",
    "    bulk_delete_lines(doc_pk,part_pk,lines2delete)\n",
    "    reorder(doc_pk,part_pk)\n",
    "\n",
    "def find_lines_without_mask(doc_pk,part_pk):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  maskless_lines = list()\n",
    "  for line_n,line in enumerate(lines):\n",
    "    try:\n",
    "      len_mask=len(line['mask'])\n",
    "    except:\n",
    "      len_mask=0\n",
    "    if len_mask<4:\n",
    "      print('line in doc',doc_pk,'part',part_pk,'line',line_n,'with only',len_mask,'points')\n",
    "      maskless_lines.append(line)\n",
    "  return maskless_lines\n",
    "\n",
    "def extend_lines(doc_pk,part_pk,extension=15,left_also=False,baseline2topline=False,y_decrease=10,repoly=True):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  pageheight,pagewidth=page_height_width(doc_pk,part_pk)\n",
    "  for line in lines:\n",
    "    baseline=line['baseline']\n",
    "    line_id=line['pk']\n",
    "    if baseline[0][0]<baseline[-1][0]:\n",
    "      if left_also:\n",
    "        baseline[0][0]=max(5,int(round((baseline[0][0])))-extension)\n",
    "      baseline[-1][0]=min(int(round((baseline[-1][0])))+extension,pagewidth-5)\n",
    "    else:\n",
    "      if left_also:\n",
    "        baseline[0][0]=min(int(round((baseline[0][0])))+extension,pagewidth-5)\n",
    "      baseline[-1][0]=max(5,int(round((baseline[-1][0])))-extension)\n",
    "    if baseline2topline:\n",
    "      baseline=[[pt[0], max(5,pt[1]-y_decrease)] for pt in baseline]\n",
    "    line['baseline'] = baseline\n",
    "  bulk_update_lines(doc_pk,part_pk,lines)\n",
    "  if repoly:\n",
    "    repolygonize(doc_pk,part_pk)\n",
    "\n",
    "def create_transcription_table(doc_pk,part_list,tr_level):\n",
    "  pages=[]\n",
    "  regions=[]\n",
    "  line_pks=[]\n",
    "  transcription_pks=[]\n",
    "  linenumbers=[]\n",
    "  txt=[]\n",
    "  regions=[]\n",
    "  for n,part_pk in enumerate(part_list):\n",
    "\n",
    "    if n % 10 == 0:\n",
    "      print(n)\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "    lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "    line_pk_list_this_transcription = [tr.get('line') for tr in transcriptions]\n",
    "    line_pk_list_this_part = [line.get('pk') for line in lines]\n",
    "    line2region_list_this_part = [line.get('region') for line in lines]\n",
    "    region_pk_list_this_part = [region.get('pk') for region in regions]\n",
    "\n",
    "    for line in lines:\n",
    "      pages.append(n+1)\n",
    "      line_pks.append(line['pk'])\n",
    "      linenumbers.append(line['order']+1)\n",
    "      try:\n",
    "        regions.append(region_pk_list_this_part.index(line['region'])+1)\n",
    "      except:\n",
    "        regions.append(None)\n",
    "      try:\n",
    "        index=line_pk_list_this_transcription.index(line['pk'])\n",
    "        transcription_pks.append(transcriptions[index]['pk'])\n",
    "        txt.append(transcriptions[index]['content'])\n",
    "      except:\n",
    "        transcription_pks.append(None)\n",
    "        txt.append('')\n",
    "  print('done')\n",
    "\n",
    "  columntitles = ['page', 'region', 'line','line_pk', 'transcription_pk','txt']\n",
    "  data =[columntitles] + list(zip(pages, regions, linenumbers,line_pks,transcription_pks,txt))\n",
    "\n",
    "  for i, d in enumerate(data):\n",
    "    tableline = ' | '.join(str(x).ljust(4) for x in d)\n",
    "    print(tableline)\n",
    "    if i == 0:\n",
    "      print('-' * len(tableline))\n",
    "\n",
    "def delete_empty_lines(doc_pk,part_pk,tr_level,min_length=1):\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  line_pks = get_pks_of_dict_list(lines)\n",
    "  lines2delete = list()\n",
    "  for tr in transcriptions:\n",
    "    if len(tr['content'])<min_length:\n",
    "      lines2delete.append(tr['line'])\n",
    "    line_pks.remove(tr['line'])\n",
    "  for line_pk in line_pks:\n",
    "    lines2delete.append(line_pk)\n",
    "  if len(lines2delete)>0:\n",
    "    bulk_delete_lines(doc_pk,part_pk,lines2delete,print_status = True)\n",
    "\n",
    "def calculate_average_line_distance(baselineCoordsList,pageheight,pagewidth):\n",
    "  average_line_height_list=[]\n",
    "  midpointList = [[(line[0][0]+line[-1][0])/2,(line[0][1]+line[-1][1])/2] for line in baselineCoordsList]\n",
    "  lineBegList = [line_intersection([[0,0],[0,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
    "  lineEndList = [line_intersection([[pagewidth,0],[pagewidth,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
    "  d_list=[]\n",
    "  for n,midpoint in enumerate(midpointList):\n",
    "    if n>0:\n",
    "      p1=array(lineBegList[n-1])\n",
    "      p2=array(lineEndList[n-1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append(cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
    "    elif len(midpointList)>1:\n",
    "      p1=array(lineBegList[n+1])\n",
    "      p2=array(lineEndList[n+1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append(-(cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
    "    average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
    "    average_line_height_list.append(average_line_height)\n",
    "  if average_line_height_list:\n",
    "    return int(round(mean(average_line_height_list)))\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "def restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,average_line_distance=0):\n",
    "\n",
    "  def repoly_extreme_line(line,direction):\n",
    "    lines_url = get_lines_url(doc_pk,part_pk)\n",
    "    # create dummy baseline above or below to limit extension of first or last line\n",
    "    phantombaseline=[[pt[0], min(pageheight-5,max(5,pt[1]+(average_line_distance+5)*direction))] for pt in line[2]]\n",
    "    phantom_data=json.dumps({'document_part':part_pk,'region':region_pk,'baseline':phantombaseline})\n",
    "    phantom_line_pk=requests.post(lines_url,headers=headers,data=phantom_data).json()['pk']\n",
    "    # repolygonize\n",
    "    res=repolygonize_line(doc_pk,part_pk,line[0])\n",
    "    print('repoly-response: ',res)\n",
    "    # delete dummy line\n",
    "    phantom_line_del=delete_line(doc_pk,part_pk,phantom_line_pk,headers=headers)\n",
    "  print(average_line_distance)\n",
    "  pageheight,pagewidth=page_height_width(doc_pk,part_pk)\n",
    "  all_lines_this_region=get_all_lines_of_region(doc_pk,part_pk,region_pk)\n",
    "# main lines only\n",
    "  main_lines_this_region =[line for line in all_lines_this_region if line['typology'==None]]\n",
    "  baselineCoordsList=[line[2] for line in main_lines_this_region]\n",
    "# calculate average line distance\n",
    "  if average_line_distance==0:\n",
    "    average_line_distance=calculate_average_line_distance(baselineCoordsList,pageheight,pagewidth)\n",
    "  if (average_line_distance>5) and (len(main_lines_this_region)>0):\n",
    "# treat first line\n",
    "    index_first = min(lines[1] for lines in main_lines_this_region)\n",
    "    line_first=[line for line in main_lines_this_region if line[1]==index_first]\n",
    "    line_first=line_first[0]\n",
    "    repoly_extreme_line(line_first,-1)\n",
    "# treat last line\n",
    "    index_last = max(lines[1] for lines in main_lines_this_region)\n",
    "    if not(index_last==index_first):\n",
    "      line_last=[line for line in main_lines_this_region if line[1]==index_last]\n",
    "      line_last=line_last[0]\n",
    "      repoly_extreme_line(line_last,1)\n",
    "      print('repolygonized extremes of part: ',part_pk)\n",
    "  else:\n",
    "    print('not able to calculate average line distance for part: ',part_pk)\n",
    "  return average_line_distance\n",
    "\n",
    "def create_normalized_polygons_around_typelist_lines(doc_pk,part_pk,regiontypelist,linetypelist,include_none_linetype=True,ascender=12,descender=24,safetydistance=5):\n",
    "  if include_none_linetype:\n",
    "    check_none=None\n",
    "  else:\n",
    "    check_none=False\n",
    "  pageheight,pagewidth=get_page_height_width(doc_pk,part_pk)\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  regions=get_all_regions_of_part(doc_pk,part_pk)\n",
    "  selected_region_pks = [region['pk'] for region in regions if region['typology'] in regiontypelist]\n",
    "  selected_lines = [line for line in lines if ((line['typology'] in linetypelist) or (line['typology']==check_none)) and line['region'] in selected_region_pks]\n",
    "  for line in selected_lines:\n",
    "    baseline = line['baseline']\n",
    "    old_pt=baseline[0]\n",
    "    for o,pt in enumerate(baseline[1:]):\n",
    "      angle=get_angle([old_pt,pt])\n",
    "      if o==0:\n",
    "        inside_above=False\n",
    "        inside_below=False\n",
    "        new_pt_above=rotateNP((old_pt[0],old_pt[1]-ascender),origin=tuple(old_pt),degrees=angle)\n",
    "        new_pt_below=rotateNP((old_pt[0],old_pt[1]+descender),origin=tuple(old_pt),degrees=angle)\n",
    "        new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "        new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "        polygon_above=[[int(coordinate) for coordinate in new_pt_above]]\n",
    "        polygon_below=[[int(coordinate) for coordinate in new_pt_below]]\n",
    "      new_pt_above=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]-ascender),origin=tuple(pt),degrees=angle)]\n",
    "      new_pt_below=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]+descender),origin=tuple(pt),degrees=angle)]\n",
    "      new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "      new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "      if o>0:\n",
    "        # if point is inside new mask and is not the last point, delete previous point, dont insert next point either but insert instead the intersection of (p-2:p-1) and (p:p+1)\n",
    "        if inside_above and not(o==len(line[1:])):\n",
    "          replace_pt=list(line_intersection([polygon_above[-3],polygon_above[-2]],[polygon_above[-1],new_pt_above]))\n",
    "          del polygon_above[-2:-1]\n",
    "          polygon_above.append(replace_pt)\n",
    "        if inside_below:\n",
    "          replace_pt=list(line_intersection([polygon_below[2],polygon_below[1]],[polygon_below[0],new_pt_below]))\n",
    "          del polygon_below[0:1]\n",
    "          polygon_below.insert(0,replace_pt)\n",
    "        pol=Polygon(polygon_above+polygon_below)\n",
    "        inside_above=pol.contains(Point(new_pt_above))\n",
    "        inside_below=pol.contains(Point(new_pt_below))\n",
    "      polygon_above.append(new_pt_above) # if point is outside add it to boundary\n",
    "      polygon_below.insert(0,new_pt_below) # if point is outside add it to boundary\n",
    "      old_pt=pt\n",
    "    if inside_above:\n",
    "      del polygon_above[-1] # if point is inside and is last point delete it\n",
    "    if inside_below:\n",
    "      del polygon_below[0] # if point is inside and is last point delete it\n",
    "    polygon_boundary=[baseline[0]]+polygon_above+[baseline[-1]]+polygon_below\n",
    "    line['mask']=polygon_boundary\n",
    "  return bulk_update_lines(doc_pk,part_pk,selected_lines)\n",
    "\n",
    "\n",
    "def simplify_hebrew(doc_nu,part_list,source_tr_level,target_tr_level):\n",
    "    vocalization=[1425,1427,1430,1436,1438,1443,1446,1453,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1471,1473,1474,1476,1477,1478,1479]\n",
    "    singlequot=[1523,8216,8217,8219,8242]\n",
    "    doublequot=[1524,8220,8221,8222,8223,8243]\n",
    "    spacechars=[9,160,8201,8195,8192]\n",
    "    deletechars=[1565,8299,8205,8300,8302]\n",
    "    toreplacechars=[(64296,1514),(64298,1513),(64299,1513),(64300,1513),(64302,1488),(64303,1488),(64305,1489),(64306,1490),(64307,1491),(64309,1493),(64315,1499),(64324,1508),(64327,1511),(64330,1514),(64331,1493),(64332,1489),(64333,1499),(64334,1508),(8277,42),(8283,46),(11799,61)]\n",
    "    for n,part in enumerate(part_list[41:]):\n",
    "        target_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'\n",
    "        #https://www.escriptorium.fr/api/documents/39/parts/14480/transcriptions/?page=2&transcription=46\n",
    "        page_nu=0\n",
    "        while True:\n",
    "            page_nu+=1\n",
    "            source_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(source_tr_level)\n",
    "            target_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(target_tr_level)\n",
    "            res = requests.get(source_tr_url, headers=headers).json()\n",
    "            print(source_tr_url)\n",
    "            res_target=requests.get(target_tr_url, headers=headers).json()\n",
    "            if res_target!={'detail': 'Invalid page.'}:\n",
    "                target_line_pk_list = [line['line'] for line in res_target['results']]\n",
    "                target_tr_pk_list = [line['pk'] for line in res_target['results']]\n",
    "                print(target_line_pk_list)\n",
    "            else:\n",
    "                target_line_pk_list=[]\n",
    "                target_tr_pk_list=[]\n",
    "            #print(res_target)\n",
    "            for m,line in enumerate(res['results']):\n",
    "                #'line': 674381, 'transcription': 46, 'content': 'מדביק מרחם שערים הליכות מצויינים',\n",
    "                line_pk=line['line']\n",
    "                print(line_pk)\n",
    "                print(line['pk'])\n",
    "                content=line['content']\n",
    "                for u in vocalization:\n",
    "                    content=content.replace(chr(u),'')\n",
    "                for u in spacechars:\n",
    "                    content=content.replace(chr(u),' ')\n",
    "                for u in deletechars:\n",
    "                    content=content.replace(chr(u),'')\n",
    "                for u in singlequot:\n",
    "                    content=content.replace(chr(u),\"'\")\n",
    "                for u in doublequot:\n",
    "                    content=content.replace(chr(u),'\"')\n",
    "                for (x,y) in toreplacechars:\n",
    "                    content=content.replace(chr(x),chr(y))\n",
    "                data = {'line': line_pk, 'transcription': target_tr_level, 'content': content}\n",
    "                if line_pk in target_line_pk_list:\n",
    "                    t_url=(root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+str(line['pk'])+'/')\n",
    "                    res_t=requests.get(t_url,headers=headers).json()\n",
    "                    print('put',res_t['content'])\n",
    "                    res_t['content']=content\n",
    "                    print(t_url)\n",
    "                    data=json.dumps(res_t, ensure_ascii=False).encode('utf8')\n",
    "                    res_put = requests.put(t_url, data=data, headers=headers)\n",
    "                    #print('put',res_put.content)\n",
    "                else:\n",
    "                    res_post = requests.post(target_tr_url, data=json.dumps(data), headers=headers)\n",
    "                    #print(n,part,m,res_post.content)\n",
    "            if not(res['next']):\n",
    "                break\n",
    "    print('done simplifying hebrew')\n",
    "\n",
    "def delete_text_in_main_regions_except_line_type(doc_nu,part_list,tr_level,allowed_line_types):\n",
    "\n",
    "    for n,part_pk in enumerate(part_list):\n",
    "        print(n,part_pk)\n",
    "        #get all main regions\n",
    "        regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "        main_region_list=[r.get('pk') for r in regions['results'] if r['typology']==2]\n",
    "\n",
    "        #get all lines of this part that are in the main columns and of the default type and delete their content\n",
    "        page_nu=0\n",
    "        del_lines_pk_list=[]\n",
    "        lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "        for line in lines:\n",
    "          if (not(line['typology'] in allowed_line_types)) and (line['region'] in main_region_list):\n",
    "            del_lines_pk_list.append(line['pk'])\n",
    "        transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "        delete_tr_list = list()\n",
    "        for tr_line in transcriptions:\n",
    "          if tr_line['line'] in del_lines_pk_list:\n",
    "            delete_tr_list.append(tr_line['pk'])\n",
    "        bulk_delete_transcriptions(doc_pk,part_pk,delete_tr_list)\n",
    "\n",
    "def loop_document_restrict_extreme_lines_all_regions(doc_pk,fix_line_height,start_item=0):\n",
    "\n",
    "  part_pk_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_pk)\n",
    "  for part_pk in part_pk_list[start_item:]:\n",
    "    print('----------------------------------------------------')\n",
    "    print('resegmenting part :',part_pk)\n",
    "    region_list=get_region_pk_list(doc_pk,part_pk)\n",
    "    main_region_list=get_main_region_pk_list(doc_pk,part_pk)\n",
    "    average_line_height_in_main_regions=[]\n",
    "    for region_pk in main_region_list:\n",
    "      average_line_height_in_main_regions.append(restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,fix_line_height))\n",
    "    if len(main_region_list)>0:\n",
    "      global_average_line_height=mean(average_line_height_in_main_regions)\n",
    "    else:\n",
    "      global_average_line_height=fix_line_height\n",
    "    non_main_regions=set(region_list)-set(main_region_list)\n",
    "    for region_pk in non_main_regions:\n",
    "      line_height=restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,global_average_line_height)\n",
    "\n",
    "  print('done')\n",
    "\n",
    "def create_normalized_polygons_around_lines(doc_nu,splitfactor=0.6,safetydistance=5,basic_line_height=120):\n",
    "\n",
    "    part_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_nu)\n",
    "\n",
    "    for n,part in enumerate(part_list):\n",
    "        parts_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/')\n",
    "        res = requests.get(parts_url,headers=headers).json()\n",
    "        pagewidth=res['image']['size'][0]\n",
    "        pageheight=res['image']['size'][1]\n",
    "        print(parts_url)\n",
    "        ## first loop: calculate average line height and distance\n",
    "        regionList = [region for region in res['regions']]\n",
    "        average_line_height_list=[]\n",
    "        average_line_height_not_null=[]\n",
    "        for m,region in enumerate(regionList):\n",
    "            baselineCoordsList = [line['baseline'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
    "            midpointList = [[(line[0][0]+line[-1][0])/2,(line[0][1]+line[-1][1])/2] for line in baselineCoordsList]\n",
    "            lineBegList = [line_intersection([[0,0],[0,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
    "            lineEndList = [line_intersection([[pagewidth,0],[pagewidth,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
    "            d_list=[]\n",
    "            for n,midpoint in enumerate(midpointList):\n",
    "                if n>0:\n",
    "                    p1=array(lineBegList[n-1])\n",
    "                    p2=array(lineEndList[n-1])\n",
    "                    p3=array(midpoint)\n",
    "                    d_list.append(cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
    "                elif len(midpointList)>1:\n",
    "                    p1=array(lineBegList[n+1])\n",
    "                    p2=array(lineEndList[n+1])\n",
    "                    p3=array(midpoint)\n",
    "                    d_list.append(-(cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
    "            if not(d_list):\n",
    "                print('one line region')\n",
    "                average_line_height_list.append(0)\n",
    "            else:\n",
    "                average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
    "                average_line_height_list.append(average_line_height)\n",
    "                average_line_height_not_null.append(average_line_height)\n",
    "        print(average_line_height_list)\n",
    "        print(average_line_height_not_null)\n",
    "        if not(average_line_height_not_null):\n",
    "            global_average_line_height=basic_line_height\n",
    "            print('bad average line height')\n",
    "        else:\n",
    "            global_average_line_height=int(round(sum(average_line_height_not_null)/len(average_line_height_not_null),0))\n",
    "            print(global_average_line_height)\n",
    "\n",
    "        for m,region in enumerate(regionList):\n",
    "            if average_line_height_list[m]==0:\n",
    "                this_line_height=global_average_line_height\n",
    "            else:\n",
    "                this_line_height=average_line_height_list[m]\n",
    "            baselineCoordsList = [line['baseline'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
    "            baselinePKList = [line['pk'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
    "            for n,line in enumerate(baselineCoordsList):\n",
    "                old_pt=line[0]\n",
    "                for o,pt in enumerate(line[1:]):\n",
    "                    #print('region '+str(m)+', line '+str(n)+', point '+str(o)+str(old_pt)+':'+str(pt))\n",
    "                    angle=get_angle([old_pt,pt])\n",
    "                    if o==0:\n",
    "                        inside_above=False\n",
    "                        inside_below=False\n",
    "                        new_pt_above=rotateNP((old_pt[0],old_pt[1]-this_line_height*splitfactor),origin=tuple(old_pt),degrees=angle)\n",
    "                        new_pt_below=rotateNP((old_pt[0],old_pt[1]+this_line_height*(1-splitfactor)),origin=tuple(old_pt),degrees=angle)\n",
    "                        #print(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "                        new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "                        new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "                        #print(new_pt_above)\n",
    "                        polygon_above=[[int(coordinate) for coordinate in new_pt_above]]\n",
    "                        polygon_below=[[int(coordinate) for coordinate in new_pt_below]]\n",
    "                    new_pt_above=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]-this_line_height*splitfactor),origin=tuple(pt),degrees=angle)]\n",
    "                    new_pt_below=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]+this_line_height*(1-splitfactor)),origin=tuple(pt),degrees=angle)]\n",
    "                    new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "                    new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "                    if o>0:\n",
    "                        if inside_above and not(o==len(line[1:])): # if point is inside and is not the last point, # delete previous point, dont insert next point either # and insert instead the intersection of (p-2:p-1) and (p:p+1)\n",
    "                            replace_pt=list(line_intersection([polygon_above[-3],polygon_above[-2]],[polygon_above[-1],new_pt_above]))\n",
    "                            del polygon_above[-2:-1]\n",
    "                            polygon_above.append(replace_pt)\n",
    "                        if inside_below:\n",
    "                            replace_pt=list(line_intersection([polygon_below[2],polygon_below[1]],[polygon_below[0],new_pt_below]))\n",
    "                            del polygon_below[0:1]\n",
    "                            polygon_below.insert(0,replace_pt)\n",
    "                        pol=Polygon(polygon_above+polygon_below)\n",
    "                        inside_above=pol.contains(Point(new_pt_above))\n",
    "                        inside_below=pol.contains(Point(new_pt_below))\n",
    "                    polygon_above.append(new_pt_above) # if point is outside add it to boundary\n",
    "                    polygon_below.insert(0,new_pt_below) # if point is outside add it to boundary\n",
    "                    old_pt=pt\n",
    "                if inside_above:\n",
    "                    del polygon_above[-1] # if point is inside and is last point delete it\n",
    "                if inside_below:\n",
    "                    del polygon_below[0] # if point is inside and is last point delete it\n",
    "                polygon_boundary=polygon_above+polygon_below\n",
    "                line_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/lines/'+str(baselinePKList[n])+'/')\n",
    "                resline = requests.get(line_url,headers=headers).json()\n",
    "                resline['mask']=polygon_boundary\n",
    "                data=json.dumps(resline)\n",
    "                rput = requests.put(line_url,headers=headers,data=data)\n",
    "                print(m,rput)\n",
    "    print('done')\n",
    "\n",
    "def find_lines_without_baseline(doc_pk,part_pk,transcription_levels = []):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  bad_lines = list()\n",
    "  for line in lines:\n",
    "    if not(line['baseline']==None):\n",
    "      if len(line['baseline'])<1:\n",
    "        bad_lines.append(line)\n",
    "    else:\n",
    "      bad_lines.append(line)\n",
    "  if not(transcription_levels==[]):\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,transcription_levels)\n",
    "  return bad_lines\n",
    "\n",
    "def merge2levels_in_third(doc_pk,part_pk,level1,level2,level3):\n",
    "  lines_without_master_transcription,proportion,empty_lines=get_lines_without_transcription_level(doc_pk,part_pk,level1)\n",
    "  if proportion>0.3:\n",
    "    lines_without_level2_transcription,proportion_level2,empty_lines=get_lines_without_transcription_level(doc_pk,part_pk,level2)\n",
    "    if proportion_verso<0.3:\n",
    "      print('from level2 to master on part ',part_pk)\n",
    "      transfer_txt2level(doc_pk,part_pk,level2,level3)\n",
    "    else:\n",
    "      print('from level1 to master on part ',part_pk)\n",
    "      transfer_txt2level(doc_pk,part_pk,level1,level3)\n",
    "  else:\n",
    "    print('master exists for part ',part_pk)\n",
    "\n",
    "def change_all_regions2main(dok_pk):\n",
    "  part_pk_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_pk)\n",
    "  for n,part_pk in enumerate(part_pk_list):\n",
    "    region_list=get_region_pk_list(doc_pk,part_pk)\n",
    "    for region_pk in region_list:\n",
    "      regions_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/blocks/'+str(region_pk)+'/'\n",
    "      res=requests.get(regions_url,headers=headers).json()\n",
    "      print(n,res['typology'])\n",
    "      if not(res['typology']==2):\n",
    "        res['typology']=2\n",
    "        data=json.dumps(res)\n",
    "        res2=requests.put(regions_url,headers=headers,data=data)\n",
    "        print(regions_url)\n",
    "        print(res2)\n",
    "\n",
    "def plot_polygon(Polygon):\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  plt.plot(*Polygon.exterior.xy)\n",
    "  #plt.plot(*maskSimple.exterior.xy)\n",
    "\n",
    "def move_line_up_down(baseline,distance,down,pageheight,pagewidth):\n",
    "  #down=1\n",
    "  #up=-1\n",
    "  safety=5\n",
    "  angle=get_angle(baseline)\n",
    "  # 0=horizontal\n",
    "  if abs(angle)<45:\n",
    "    if baseline[0][0]<baseline[-1][0]:\n",
    "      vectorx,vectory=get_vector(angle+90*down,distance) #(LTR)\n",
    "    else:\n",
    "      vectorx,vectory=get_vector(angle+90*down,distance) #(RTL) (upside down)\n",
    "  else:\n",
    "    if baseline[0][1]<baseline[-1][1]:\n",
    "      vectorx,vectory=get_vector(angle+90*down,distance) #(vertical up)\n",
    "    else:\n",
    "      vectorx,vectory=get_vector(angle+90*down,distance) #(vertical down)\n",
    "  new_baseline=[[max(safety,min(pagewidth-safety,p[0]+vectorx)),max(safety,min(pageheight-safety,p[1]+vectory))] for p in baseline]\n",
    "  return(new_baseline)\n",
    "\n",
    "def extend_single_line(baseline,distance,do_left,do_right,pageheight,pagewidth):\n",
    "  angle=get_angle(baseline)\n",
    "  # 0=horizontal\n",
    "  vectorx,vectory=get_vector(angle,distance)\n",
    "  p1=baseline[0]\n",
    "  pz=baseline[-1]\n",
    "  p0=[]\n",
    "  pend=[]\n",
    "  if abs(angle)<45:\n",
    "    if baseline[0][0]<baseline[-1][0]:\n",
    "      if do_left:\n",
    "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
    "      if do_right:\n",
    "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
    "    else:\n",
    "      if do_left:\n",
    "        p0=[[max(1,min(pagewidth,p1[0]+vectorx)),max(1,min(pageheight,p1[1]+vectory))]]\n",
    "      if do_right:\n",
    "        pend=[[max(1,min(pagewidth,pz[0]-vectorx)),max(1,min(pageheight,pz[1]-vectory))]]\n",
    "  else:\n",
    "    if baseline[0][1]<baseline[-1][1]:\n",
    "      if do_left:\n",
    "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
    "      if do_right:\n",
    "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
    "    else:\n",
    "      if do_left:\n",
    "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
    "      if do_right:\n",
    "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
    "  if not(p0==[]):\n",
    "    baseline=baseline[1:]\n",
    "  if not(pend==[]):\n",
    "    baseline=baseline[:-1]\n",
    "  new_baseline=p0+baseline+pend\n",
    "  return(new_baseline)\n",
    "\n",
    "\n",
    "def move_all_lines_of_part_up_down(doc_pk,part_pk,distance=10,down=1,repoly=True,default2none=True):\n",
    "  if default2none:\n",
    "    default_line_type_pks = get_default_line_type_pks(doc_pk)\n",
    "  lines=get_all_lines_of_part(doc_pk,part_pk)\n",
    "  #[[line['pk'],line['order'],line['baseline'],line['typology'],line['region']] for line in data['results']]\n",
    "  pageheight,pagewidth=get_page_height_width(doc_pk,part_pk)\n",
    "  print(part_pk)\n",
    "  for line in lines:\n",
    "    baseline=line['baseline']\n",
    "    line_pk=line['pk']\n",
    "    line['baseline']=move_line_up_down(baseline,distance,down,pageheight,pagewidth)\n",
    "    if default2none:\n",
    "      if line['typology'] in default_line_type_pks:\n",
    "        line['typology'] = None\n",
    "  bulk_update_lines(doc_pk,part_pk,lines)\n",
    "  if repoly:\n",
    "    r = repolygonize(doc_pk,part_pk, print_status= False)\n",
    "\n",
    "\n",
    "def calculate_regions_for_unlinked_lines(doc_nu,part,margin_type,paratext_type):\n",
    "# calculate regions for unlinked lines by taking the minima and maxima of their boundaries\n",
    "    from shapely.ops import cascaded_union\n",
    "\n",
    "    parts_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/')\n",
    "    print(parts_url)\n",
    "    res = requests.get(parts_url,headers=headers).json()\n",
    "    regpoly_list=[]\n",
    "    line_links_list=[]\n",
    "\n",
    "    for line_n,lines in enumerate(res['lines']):\n",
    "        if lines['region']==None:\n",
    "            line_array=array(lines['mask'])\n",
    "            try:\n",
    "                x1 = min(line_array[:,0])\n",
    "                y1 = min(line_array[:,1])\n",
    "                x2 = max(line_array[:,0])\n",
    "                y2 = max(line_array[:,1])\n",
    "                poly = Polygon([[x1,y1],[x2,y1],[x2,y2],[x1,y2]])\n",
    "                if len(regpoly_list)==0:\n",
    "                    regpoly_list.append(poly)\n",
    "                    line_links_list=[[line_n]]\n",
    "                else:\n",
    "                    no_join_found = True\n",
    "                    for n,poly1 in enumerate(regpoly_list):\n",
    "                        if poly.intersects(poly1): #poly overlaps with poly1 :\n",
    "                    # merge: https://deparkes.co.uk/2015/02/28/how-to-merge-polygons-in-python/\n",
    "                            polygons = [poly1, poly]\n",
    "                            union_poly = cascaded_union(polygons)\n",
    "                    # replace poly1 by union_poly\n",
    "                            regpoly_list[n] = union_poly\n",
    "                            poly = union_poly\n",
    "                            line_links_list[n].append(line_n)\n",
    "                            no_join_found = False\n",
    "                    if no_join_found:\n",
    "                        regpoly_list.append(poly)\n",
    "                        line_links_list.append([line_n])\n",
    "            except:\n",
    "                print('error at:',str(part))\n",
    "    blocks_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/'\n",
    "    data=requests.get(blocks_url,headers=headers).json()\n",
    "    existing_regions=data['regions']\n",
    "    post_url = root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+ '/blocks/'\n",
    "    max_region_n=-1\n",
    "    if len(existing_regions)==0:\n",
    "        max_region_size=0\n",
    "        for region_n,region in enumerate(regpoly_list):\n",
    "            this_region_area=region.area\n",
    "            if this_region_area>max_region_size:\n",
    "                max_region_size=this_region_area\n",
    "                max_region_n=region_n\n",
    "        maincol=regpoly_list[max_region_n]\n",
    "        maincol_point_list = [[int(float(j)) for j in i] for i in list(maincol.exterior.coords)]\n",
    "        block_json={'document_part': part,'box': maincol_point_list,'typology': 2}\n",
    "        r = requests.post(post_url,headers=headers, data=json.dumps(block_json))\n",
    "        y_main_min = min(array(maincol_point_list)[:,1])\n",
    "        y_main_max = max(array(maincol_point_list)[:,1])\n",
    "    else:\n",
    "        y_main_min=10000\n",
    "        y_main_max=0\n",
    "        for region in existing_regions:\n",
    "            if len(region['box'])<3:\n",
    "                print('delete '+str(region['pk']))\n",
    "                delete_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+ '/blocks/'+str(region['pk']) + '/'\n",
    "                r = requests.delete(delete_url,headers=headers)\n",
    "                print(r.status_code)\n",
    "            else:\n",
    "                maincol=Polygon(region['box'])\n",
    "                y1 = min(array(region['box'])[:,1])\n",
    "                if y1<y_main_min:\n",
    "                    y_main_min=y1\n",
    "                y2 = max(array(region['box'])[:,1])\n",
    "                if y2>y_main_max:\n",
    "                    y_main_max=y2\n",
    "    if max_region_n>-1:\n",
    "        del regpoly_list[max_region_n] # KICK MAINCOL out of REGPOLYLIST\n",
    "    for region_n,poly in enumerate(regpoly_list):\n",
    "        poly_point_list = [[int(float(j)) for j in i] for i in list(poly.exterior.coords)]\n",
    "        y_poly = mean(array(poly_point_list)[:,1])\n",
    "        if (y_poly<y_main_min) or (y_poly>y_main_max):\n",
    "            region_type=paratext_type\n",
    "        else:\n",
    "            region_type=margin_type\n",
    "        block_json={'document_part': part,'box': poly_point_list,'typology': region_type}\n",
    "        r = requests.post(post_url,headers=headers, data=json.dumps(block_json))\n",
    "    reorder_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/recalculate_ordering/'\n",
    "    reorder_res = requests.post(reorder_url,headers=headers)\n",
    "\n",
    "def fast_merge_stuttering_lines(doc_pk, part_pk, regions_pks_list, delta_y):\n",
    "    distance_matrix_y = create_distance_matrix_y(doc_pk, part_pk, regions_pks_list)\n",
    "    lines = identify_group_of_lines_with_small_distances(doc_pk, part_pk, regions_pks_list, distance_matrix_y, delta_y)\n",
    "\n",
    "    for line in lines:\n",
    "        print(f\"lignes à fusionner:{line}\") # for debug\n",
    "        merge_lines(doc_pk,part_pk,line)\n",
    "\n",
    "    new_region_data = create_region_for_corrected_segmentation(doc_pk, part_pk, regions_pks_list)\n",
    "    link_merged_lines_to_new_region(doc_pk, part_pk, new_region_data)\n",
    "    reorder(doc_pk,part_pk,print_status = True)\n",
    "    repolygonize(doc_pk,part_pk, print_status= True)\n",
    "\n",
    "    print(\"The text no longer stutters!\")\n",
    "\n",
    "def create_distance_matrix_y(doc_pk, part_pk, regions_pks_list):\n",
    "    # Now takes as argument a list of regions region_pk = [region_pk1, region_pk2, etc. ]\n",
    "    # Merge the region line dictionary lists\n",
    "\n",
    "    lines_from_selected_regions = [] # List containing all the lines of all the regions selected in regions_pks_list\n",
    "    for region_pk in regions_pks_list:\n",
    "        lines_from_single_region = get_all_lines_of_region(doc_pk, part_pk, region_pk)\n",
    "        lines_from_selected_regions += lines_from_single_region\n",
    "\n",
    "    # Retrieve the coordinates of the midpoints of the lines in the region\n",
    "    lines = lines_from_selected_regions\n",
    "    coordinates = []\n",
    "    for line in lines:\n",
    "        line_mid_point = get_line_mid_point(line)\n",
    "        coordinates.append(line_mid_point)\n",
    "\n",
    "    # Calculating the y distance between two points\n",
    "    def distance_y(point1, point2):\n",
    "        return abs(point1[1] - point2[1])\n",
    "\n",
    "    # Construction of the matrix of interline distances projected onto the y-axis\n",
    "    distance_matrix_y = []\n",
    "    for i in range(len(coordinates)):\n",
    "        line_dist_y = [distance_y(coordinates[i], coordinates[j]) if i != j else sys.maxsize for j in range(len(coordinates))] # je reprends ici la sémantique du code d'eSc\n",
    "        distance_matrix_y.append(line_dist_y)\n",
    "    print(distance_matrix_y) # pour debug\n",
    "\n",
    "    return distance_matrix_y\n",
    "\n",
    "def link_merged_lines_to_new_region(doc_pk, part_pk, new_region_data):\n",
    "\n",
    "    # Fonction pour vérifier si un point (x, y) est à l'intérieur d'un polygone\n",
    "    def point_inside_polygon(x, y, poly):\n",
    "        n = len(poly)\n",
    "        inside = False\n",
    "        p1x, p1y = poly[0]\n",
    "        for i in range(n + 1):\n",
    "            p2x, p2y = poly[i % n]\n",
    "            if y > min(p1y, p2y):\n",
    "                if y <= max(p1y, p2y):\n",
    "                    if x <= max(p1x, p2x):\n",
    "                        if p1y != p2y:\n",
    "                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                        if p1x == p2x or x <= xinters:\n",
    "                            inside = not inside\n",
    "            p1x, p1y = p2x, p2y\n",
    "        return inside\n",
    "\n",
    "    # List of dictionaries for all lines in the part document\n",
    "    lines_list = get_all_lines_of_part(doc_pk, part_pk)\n",
    "\n",
    "    #Information about the new region \"new_region_data\n",
    "    region_pk = new_region_data['pk']\n",
    "    print(region_pk)\n",
    "    region_box = new_region_data['box']\n",
    "\n",
    "    # Initialise the line_data_update dictionary before the loop, which will contain the line data to be updated.\n",
    "    line_data_update = {}\n",
    "\n",
    "    # Browse all lines and assign the region if the mask is inside the region\n",
    "    for line in lines_list:\n",
    "        is_inside_region = all(point_inside_polygon(x, y, region_box) for x, y in line['mask'])\n",
    "        if is_inside_region:\n",
    "            line['region'] = region_pk\n",
    "            line_data_update = {'region': line['region']}\n",
    "        update_url = get_specific_line_url(doc_pk, part_pk, line['pk'])\n",
    "        r=requests.patch(update_url,headers=headers,data=json.dumps(line_data_update))\n",
    "\n",
    "\n",
    "    # Display updated lines\n",
    "    # for line in lines_list:\n",
    "    # print(line)\n",
    "\n",
    "\n",
    "def create_region_for_corrected_segmentation(doc_pk, part_pk, regions_pks_list):\n",
    "\n",
    "    # Retrieve the coordinates of the masks for the lines in each region of regions_pks_list\n",
    "    # and concatenate them.\n",
    "    # mask_coordinates = [mask_coordinates1, mask_coordinates2, ...]\n",
    "\n",
    "    lines_mask_coordinates = []\n",
    "    lines_url=get_lines_url(doc_pk,part_pk)\n",
    "    lines=loop_through_itempages_and_get_all(lines_url)\n",
    "    for line in lines:\n",
    "        if line['pk'] in regions_pks_list:\n",
    "            mask=line['mask']\n",
    "            lines_mask_coordinates.append(mask)\n",
    "\n",
    "    # Retrieve the coordinates of regions from regions_pks_list\n",
    "    regions_coordinates = []\n",
    "    all_regions_of_part = get_all_regions_of_part(doc_pk, part_pk)\n",
    "\n",
    "    for region in all_regions_of_part:\n",
    "        # if the region's region_pk is in regions_pks_list\n",
    "        # then its co-ordinates are added to the list of region co-ordinates regions_coordinates\n",
    "        if region['pk'] in regions_pks_list:\n",
    "            regions_coordinates.append(region['box'])\n",
    "    print(f\"coordonnées des régions de regions_pks_list: {regions_coordinates}\")\n",
    "\n",
    "    # Concatenate\n",
    "    poly_coordinates = regions_coordinates + lines_mask_coordinates\n",
    "\n",
    "    # Calculation of the convexHull of this zone\n",
    "\n",
    "    # Convert the nested list into a NumPy array\n",
    "    points = numpy.array([point for poly in poly_coordinates for point in poly])\n",
    "\n",
    "    # Calculation of convex hull\n",
    "    hull = ConvexHull(points)\n",
    "\n",
    "    # Obtain the indices of the vertices of the hull convex\n",
    "    convex_hull_indices = hull.vertices\n",
    "\n",
    "    # Obtain the coordinates of the convex hull points\n",
    "    convex_hull_points = points[convex_hull_indices]\n",
    "\n",
    "    # Convert the list of lists into a list of lists\n",
    "    convex_hull_points_lists = [list(point) for point in convex_hull_points]\n",
    "\n",
    "    # Passing int32 to int so that the json request can succeed.\n",
    "    convex_hull_points_lists = [[int(x), int(y)] for x, y in convex_hull_points_lists]\n",
    "\n",
    "    # Abolition of the old regions\n",
    "    for region_pk in regions_pks_list:\n",
    "        delete_region(doc_pk,part_pk,region_pk)\n",
    "\n",
    "    # Creation of the new region\n",
    "\n",
    "    region_url = get_regions_url(doc_pk,part_pk)\n",
    "    reg_polygon = convex_hull_points_lists\n",
    "    regiontype_pk = None\n",
    "\n",
    "    data = {'document_part':part_pk,'box':reg_polygon,'typology':regiontype_pk}\n",
    "    r = requests.post(url=region_url,headers=headers,data=json.dumps(data))\n",
    "    new_region_data = r.json()\n",
    "\n",
    "    return new_region_data\n",
    "\n",
    "\n",
    "# 2. Identify groups of lines whose vertical distance (y) between their midpoints is less than the delta_y interval\n",
    "# Identify and record the references of these lines in a dictionary.\n",
    "def identify_group_of_lines_with_small_distances(doc_pk, part_pk, regions_pks_list, distance_matrix_y, delta_y):\n",
    "\n",
    "    # Creation of a dictionary of the lines of the regions selected in regions_pks_list\n",
    "    lines_from_selected_regions = [] # List containing all the lines of all the regions selected in regions_pks_list\n",
    "    for region_pk in regions_pks_list:\n",
    "        lines_from_single_region = get_all_lines_of_region(doc_pk, part_pk, region_pk)\n",
    "        lines_from_selected_regions += lines_from_single_region\n",
    "\n",
    "    line_dict = lines_from_selected_regions\n",
    "\n",
    "    # Creation of the list hosting the row group references to be merged\n",
    "    # I want to format this data so that it can be easily passed to the API endpoint merge.\n",
    "    # [{'lines\": ['pk_number1', 'pk_number2', 'pk_number3']}, {'lines\": ['pk_number5', 'pk_number6']}, ... ]\n",
    "    group_of_lines_with_small_distances = []\n",
    "\n",
    "    # Identify and import groups of lines to be merged.\n",
    "    for i in range(len(distance_matrix_y)):\n",
    "        lines_to_merge = []\n",
    "        lines_to_merge.append(str(line_dict[i]['pk'])) # I import the first line i, which will be used for comparison\n",
    "\n",
    "        for j in range(i + 1, len(distance_matrix_y)): # Evaluation of the distance y between line i and all the others.\n",
    "            if distance_matrix_y[i][j] <= delta_y:     # If this distance is less than the delta_y interval, I import the line\n",
    "                lines_to_merge.append(str(line_dict[j]['pk'])) # in a \"line_to_merge\" dictionary\n",
    "        current_group = {'lines': lines_to_merge} # Creation of the group dictionary in the form {'lines': lines_to_merge}\n",
    "        group_of_lines_with_small_distances.append(current_group)\n",
    "\n",
    "    # Filter the list of dictionaries of lines to be merged, to remove solitary lines.\n",
    "    data_filtered = []\n",
    "    for entry in group_of_lines_with_small_distances:\n",
    "        if len(entry['lines']) >= 2:\n",
    "            data_filtered.append(entry)\n",
    "\n",
    "    # Delete duplicates\n",
    "    data_no_duplicates = [{'lines': list(set(d['lines']))} for d in data_filtered]\n",
    "\n",
    "    group_of_lines_with_small_distances = data_no_duplicates\n",
    "    return group_of_lines_with_small_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1742386968904,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "qvCvJredQSYj"
   },
   "outputs": [],
   "source": [
    "# @title more complex\n",
    "\n",
    "def get_delta_y_with_angle_and_x(angle,x):\n",
    "    return x * math.tan(angle * math.pi / 180)\n",
    "\n",
    "def sort_sublists_by_element(sub_li,n):\n",
    "      return(sorted(sub_li, key=lambda x: x[n]))\n",
    "\n",
    "def extend_lines2region_boundaries(doc_pk,part_pk):\n",
    "#if True:\n",
    "\n",
    "\n",
    "\n",
    "  page_height,page_width = get_page_height_width(doc_pk,part_pk)\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  new_lines=list()\n",
    "  for region in regions:\n",
    "    region['Circumference']=LineString(region['box'])\n",
    "\n",
    "  for line in lines:\n",
    "    if line['baseline'][0][0] > line['baseline'][-1][0]:\n",
    "      invert = True\n",
    "      line['baseline'].reverse()\n",
    "    else:\n",
    "      invert = False\n",
    "    this_region = search(line['region'],regions)\n",
    "    line_left_angle = get_angle([line['baseline'][0],line['baseline'][1]])\n",
    "    left_delta_y = get_delta_y_with_angle_and_x(line_left_angle,line['baseline'][0][0])\n",
    "    line_right_angle = get_angle([line['baseline'][-2],line['baseline'][-1]])\n",
    "    right_delta_y = get_delta_y_with_angle_and_x(line_right_angle,page_width-line['baseline'][-1][0])\n",
    "    new_p_left = [1,line['baseline'][0][1]+left_delta_y]\n",
    "    new_p_right = [page_width,line['baseline'][-1][1]+right_delta_y]\n",
    "    extended_line = LineString([new_p_left]+line['baseline']+[new_p_right])\n",
    "    #print()\n",
    "    #print(line['pk'],line['order'])\n",
    "    #print('beg:',line_left_angle,left_delta_y)\n",
    "    #print('end :',line_right_angle,right_delta_y)\n",
    "    #print('region boundary',this_region['Circumference'])\n",
    "    #print('extended_line:',extended_line)\n",
    "\n",
    "    intersection_points = intersection(extended_line, this_region['Circumference'])\n",
    "    new_points = sort_sublists_by_element([[int(round(coordinate[0],0)) for coordinate in p.xy] for p in intersection_points.geoms],0)\n",
    "    new_p_left = new_points[0]\n",
    "    new_p_right = new_points[-1]\n",
    "    #print('nps',new_points)\n",
    "    #print('np left',new_p_left)\n",
    "    #print('np right',new_p_right)\n",
    "    #print('old bl',line['baseline'])\n",
    "    new_baseline = [new_p_left]+line['baseline']+[new_p_right]\n",
    "    if invert:\n",
    "      new_baseline.reverse()\n",
    "    new_line = {'pk':line['pk'],'baseline':new_baseline}\n",
    "    new_lines.append(new_line)\n",
    "  bulk_update_lines(doc_pk,part_pk,new_lines)\n",
    "  repolygonize(doc_pk,part_pk)\n",
    "\n",
    "def get_ascender_descender_polys(line_x,direction = 'RTL'):\n",
    "  def poly_height(poly,baseline):\n",
    "    baseline_length = get_baseline_length(baseline)\n",
    "    if baseline_length > 0:\n",
    "      return poly.area/baseline_length\n",
    "    else:\n",
    "      return None\n",
    "  found = [False,False]\n",
    "  best_dist1 = 10000000\n",
    "  best_dist2 = 10000000\n",
    "  for i,p in enumerate(line_x['mask']):\n",
    "    this_dist1 = dist2points(line_x['baseline'][0][0],line_x['baseline'][0][1],p[0],p[1])\n",
    "    this_dist2 = dist2points(line_x['baseline'][-1][0],line_x['baseline'][-1][1],p[0],p[1])\n",
    "    if p == line_x['baseline'][0]:\n",
    "      found[0] = True\n",
    "      best_dist1 = 0\n",
    "    elif p == line_x['baseline'][-1]:\n",
    "      found[1] = True\n",
    "      best_dist = 0\n",
    "    else:\n",
    "      if this_dist1<best_dist1:\n",
    "        best_dist1 = this_dist1\n",
    "        best_i1 = i\n",
    "      elif this_dist2<best_dist2:\n",
    "        best_dist2 = this_dist2\n",
    "        best_i2 = i\n",
    "  if not(found[0]):\n",
    "    line_x['baseline'][0]=line_x['mask'][best_i1]\n",
    "  if not(found[1]):\n",
    "    line_x['baseline'][-1]=line_x['mask'][best_i2]\n",
    "  #if not(found[0]) or not(found[1]):\n",
    "    #print(found)\n",
    "    #print(line_x['mask'])\n",
    "    #print(line_x['baseline'])\n",
    "\n",
    "  polys = poly_split(Polygon(line_x['mask']), LineString(line_x['baseline']))\n",
    "  #print(polys)\n",
    "  poly1,poly2 = [poly for poly in polys.geoms]\n",
    "  xx, yy = poly1.boundary.coords.xy\n",
    "  poly1_points = [[p[0],p[1]] for p in zip(xx,yy)]\n",
    "  orientation = poly1_points[0]==line_x['baseline'][0]\n",
    "  if direction=='RTL':\n",
    "    orientation=not(orientation)\n",
    "  poly1_height = round(poly_height(poly1,line_x['baseline']),1)\n",
    "  poly2_height = round(poly_height(poly2,line_x['baseline']),1)\n",
    "  if orientation:\n",
    "    return poly1, poly2,poly1_height,poly2_height\n",
    "  else:\n",
    "    return poly2,poly1,poly2_height,poly1_height\n",
    "\n",
    "def calculate_average_line_distance(lines):\n",
    "\n",
    "  average_line_height_list=[]\n",
    "  midpointList = [get_line_mid_point(line) for line in lines]\n",
    "  #print(midpointList)\n",
    "  lineBegList = [get_line_beg_point(line) for line in lines]\n",
    "  lineEndList = [get_line_end_point(line) for line in lines]\n",
    "  d_list=[]\n",
    "  for n,midpoint in enumerate(midpointList):\n",
    "    if n>0:\n",
    "      p1=array(lineBegList[n-1])\n",
    "      p2=array(lineEndList[n-1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append(-cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
    "    elif len(midpointList)>1:\n",
    "      p1=array(lineBegList[n+1])\n",
    "      p2=array(lineEndList[n+1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append((cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
    "    average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
    "    average_line_height_list.append(average_line_height)\n",
    "    #print(average_line_height)\n",
    "  if average_line_height_list:\n",
    "    return int(round(mean(average_line_height_list)))\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "def create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "    doc_pk,part_pk,method='fix',regiontypelist=[],linetypelist=[],\n",
    "    include_none_linetype=True,ascender=12,descender=24, safetydistance=5):\n",
    "# possible methods:\n",
    "# 'fix': fixed values for the distance above (ascender) and below (descender) the baseline.\n",
    "# 'average_line_dist': creates ascender polygons by multiplying the average line distance with the value in \"ascenders\" and analogously descender polys with \"descenders\"\n",
    "# 'average_line_height': calculates first the average current polygon height and adds the values in the ascender and descender parameters\n",
    "# proportion should be between 0 and 1\n",
    "  if not(method in ['fix','average_line_dist','average_line_height']):\n",
    "    print('unknown method type')\n",
    "    #kaputt # fail\n",
    "  if include_none_linetype:\n",
    "    check_none=None\n",
    "  else:\n",
    "    check_none=False\n",
    "  pageheight,pagewidth=get_page_height_width(doc_pk,part_pk)\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  regions=get_all_regions_of_part(doc_pk,part_pk)\n",
    "  if regiontypelist == []: ## empty list means take all regions regardless of regiontype\n",
    "    regiontypelist = set([region['typology'] for region in regions])\n",
    "  if linetypelist ==[]: ## empty list means take all lines regardless of linetype\n",
    "    linetypelist = set([line['typology'] for line in lines])\n",
    "  selected_region_pks = [region['pk'] for region in regions if region['typology'] in regiontypelist]\n",
    "  selected_lines = [line for line in lines if ((line['typology'] in linetypelist) or (line['typology']==check_none)) and line['region'] in selected_region_pks]\n",
    "\n",
    "  if method == 'average_line_dist':\n",
    "    for region in regions:\n",
    "      if region['typology'] in regiontypelist:\n",
    "        region['line_dist'] = calculate_average_line_distance(selected_lines)\n",
    "        region['ascender'] = ascender * region['line_dist']\n",
    "        region['descender'] = descender * region['line_dist']\n",
    "      else:\n",
    "        region['line_dist'] = None\n",
    "        region['ascender'] = None\n",
    "        region['descender'] = None\n",
    "  elif method == 'average_line_height':\n",
    "    for region in regions:\n",
    "      if region['typology'] in regiontypelist:\n",
    "        ascenders = list()\n",
    "        descenders = list()\n",
    "        for line in selected_lines:\n",
    "          Poly1,Poly2,a,d = get_ascender_descender_polys(line)\n",
    "          ascenders.append(a)\n",
    "          descenders.append(d)\n",
    "        region['ascender'] = mean(ascenders) + ascender\n",
    "        region['descender'] = mean(descenders) + descender\n",
    "\n",
    "  for line in selected_lines:\n",
    "    baseline = line['baseline']\n",
    "    old_pt=baseline[0]\n",
    "\n",
    "    if method in ['average_line_dist','average_line_height']:\n",
    "      region_pk = line['region']\n",
    "      ascender = search(region_pk,regions)['ascender']\n",
    "      descender = search(region_pk,regions)['descender']\n",
    "    if (ascender == None) or (descender == None):\n",
    "      print('no ascender or descender value for line',line['pk'])\n",
    "      continue\n",
    "    for o,pt in enumerate(baseline[1:]):\n",
    "      angle=get_angle([old_pt,pt])\n",
    "      if o==0:\n",
    "        inside_above=False\n",
    "        inside_below=False\n",
    "        new_pt_above=rotateNP((old_pt[0],old_pt[1]-ascender),origin=tuple(old_pt),degrees=angle)\n",
    "        new_pt_below=rotateNP((old_pt[0],old_pt[1]+descender),origin=tuple(old_pt),degrees=angle)\n",
    "        new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "        new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "        polygon_above=[[int(coordinate) for coordinate in new_pt_above]]\n",
    "        polygon_below=[[int(coordinate) for coordinate in new_pt_below]]\n",
    "      new_pt_above=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]-ascender),origin=tuple(pt),degrees=angle)]\n",
    "      new_pt_below=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]+descender),origin=tuple(pt),degrees=angle)]\n",
    "      new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "      new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "      if o>0:\n",
    "        # if point is inside new mask and is not the last point, delete previous point, dont insert next point either but insert instead the intersection of (p-2:p-1) and (p:p+1)\n",
    "        if inside_above and not(o==len(line[1:])):\n",
    "          replace_pt=list(line_intersection([polygon_above[-3],polygon_above[-2]],[polygon_above[-1],new_pt_above]))\n",
    "          del polygon_above[-2:-1]\n",
    "          polygon_above.append(replace_pt)\n",
    "        if inside_below:\n",
    "          replace_pt=list(line_intersection([polygon_below[2],polygon_below[1]],[polygon_below[0],new_pt_below]))\n",
    "          del polygon_below[0:1]\n",
    "          polygon_below.insert(0,replace_pt)\n",
    "        pol=Polygon(polygon_above+polygon_below)\n",
    "        inside_above=pol.contains(Point(new_pt_above))\n",
    "        inside_below=pol.contains(Point(new_pt_below))\n",
    "      polygon_above.append(new_pt_above) # if point is outside add it to boundary\n",
    "      polygon_below.insert(0,new_pt_below) # if point is outside add it to boundary\n",
    "      old_pt=pt\n",
    "    if inside_above:\n",
    "      del polygon_above[-1] # if point is inside and is last point delete it\n",
    "    if inside_below:\n",
    "      del polygon_below[0] # if point is inside and is last point delete it\n",
    "    polygon_boundary=[baseline[0]]+polygon_above+[baseline[-1]]+polygon_below\n",
    "    line['mask']=polygon_boundary\n",
    "  return bulk_update_lines(doc_pk,part_pk,selected_lines)\n",
    "#create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    " #   doc_pk,part_pk,method='average_line_height',regiontypelist=[],linetypelist=[],\n",
    "  #  include_none_linetype=True,ascender=-30,descender=-10, safetydistance=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1742386972584,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "qnvdLgBZOCXD"
   },
   "outputs": [],
   "source": [
    "# @title normalize_existing_transcription_in_manual\n",
    "def normalize_existing_transcription_in_manual(doc_pk):\n",
    "\n",
    "  #tr_level = create_transcription_levels(doc_pk,'master')\n",
    "  part_pks = [part['pk'] for part in get_all_parts(doc_pk)]\n",
    "  manual_tr_level = get_specific_tr_level_pk(doc_pk,'manual')\n",
    "  chars = get_set_of_all_characters_in_transcription_level(doc_pk,part_pks,manual_tr_level)\n",
    "  for c in chars:\n",
    "    print(c,ord(c))\n",
    "  master_level = create_transcription_levels(doc_pk,'master')\n",
    "  print('master_level:',master_level)\n",
    "\n",
    "\n",
    "  chars2delete = '̊'\n",
    "  chars2replace = '{}<>'\n",
    "  replacementchars = '⟦⟧()'\n",
    "  replacement_tuples = [(old_c,new_c) for old_c,new_c in zip(chars2replace,replacementchars)]\n",
    "  chars_in_brackets = re.compile(\"(\\[[א-ת' :]*\\])\")\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    trs = get_page_transcription(doc_pk,part_pk,manual_tr_level)\n",
    "    new_trs = list()\n",
    "    for tr in trs:\n",
    "      linetxt = tr['content']\n",
    "      new_tr = dict()\n",
    "      if not(linetxt==''):\n",
    "        #print(linetxt)\n",
    "        #step 1: delete char 778 (circellus)\n",
    "        for c in chars2delete:\n",
    "          linetxt = linetxt.replace(c,'')\n",
    "        #step 2: switch all {} to ⟦⟧ !!! WE NEED A DISTINCTION BETWEEN DELETION AND LACUNA\n",
    "        #step 3: switch all <> to ()\n",
    "        for old_c,new_c in replacement_tuples:\n",
    "          linetxt = linetxt.replace(old_c,new_c)\n",
    "        #step 4: replace ':' with ' :'\n",
    "        linetxt = linetxt.replace(':',' :')\n",
    "        #step 5: delete all chars in brackets\n",
    "        linetxt = re.sub(chars_in_brackets,'[ ]',linetxt)\n",
    "        #step 6: delete opening bracket at line beginnings and end brackets and line\n",
    "        if linetxt[0:2] == '[ ' and len(linetxt)>3:\n",
    "          linetxt = linetxt[2:]\n",
    "        if linetxt[-3:-1] == '[ ' and len(linetxt)>3:\n",
    "          linetxt = linetxt[:-2]\n",
    "        new_tr['content'] = linetxt\n",
    "      else:\n",
    "        new_tr['content'] = ''\n",
    "      new_tr['line'] = tr['line']\n",
    "      new_tr['transcription'] = master_level\n",
    "      new_trs.append(new_tr)\n",
    "    r = bulk_create_transcriptions(doc_pk,part_pk,new_trs)\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1742386990488,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "RoIPx_UAGZFc"
   },
   "outputs": [],
   "source": [
    "# @title metadata_reading_writing\n",
    "\n",
    "def convert_eScr_metadata2dsbe_metadata_dict(doc_pk,part_pk,eScr_metadata_dict_list):\n",
    "  dsbe_metadata_dict_list=[]\n",
    "  for item in eScr_metadata_dict_list:\n",
    "    #print(item)\n",
    "    dsbe_metadata_dict = {'pk':item['pk'],'doc_pk':doc_pk,'part_pk':part_pk,'keyname':item['key']['name'],'cidoc_id':item['key']['cidoc_id'],'value':item['value']}\n",
    "    dsbe_metadata_dict_list.append(dsbe_metadata_dict)\n",
    "  return dsbe_metadata_dict_list\n",
    "\n",
    "def update_part_metadata(doc_pk,part_pk,eScr_metadata):\n",
    "  metadata_url = get_metadata_url(doc_pk,part_pk)\n",
    "  print(metadata_url)\n",
    "  r = update_item(metadata_url,eScr_metadata)\n",
    "  return r\n",
    "\n",
    "def update_dsbe_metadata_dict2eScr(dsbe_metadata_dict_list):\n",
    "  doc_pks=set()\n",
    "  for item in dsbe_metadata_dict_list:\n",
    "    doc_pks.add(item['doc_pk'])\n",
    "  for doc_pk in doc_pks:\n",
    "    metadata_this_doc = search_any('doc_pk',doc_pk,dsbe_metadata_dict_list)\n",
    "    part_pks=set()\n",
    "    for item in metadata_this_doc:\n",
    "      part_pks.add(item['part_pk'])\n",
    "    for part_pk in part_pks:\n",
    "      metadata_this_part=search_any('part_pk',part_pk,metadata_this_doc)\n",
    "      eScr_metadata=[]\n",
    "      for dsbe_item in metadata_this_part:\n",
    "        eScr_item = {'pk':item['pk'],'key':{'name':item['keyname'],'cidoc_id':item['cidoc_id']},'value':item['value']}\n",
    "        eScr_metadata.append(eScr_item)\n",
    "      r=update_part_metadata(doc_pk,part_pk,eScr_metadata)\n",
    "      print(r.content)\n",
    "\n",
    "def create_diverse_metadata(dsbe_metadata_dict_list):\n",
    "  for item in dsbe_metadata_dict_list:\n",
    "    doc_pk = item['doc_pk']\n",
    "    part_pk = item['part_pk']\n",
    "    eScr_item = {'key':{'name':item['keyname'],'cidoc_id':item['cidoc_id']},'value':item['value']}\n",
    "    r=create_part_metadata(doc_pk,part_pk,eScr_item)\n",
    "    print(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1742386993951,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "A8q7Ln1tlXeI",
    "outputId": "e00c4655-9ab9-4529-8f6e-58b053b86362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switching to  Local_INSTANCE\n",
      "https://env-9828609.us.reclaim.cloud/api/documents/\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "\n",
    "servername= 'Local_INSTANCE' # Add the instance name which you set in the first cell. It will fetch your connections credentials for your account.\n",
    "root_url,headers,headersbrief=get_serverinfo(servername,serverconnections)\n",
    "doc_url=get_documents_url()\n",
    "print(doc_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5JAzUcsoLso"
   },
   "source": [
    "This opens the same data directly as json. But you can get this directly with requests.get as we did before for sefaria, can't we? Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5639,
     "status": "ok",
     "timestamp": 1742387024738,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "cK4KTJ3d_liY",
    "outputId": "b9e21cae-020e-48fd-a0b6-c5a0eb50fd2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the full data returned - a dictionary:\n",
      "{'count': 556, 'next': 'http://env-9828609.us.reclaim.cloud/api/documents/?page=2', 'previous': None, 'results': [{'pk': 1, 'name': 'Trial Farsi Project', 'project': 'trial-project', 'transcriptions': [{'pk': 1, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T12:24:55.658199Z', 'comments': None}], 'main_script': None, 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 7, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 9, 'name': 'Date(s)'}, {'pk': 6, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 10, 'name': 'Metadata'}, {'pk': 8, 'name': 'Motto'}, {'pk': 5, 'name': 'Pricing'}, {'pk': 27, 'name': 'text'}, {'pk': 28, 'name': 'text'}], 'valid_line_types': [{'pk': 7, 'name': 'default'}, {'pk': 6, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 88, 'tags': [], 'created_at': '2024-02-14T12:24:55.654204Z', 'updated_at': '2024-02-14T12:24:55.654222Z', 'project_name': 'Trial project', 'project_id': 2}, {'pk': 2, 'name': 'Habl_2-12', 'project': 'my-new-farsi-project', 'transcriptions': [{'pk': 6, 'name': 'kraken:All Arabic Scripts', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-15T09:47:10.330140Z', 'comments': None}, {'pk': 5, 'name': 'kraken:persian_best', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-15T09:45:43.901274Z', 'comments': None}, {'pk': 2, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:14:47.558084Z', 'comments': None}], 'main_script': 'Old Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 13, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 15, 'name': 'Date(s)'}, {'pk': 16, 'name': 'footer'}, {'pk': 12, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 14, 'name': 'Metadata'}, {'pk': 17, 'name': 'Motto'}, {'pk': 11, 'name': 'Pricing'}], 'valid_line_types': [], 'valid_part_types': [], 'parts_count': 44, 'tags': [], 'created_at': '2024-02-14T18:14:47.555238Z', 'updated_at': '2024-02-14T18:14:47.555259Z', 'project_name': 'My New Farsi Project', 'project_id': 4}, {'pk': 3, 'name': 'LA_GD_HBL_23_Ausgabe_12_Rabīʿ_āṯ-ṯānī_1325_5_Ḫordād_829_25_Mai_1907', 'project': 'my-new-farsi-project', 'transcriptions': [{'pk': 4, 'name': 'transcript', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:29:08.024746Z', 'comments': None}, {'pk': 3, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:26:40.171213Z', 'comments': None}], 'main_script': 'Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 24, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 22, 'name': 'Date(s)'}, {'pk': 25, 'name': 'footer'}, {'pk': 18, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 19, 'name': 'Metadata'}, {'pk': 21, 'name': 'Motto'}, {'pk': 23, 'name': 'other'}, {'pk': 20, 'name': 'Pricing'}], 'valid_line_types': [], 'valid_part_types': [], 'parts_count': 4, 'tags': [], 'created_at': '2024-02-14T18:26:40.168332Z', 'updated_at': '2024-10-09T09:20:51.478661Z', 'project_name': 'My New Farsi Project', 'project_id': 4}, {'pk': 4, 'name': 'Farsi HTR\\\\OCR Test', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 38, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9572961859096191, 'created_at': '2024-06-13T18:11:39.430417Z', 'comments': None}, {'pk': 14, 'name': 'kraken:Persian (persian_best base)', 'archived': False, 'avg_confidence': 0.9568122770999754, 'created_at': '2024-04-09T07:25:05.107953Z', 'comments': None}, {'pk': 36, 'name': 'kraken:first_persian_model', 'archived': False, 'avg_confidence': 0.9412393656690228, 'created_at': '2024-06-13T12:55:32.359312Z', 'comments': None}, {'pk': 11, 'name': 'Page new', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T10:10:07.983578Z', 'comments': None}, {'pk': 10, 'name': 'Page_7', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T10:02:04.152907Z', 'comments': None}, {'pk': 9, 'name': 'Page', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:48:35.924187Z', 'comments': None}, {'pk': 8, 'name': 'Default PAGE Import', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:34:09.422322Z', 'comments': None}, {'pk': 7, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:27:46.698316Z', 'comments': None}], 'main_script': 'Arabic', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 26, 'name': 'text'}], 'valid_line_types': [{'pk': 5, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 8, 'tags': [], 'created_at': '2024-02-19T09:27:46.695565Z', 'updated_at': '2024-02-19T09:27:46.695585Z', 'project_name': 'Farsi Balslev', 'project_id': 5}, {'pk': 7, 'name': 'Sur Israfil 1-11', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 17, 'name': 'kraken:Persian (persian_best base)', 'archived': False, 'avg_confidence': 0.9374685632370211, 'created_at': '2024-04-09T07:47:35.281438Z', 'comments': None}, {'pk': 37, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9559087378969109, 'created_at': '2024-06-13T17:39:13.366981Z', 'comments': None}, {'pk': 35, 'name': 'kraken:first_persian_model', 'archived': False, 'avg_confidence': 0.9074163640921221, 'created_at': '2024-06-13T12:29:49.657423Z', 'comments': None}, {'pk': 16, 'name': 'layout', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-09T07:43:35.716440Z', 'comments': None}, {'pk': 15, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-09T07:42:01.243240Z', 'comments': None}], 'main_script': 'Arabic', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 64, 'name': 'column'}, {'pk': 31, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 34, 'name': 'Date(s)'}, {'pk': 35, 'name': 'footer'}, {'pk': 67, 'name': 'footnote'}, {'pk': 29, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 33, 'name': 'Metadata'}, {'pk': 32, 'name': 'Motto'}, {'pk': 66, 'name': 'pagefooter'}, {'pk': 63, 'name': 'Pagenumber'}, {'pk': 30, 'name': 'Pricing'}, {'pk': 69, 'name': 'tab-header'}, {'pk': 68, 'name': 'tab-header-sup'}, {'pk': 36, 'name': 'text'}, {'pk': 65, 'name': 'Title'}], 'valid_line_types': [{'pk': 21, 'name': 'begin-article'}, {'pk': 22, 'name': 'begin-paragraph'}, {'pk': 8, 'name': 'default'}, {'pk': 23, 'name': 'h2'}], 'valid_part_types': [], 'parts_count': 90, 'tags': [], 'created_at': '2024-04-09T07:42:01.022926Z', 'updated_at': '2024-04-09T07:42:01.022944Z', 'project_name': 'Farsi Balslev', 'project_id': 5}, {'pk': 8, 'name': 'Uri_Train_Set', 'project': 'gnazim-project', 'transcriptions': [{'pk': 20, 'name': 'XML Import II', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:15:35.138131Z', 'comments': None}, {'pk': 19, 'name': 'XML Import I', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:07:02.945464Z', 'comments': None}, {'pk': 18, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T09:55:19.684869Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 1, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 43, 'name': 'Addition'}, {'pk': 42, 'name': 'Center Text'}, {'pk': 1, 'name': 'Header'}, {'pk': 41, 'name': 'Platform'}], 'valid_line_types': [{'pk': 9, 'name': 'default'}, {'pk': 27, 'name': 'Fragmented'}, {'pk': 20, 'name': 'Handwritten'}], 'valid_part_types': [], 'parts_count': 250, 'tags': [{'pk': 6, 'name': 'GroundTruth', 'color': '#763f75'}], 'created_at': '2024-04-10T09:55:19.681656Z', 'updated_at': '2024-10-08T07:06:33.343832Z', 'project_name': 'Gnazim Project', 'project_id': 9}, {'pk': 9, 'name': 'Big Dataset', 'project': 'gnazim-project', 'transcriptions': [{'pk': 22, 'name': 'transcript_2024-04-10', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:55:45.891458Z', 'comments': None}, {'pk': 21, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:34:01.293492Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 86, 'name': 'Addition'}, {'pk': 87, 'name': 'Center Text'}, {'pk': 136, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 135, 'name': 'Date(s)'}, {'pk': 134, 'name': 'footer'}, {'pk': 131, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 137, 'name': 'Metadata'}, {'pk': 133, 'name': 'Motto'}, {'pk': 88, 'name': 'Platform'}, {'pk': 132, 'name': 'Pricing'}, {'pk': 38, 'name': 'text'}], 'valid_line_types': [{'pk': 10, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 2278, 'tags': [], 'created_at': '2024-04-10T10:34:01.277622Z', 'updated_at': '2024-08-06T18:49:19.483169Z', 'project_name': 'Gnazim Project', 'project_id': 9}, {'pk': 11, 'name': 'Small_Dataset', 'project': 'gnazim-project', 'transcriptions': [{'pk': 25, 'name': 'Default PAGE Import', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-28T19:17:27.420913Z', 'comments': None}, {'pk': 24, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-28T19:15:28.875792Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 80, 'name': 'Addition'}, {'pk': 81, 'name': 'Addition'}, {'pk': 83, 'name': 'Center Text'}, {'pk': 82, 'name': 'Center Text'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 84, 'name': 'Platform'}, {'pk': 85, 'name': 'Platform'}, {'pk': 39, 'name': 'text'}], 'valid_line_types': [{'pk': 11, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 20, 'tags': [], 'created_at': '2024-05-28T19:15:28.872983Z', 'updated_at': '2024-05-28T19:15:28.873001Z', 'project_name': 'Gnazim Project', 'project_id': 9}, {'pk': 12, 'name': 'Tamar Argentina', 'project': 'trial-project', 'transcriptions': [{'pk': 30, 'name': 'kraken:catmus-print-fondue-large', 'archived': False, 'avg_confidence': 0.9705650172928278, 'created_at': '2024-05-30T11:56:08.162518Z', 'comments': None}, {'pk': 29, 'name': 'kraken:catmus-print-fondue-tiny-2024-01-31', 'archived': False, 'avg_confidence': 0.9533827327695529, 'created_at': '2024-05-30T11:52:25.684026Z', 'comments': None}, {'pk': 28, 'name': 'kraken:catmus-print-fondue-small-2024-01-31', 'archived': False, 'avg_confidence': 0.960818562174577, 'created_at': '2024-05-30T11:42:10.874592Z', 'comments': None}, {'pk': 27, 'name': 'kraken:german_print', 'archived': False, 'avg_confidence': 0.9433775062648883, 'created_at': '2024-05-30T11:38:32.691970Z', 'comments': None}, {'pk': 26, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-30T11:28:11.579675Z', 'comments': None}], 'main_script': 'Latin', 'read_direction': 'ltr', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 40, 'name': 'text'}], 'valid_line_types': [{'pk': 12, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 1, 'tags': [], 'created_at': '2024-05-30T11:28:11.576842Z', 'updated_at': '2024-05-30T11:28:11.576863Z', 'project_name': 'Trial project', 'project_id': 2}, {'pk': 13, 'name': 'Combined GT', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 39, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9236902422941722, 'created_at': '2024-06-18T09:37:27.480945Z', 'comments': None}, {'pk': 34, 'name': 'transcript_2024-06-05', 'archived': False, 'avg_confidence': None, 'created_at': '2024-06-05T09:38:28.720476Z', 'comments': None}, {'pk': 31, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-06-05T09:06:06.671477Z', 'comments': None}], 'main_script': 'Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 50, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 53, 'name': 'Date(s)'}, {'pk': 51, 'name': 'footer'}, {'pk': 46, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 47, 'name': 'Metadata'}, {'pk': 48, 'name': 'Motto'}, {'pk': 54, 'name': 'other'}, {'pk': 52, 'name': 'paragraph'}, {'pk': 49, 'name': 'Pricing'}], 'valid_line_types': [{'pk': 16, 'name': 'paragraph'}], 'valid_part_types': [], 'parts_count': 24, 'tags': [], 'created_at': '2024-06-05T09:06:06.668592Z', 'updated_at': '2024-06-05T09:06:06.668614Z', 'project_name': 'Farsi Balslev', 'project_id': 5}]}\n",
      "\n",
      "This is the loop through the keys of the dictionary:\n",
      "count <class 'int'> 556\n",
      "next <class 'str'> http://env-9828609.us.reclaim.cloud/api/documents/?page=2\n",
      "previous <class 'NoneType'> None\n",
      "results <class 'list'> [{'pk': 1, 'name': 'Trial Farsi Project', 'project': 'trial-project', 'transcriptions': [{'pk': 1, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T12:24:55.658199Z', 'comments': None}], 'main_script': None, 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 7, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 9, 'name': 'Date(s)'}, {'pk': 6, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 10, 'name': 'Metadata'}, {'pk': 8, 'name': 'Motto'}, {'pk': 5, 'name': 'Pricing'}, {'pk': 27, 'name': 'text'}, {'pk': 28, 'name': 'text'}], 'valid_line_types': [{'pk': 7, 'name': 'default'}, {'pk': 6, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 88, 'tags': [], 'created_at': '2024-02-14T12:24:55.654204Z', 'updated_at': '2024-02-14T12:24:55.654222Z', 'project_name': 'Trial project', 'project_id': 2}, {'pk': 2, 'name': 'Habl_2-12', 'project': 'my-new-farsi-project', 'transcriptions': [{'pk': 6, 'name': 'kraken:All Arabic Scripts', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-15T09:47:10.330140Z', 'comments': None}, {'pk': 5, 'name': 'kraken:persian_best', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-15T09:45:43.901274Z', 'comments': None}, {'pk': 2, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:14:47.558084Z', 'comments': None}], 'main_script': 'Old Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 13, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 15, 'name': 'Date(s)'}, {'pk': 16, 'name': 'footer'}, {'pk': 12, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 14, 'name': 'Metadata'}, {'pk': 17, 'name': 'Motto'}, {'pk': 11, 'name': 'Pricing'}], 'valid_line_types': [], 'valid_part_types': [], 'parts_count': 44, 'tags': [], 'created_at': '2024-02-14T18:14:47.555238Z', 'updated_at': '2024-02-14T18:14:47.555259Z', 'project_name': 'My New Farsi Project', 'project_id': 4}, {'pk': 3, 'name': 'LA_GD_HBL_23_Ausgabe_12_Rabīʿ_āṯ-ṯānī_1325_5_Ḫordād_829_25_Mai_1907', 'project': 'my-new-farsi-project', 'transcriptions': [{'pk': 4, 'name': 'transcript', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:29:08.024746Z', 'comments': None}, {'pk': 3, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:26:40.171213Z', 'comments': None}], 'main_script': 'Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 24, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 22, 'name': 'Date(s)'}, {'pk': 25, 'name': 'footer'}, {'pk': 18, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 19, 'name': 'Metadata'}, {'pk': 21, 'name': 'Motto'}, {'pk': 23, 'name': 'other'}, {'pk': 20, 'name': 'Pricing'}], 'valid_line_types': [], 'valid_part_types': [], 'parts_count': 4, 'tags': [], 'created_at': '2024-02-14T18:26:40.168332Z', 'updated_at': '2024-10-09T09:20:51.478661Z', 'project_name': 'My New Farsi Project', 'project_id': 4}, {'pk': 4, 'name': 'Farsi HTR\\\\OCR Test', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 38, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9572961859096191, 'created_at': '2024-06-13T18:11:39.430417Z', 'comments': None}, {'pk': 14, 'name': 'kraken:Persian (persian_best base)', 'archived': False, 'avg_confidence': 0.9568122770999754, 'created_at': '2024-04-09T07:25:05.107953Z', 'comments': None}, {'pk': 36, 'name': 'kraken:first_persian_model', 'archived': False, 'avg_confidence': 0.9412393656690228, 'created_at': '2024-06-13T12:55:32.359312Z', 'comments': None}, {'pk': 11, 'name': 'Page new', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T10:10:07.983578Z', 'comments': None}, {'pk': 10, 'name': 'Page_7', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T10:02:04.152907Z', 'comments': None}, {'pk': 9, 'name': 'Page', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:48:35.924187Z', 'comments': None}, {'pk': 8, 'name': 'Default PAGE Import', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:34:09.422322Z', 'comments': None}, {'pk': 7, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:27:46.698316Z', 'comments': None}], 'main_script': 'Arabic', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 26, 'name': 'text'}], 'valid_line_types': [{'pk': 5, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 8, 'tags': [], 'created_at': '2024-02-19T09:27:46.695565Z', 'updated_at': '2024-02-19T09:27:46.695585Z', 'project_name': 'Farsi Balslev', 'project_id': 5}, {'pk': 7, 'name': 'Sur Israfil 1-11', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 17, 'name': 'kraken:Persian (persian_best base)', 'archived': False, 'avg_confidence': 0.9374685632370211, 'created_at': '2024-04-09T07:47:35.281438Z', 'comments': None}, {'pk': 37, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9559087378969109, 'created_at': '2024-06-13T17:39:13.366981Z', 'comments': None}, {'pk': 35, 'name': 'kraken:first_persian_model', 'archived': False, 'avg_confidence': 0.9074163640921221, 'created_at': '2024-06-13T12:29:49.657423Z', 'comments': None}, {'pk': 16, 'name': 'layout', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-09T07:43:35.716440Z', 'comments': None}, {'pk': 15, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-09T07:42:01.243240Z', 'comments': None}], 'main_script': 'Arabic', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 64, 'name': 'column'}, {'pk': 31, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 34, 'name': 'Date(s)'}, {'pk': 35, 'name': 'footer'}, {'pk': 67, 'name': 'footnote'}, {'pk': 29, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 33, 'name': 'Metadata'}, {'pk': 32, 'name': 'Motto'}, {'pk': 66, 'name': 'pagefooter'}, {'pk': 63, 'name': 'Pagenumber'}, {'pk': 30, 'name': 'Pricing'}, {'pk': 69, 'name': 'tab-header'}, {'pk': 68, 'name': 'tab-header-sup'}, {'pk': 36, 'name': 'text'}, {'pk': 65, 'name': 'Title'}], 'valid_line_types': [{'pk': 21, 'name': 'begin-article'}, {'pk': 22, 'name': 'begin-paragraph'}, {'pk': 8, 'name': 'default'}, {'pk': 23, 'name': 'h2'}], 'valid_part_types': [], 'parts_count': 90, 'tags': [], 'created_at': '2024-04-09T07:42:01.022926Z', 'updated_at': '2024-04-09T07:42:01.022944Z', 'project_name': 'Farsi Balslev', 'project_id': 5}, {'pk': 8, 'name': 'Uri_Train_Set', 'project': 'gnazim-project', 'transcriptions': [{'pk': 20, 'name': 'XML Import II', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:15:35.138131Z', 'comments': None}, {'pk': 19, 'name': 'XML Import I', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:07:02.945464Z', 'comments': None}, {'pk': 18, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T09:55:19.684869Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 1, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 43, 'name': 'Addition'}, {'pk': 42, 'name': 'Center Text'}, {'pk': 1, 'name': 'Header'}, {'pk': 41, 'name': 'Platform'}], 'valid_line_types': [{'pk': 9, 'name': 'default'}, {'pk': 27, 'name': 'Fragmented'}, {'pk': 20, 'name': 'Handwritten'}], 'valid_part_types': [], 'parts_count': 250, 'tags': [{'pk': 6, 'name': 'GroundTruth', 'color': '#763f75'}], 'created_at': '2024-04-10T09:55:19.681656Z', 'updated_at': '2024-10-08T07:06:33.343832Z', 'project_name': 'Gnazim Project', 'project_id': 9}, {'pk': 9, 'name': 'Big Dataset', 'project': 'gnazim-project', 'transcriptions': [{'pk': 22, 'name': 'transcript_2024-04-10', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:55:45.891458Z', 'comments': None}, {'pk': 21, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:34:01.293492Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 86, 'name': 'Addition'}, {'pk': 87, 'name': 'Center Text'}, {'pk': 136, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 135, 'name': 'Date(s)'}, {'pk': 134, 'name': 'footer'}, {'pk': 131, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 137, 'name': 'Metadata'}, {'pk': 133, 'name': 'Motto'}, {'pk': 88, 'name': 'Platform'}, {'pk': 132, 'name': 'Pricing'}, {'pk': 38, 'name': 'text'}], 'valid_line_types': [{'pk': 10, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 2278, 'tags': [], 'created_at': '2024-04-10T10:34:01.277622Z', 'updated_at': '2024-08-06T18:49:19.483169Z', 'project_name': 'Gnazim Project', 'project_id': 9}, {'pk': 11, 'name': 'Small_Dataset', 'project': 'gnazim-project', 'transcriptions': [{'pk': 25, 'name': 'Default PAGE Import', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-28T19:17:27.420913Z', 'comments': None}, {'pk': 24, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-28T19:15:28.875792Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 80, 'name': 'Addition'}, {'pk': 81, 'name': 'Addition'}, {'pk': 83, 'name': 'Center Text'}, {'pk': 82, 'name': 'Center Text'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 84, 'name': 'Platform'}, {'pk': 85, 'name': 'Platform'}, {'pk': 39, 'name': 'text'}], 'valid_line_types': [{'pk': 11, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 20, 'tags': [], 'created_at': '2024-05-28T19:15:28.872983Z', 'updated_at': '2024-05-28T19:15:28.873001Z', 'project_name': 'Gnazim Project', 'project_id': 9}, {'pk': 12, 'name': 'Tamar Argentina', 'project': 'trial-project', 'transcriptions': [{'pk': 30, 'name': 'kraken:catmus-print-fondue-large', 'archived': False, 'avg_confidence': 0.9705650172928278, 'created_at': '2024-05-30T11:56:08.162518Z', 'comments': None}, {'pk': 29, 'name': 'kraken:catmus-print-fondue-tiny-2024-01-31', 'archived': False, 'avg_confidence': 0.9533827327695529, 'created_at': '2024-05-30T11:52:25.684026Z', 'comments': None}, {'pk': 28, 'name': 'kraken:catmus-print-fondue-small-2024-01-31', 'archived': False, 'avg_confidence': 0.960818562174577, 'created_at': '2024-05-30T11:42:10.874592Z', 'comments': None}, {'pk': 27, 'name': 'kraken:german_print', 'archived': False, 'avg_confidence': 0.9433775062648883, 'created_at': '2024-05-30T11:38:32.691970Z', 'comments': None}, {'pk': 26, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-30T11:28:11.579675Z', 'comments': None}], 'main_script': 'Latin', 'read_direction': 'ltr', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 40, 'name': 'text'}], 'valid_line_types': [{'pk': 12, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 1, 'tags': [], 'created_at': '2024-05-30T11:28:11.576842Z', 'updated_at': '2024-05-30T11:28:11.576863Z', 'project_name': 'Trial project', 'project_id': 2}, {'pk': 13, 'name': 'Combined GT', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 39, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9236902422941722, 'created_at': '2024-06-18T09:37:27.480945Z', 'comments': None}, {'pk': 34, 'name': 'transcript_2024-06-05', 'archived': False, 'avg_confidence': None, 'created_at': '2024-06-05T09:38:28.720476Z', 'comments': None}, {'pk': 31, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-06-05T09:06:06.671477Z', 'comments': None}], 'main_script': 'Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 50, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 53, 'name': 'Date(s)'}, {'pk': 51, 'name': 'footer'}, {'pk': 46, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 47, 'name': 'Metadata'}, {'pk': 48, 'name': 'Motto'}, {'pk': 54, 'name': 'other'}, {'pk': 52, 'name': 'paragraph'}, {'pk': 49, 'name': 'Pricing'}], 'valid_line_types': [{'pk': 16, 'name': 'paragraph'}], 'valid_part_types': [], 'parts_count': 24, 'tags': [], 'created_at': '2024-06-05T09:06:06.668592Z', 'updated_at': '2024-06-05T09:06:06.668614Z', 'project_name': 'Farsi Balslev', 'project_id': 5}]\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "r=requests.get(url=doc_url,headers=headers)\n",
    "docs_dict = json.loads(r.content)\n",
    "print('This is the full data returned - a dictionary:')\n",
    "print(docs_dict)\n",
    "print()\n",
    "print('This is the loop through the keys of the dictionary:')\n",
    "for key in docs_dict:\n",
    "  print(key,type(docs_dict[key]),docs_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnKoSqLD8Jgv"
   },
   "source": [
    "Results is a `list`. Each of your documents is an entry in this list, so let's loop through the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1732774214416,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "8jMZGfYz6rGO",
    "outputId": "d67ca00b-5ac9-4f60-e5d7-04ca57a34a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'pk': 1, 'name': 'Trial Farsi Project', 'project': 'trial-project', 'transcriptions': [{'pk': 1, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T12:24:55.658199Z', 'comments': None}], 'main_script': None, 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 7, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 9, 'name': 'Date(s)'}, {'pk': 6, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 10, 'name': 'Metadata'}, {'pk': 8, 'name': 'Motto'}, {'pk': 5, 'name': 'Pricing'}, {'pk': 27, 'name': 'text'}, {'pk': 28, 'name': 'text'}], 'valid_line_types': [{'pk': 7, 'name': 'default'}, {'pk': 6, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 88, 'tags': [], 'created_at': '2024-02-14T12:24:55.654204Z', 'updated_at': '2024-02-14T12:24:55.654222Z', 'project_name': 'Trial project', 'project_id': 2}\n",
      "1 {'pk': 2, 'name': 'Habl_2-12', 'project': 'my-new-farsi-project', 'transcriptions': [{'pk': 6, 'name': 'kraken:All Arabic Scripts', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-15T09:47:10.330140Z', 'comments': None}, {'pk': 5, 'name': 'kraken:persian_best', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-15T09:45:43.901274Z', 'comments': None}, {'pk': 2, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:14:47.558084Z', 'comments': None}], 'main_script': 'Old Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 13, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 15, 'name': 'Date(s)'}, {'pk': 16, 'name': 'footer'}, {'pk': 12, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 14, 'name': 'Metadata'}, {'pk': 17, 'name': 'Motto'}, {'pk': 11, 'name': 'Pricing'}], 'valid_line_types': [], 'valid_part_types': [], 'parts_count': 44, 'tags': [], 'created_at': '2024-02-14T18:14:47.555238Z', 'updated_at': '2024-02-14T18:14:47.555259Z', 'project_name': 'My New Farsi Project', 'project_id': 4}\n",
      "2 {'pk': 3, 'name': 'LA_GD_HBL_23_Ausgabe_12_Rabīʿ_āṯ-ṯānī_1325_5_Ḫordād_829_25_Mai_1907', 'project': 'my-new-farsi-project', 'transcriptions': [{'pk': 4, 'name': 'transcript', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:29:08.024746Z', 'comments': None}, {'pk': 3, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-14T18:26:40.171213Z', 'comments': None}], 'main_script': 'Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 24, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 22, 'name': 'Date(s)'}, {'pk': 25, 'name': 'footer'}, {'pk': 18, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 19, 'name': 'Metadata'}, {'pk': 21, 'name': 'Motto'}, {'pk': 23, 'name': 'other'}, {'pk': 20, 'name': 'Pricing'}], 'valid_line_types': [], 'valid_part_types': [], 'parts_count': 4, 'tags': [], 'created_at': '2024-02-14T18:26:40.168332Z', 'updated_at': '2024-10-09T09:20:51.478661Z', 'project_name': 'My New Farsi Project', 'project_id': 4}\n",
      "3 {'pk': 4, 'name': 'Farsi HTR\\\\OCR Test', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 38, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9572961859096191, 'created_at': '2024-06-13T18:11:39.430417Z', 'comments': None}, {'pk': 14, 'name': 'kraken:Persian (persian_best base)', 'archived': False, 'avg_confidence': 0.9568122770999754, 'created_at': '2024-04-09T07:25:05.107953Z', 'comments': None}, {'pk': 36, 'name': 'kraken:first_persian_model', 'archived': False, 'avg_confidence': 0.9412393656690228, 'created_at': '2024-06-13T12:55:32.359312Z', 'comments': None}, {'pk': 11, 'name': 'Page new', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T10:10:07.983578Z', 'comments': None}, {'pk': 10, 'name': 'Page_7', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T10:02:04.152907Z', 'comments': None}, {'pk': 9, 'name': 'Page', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:48:35.924187Z', 'comments': None}, {'pk': 8, 'name': 'Default PAGE Import', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:34:09.422322Z', 'comments': None}, {'pk': 7, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-02-19T09:27:46.698316Z', 'comments': None}], 'main_script': 'Arabic', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 26, 'name': 'text'}], 'valid_line_types': [{'pk': 5, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 8, 'tags': [], 'created_at': '2024-02-19T09:27:46.695565Z', 'updated_at': '2024-02-19T09:27:46.695585Z', 'project_name': 'Farsi Balslev', 'project_id': 5}\n",
      "4 {'pk': 7, 'name': 'Sur Israfil 1-11', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 17, 'name': 'kraken:Persian (persian_best base)', 'archived': False, 'avg_confidence': 0.9374685632370211, 'created_at': '2024-04-09T07:47:35.281438Z', 'comments': None}, {'pk': 37, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9559087378969109, 'created_at': '2024-06-13T17:39:13.366981Z', 'comments': None}, {'pk': 35, 'name': 'kraken:first_persian_model', 'archived': False, 'avg_confidence': 0.9074163640921221, 'created_at': '2024-06-13T12:29:49.657423Z', 'comments': None}, {'pk': 16, 'name': 'layout', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-09T07:43:35.716440Z', 'comments': None}, {'pk': 15, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-09T07:42:01.243240Z', 'comments': None}], 'main_script': 'Arabic', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 64, 'name': 'column'}, {'pk': 31, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 34, 'name': 'Date(s)'}, {'pk': 35, 'name': 'footer'}, {'pk': 67, 'name': 'footnote'}, {'pk': 29, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 33, 'name': 'Metadata'}, {'pk': 32, 'name': 'Motto'}, {'pk': 66, 'name': 'pagefooter'}, {'pk': 63, 'name': 'Pagenumber'}, {'pk': 30, 'name': 'Pricing'}, {'pk': 69, 'name': 'tab-header'}, {'pk': 68, 'name': 'tab-header-sup'}, {'pk': 36, 'name': 'text'}, {'pk': 65, 'name': 'Title'}], 'valid_line_types': [{'pk': 21, 'name': 'begin-article'}, {'pk': 22, 'name': 'begin-paragraph'}, {'pk': 8, 'name': 'default'}, {'pk': 23, 'name': 'h2'}], 'valid_part_types': [], 'parts_count': 90, 'tags': [], 'created_at': '2024-04-09T07:42:01.022926Z', 'updated_at': '2024-04-09T07:42:01.022944Z', 'project_name': 'Farsi Balslev', 'project_id': 5}\n",
      "5 {'pk': 8, 'name': 'Uri_Train_Set', 'project': 'gnazim-project', 'transcriptions': [{'pk': 20, 'name': 'XML Import II', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:15:35.138131Z', 'comments': None}, {'pk': 19, 'name': 'XML Import I', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:07:02.945464Z', 'comments': None}, {'pk': 18, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T09:55:19.684869Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 1, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 43, 'name': 'Addition'}, {'pk': 42, 'name': 'Center Text'}, {'pk': 1, 'name': 'Header'}, {'pk': 41, 'name': 'Platform'}], 'valid_line_types': [{'pk': 9, 'name': 'default'}, {'pk': 27, 'name': 'Fragmented'}, {'pk': 20, 'name': 'Handwritten'}], 'valid_part_types': [], 'parts_count': 250, 'tags': [{'pk': 6, 'name': 'GroundTruth', 'color': '#763f75'}], 'created_at': '2024-04-10T09:55:19.681656Z', 'updated_at': '2024-10-08T07:06:33.343832Z', 'project_name': 'Gnazim Project', 'project_id': 9}\n",
      "6 {'pk': 9, 'name': 'Big Dataset', 'project': 'gnazim-project', 'transcriptions': [{'pk': 22, 'name': 'transcript_2024-04-10', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:55:45.891458Z', 'comments': None}, {'pk': 21, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:34:01.293492Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 86, 'name': 'Addition'}, {'pk': 87, 'name': 'Center Text'}, {'pk': 136, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 135, 'name': 'Date(s)'}, {'pk': 134, 'name': 'footer'}, {'pk': 131, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 137, 'name': 'Metadata'}, {'pk': 133, 'name': 'Motto'}, {'pk': 88, 'name': 'Platform'}, {'pk': 132, 'name': 'Pricing'}, {'pk': 38, 'name': 'text'}], 'valid_line_types': [{'pk': 10, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 2278, 'tags': [], 'created_at': '2024-04-10T10:34:01.277622Z', 'updated_at': '2024-08-06T18:49:19.483169Z', 'project_name': 'Gnazim Project', 'project_id': 9}\n",
      "7 {'pk': 11, 'name': 'Small_Dataset', 'project': 'gnazim-project', 'transcriptions': [{'pk': 25, 'name': 'Default PAGE Import', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-28T19:17:27.420913Z', 'comments': None}, {'pk': 24, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-28T19:15:28.875792Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 80, 'name': 'Addition'}, {'pk': 81, 'name': 'Addition'}, {'pk': 83, 'name': 'Center Text'}, {'pk': 82, 'name': 'Center Text'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 84, 'name': 'Platform'}, {'pk': 85, 'name': 'Platform'}, {'pk': 39, 'name': 'text'}], 'valid_line_types': [{'pk': 11, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 20, 'tags': [], 'created_at': '2024-05-28T19:15:28.872983Z', 'updated_at': '2024-05-28T19:15:28.873001Z', 'project_name': 'Gnazim Project', 'project_id': 9}\n",
      "8 {'pk': 12, 'name': 'Tamar Argentina', 'project': 'trial-project', 'transcriptions': [{'pk': 30, 'name': 'kraken:catmus-print-fondue-large', 'archived': False, 'avg_confidence': 0.9705650172928278, 'created_at': '2024-05-30T11:56:08.162518Z', 'comments': None}, {'pk': 29, 'name': 'kraken:catmus-print-fondue-tiny-2024-01-31', 'archived': False, 'avg_confidence': 0.9533827327695529, 'created_at': '2024-05-30T11:52:25.684026Z', 'comments': None}, {'pk': 28, 'name': 'kraken:catmus-print-fondue-small-2024-01-31', 'archived': False, 'avg_confidence': 0.960818562174577, 'created_at': '2024-05-30T11:42:10.874592Z', 'comments': None}, {'pk': 27, 'name': 'kraken:german_print', 'archived': False, 'avg_confidence': 0.9433775062648883, 'created_at': '2024-05-30T11:38:32.691970Z', 'comments': None}, {'pk': 26, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-05-30T11:28:11.579675Z', 'comments': None}], 'main_script': 'Latin', 'read_direction': 'ltr', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 3, 'name': 'Commentary'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 40, 'name': 'text'}], 'valid_line_types': [{'pk': 12, 'name': 'default'}], 'valid_part_types': [], 'parts_count': 1, 'tags': [], 'created_at': '2024-05-30T11:28:11.576842Z', 'updated_at': '2024-05-30T11:28:11.576863Z', 'project_name': 'Trial project', 'project_id': 2}\n",
      "9 {'pk': 13, 'name': 'Combined GT', 'project': 'farsi-balslev', 'transcriptions': [{'pk': 39, 'name': 'kraken:second_persian_model_best', 'archived': False, 'avg_confidence': 0.9236902422941722, 'created_at': '2024-06-18T09:37:27.480945Z', 'comments': None}, {'pk': 34, 'name': 'transcript_2024-06-05', 'archived': False, 'avg_confidence': None, 'created_at': '2024-06-05T09:38:28.720476Z', 'comments': None}, {'pk': 31, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-06-05T09:06:06.671477Z', 'comments': None}], 'main_script': 'Persian', 'read_direction': 'rtl', 'line_offset': 0, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 50, 'name': 'Column'}, {'pk': 3, 'name': 'Commentary'}, {'pk': 53, 'name': 'Date(s)'}, {'pk': 51, 'name': 'footer'}, {'pk': 46, 'name': 'header'}, {'pk': 1, 'name': 'Header'}, {'pk': 4, 'name': 'Illustration'}, {'pk': 2, 'name': 'Main'}, {'pk': 47, 'name': 'Metadata'}, {'pk': 48, 'name': 'Motto'}, {'pk': 54, 'name': 'other'}, {'pk': 52, 'name': 'paragraph'}, {'pk': 49, 'name': 'Pricing'}], 'valid_line_types': [{'pk': 16, 'name': 'paragraph'}], 'valid_part_types': [], 'parts_count': 24, 'tags': [], 'created_at': '2024-06-05T09:06:06.668592Z', 'updated_at': '2024-06-05T09:06:06.668614Z', 'project_name': 'Farsi Balslev', 'project_id': 5}\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "for i,doc in enumerate(docs_dict['results']):\n",
    "  print(i,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHj5yqL6IaiV"
   },
   "source": [
    "`doc_dict` contains the json you received from eScriptorium into a dictionary that you can query like any other dictionary.\n",
    "\n",
    "E.g. get a list of the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1728290655342,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "F0nunmFJIoFu",
    "outputId": "ef6a3b06-614a-4a26-aac6-fbc01f1abfb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count type: <class 'int'>\n",
      "next type: <class 'str'>\n",
      "previous type: <class 'NoneType'>\n",
      "results type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for key in docs_dict:\n",
    "  print(key,'type:',type(docs_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXYnniBpI9RJ"
   },
   "source": [
    "There are pages like this not only for documents, but also for parts (images),\n",
    "regions, lines, transcriptions, the different ontology items (regiontypes, linetypes, annotations etc). They are all built in the same way.\n",
    "\n",
    "*   `count` gives the total number of elements. 41 in my case means that I have 41 documents.\n",
    "*   `next` indicates the url where you can get the next 10 elements if you have more than 10. If not, it says `None`.\n",
    "*   `previous` indicates the url where you can get the previous 10 elements if there are any. If not, it says `None`.\n",
    "*   `results` is a list with the information for each of the 10 documents stored as another dictionary. Let's look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1732774276733,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "5ErNhEXsMX6V",
    "outputId": "485d0f96-e2b1-411c-f2e1-731fd1dfbd61"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'http://env-9828609.us.reclaim.cloud/api/documents/?page=2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_dict['next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1728372608507,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "i6rOrEdMJ9-E",
    "outputId": "e4f0646a-f14c-4703-ec81-daddef0cff24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get document segmentation ontology for document:  8\n",
      "https://env-9828609.us.reclaim.cloud/api/documents/8/\n",
      "Document: 8  with  250  parts\n",
      "region types: [{'pk': 43, 'name': 'Addition'}, {'pk': 42, 'name': 'Center Text'}, {'pk': 1, 'name': 'Header'}, {'pk': 41, 'name': 'Platform'}]\n",
      "line types: [{'pk': 9, 'name': 'default'}, {'pk': 27, 'name': 'Fragmented'}, {'pk': 20, 'name': 'Handwritten'}]\n",
      "transcription_level_list: [{'pk': 20, 'name': 'XML Import II', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:15:35.138131Z', 'comments': None}, {'pk': 19, 'name': 'XML Import I', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:07:02.945464Z', 'comments': None}, {'pk': 18, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T09:55:19.684869Z', 'comments': None}]\n",
      "{'pk': 8, 'name': 'Uri_Train_Set', 'project': 'gnazim-project', 'transcriptions': [{'pk': 20, 'name': 'XML Import II', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:15:35.138131Z', 'comments': None}, {'pk': 19, 'name': 'XML Import I', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:07:02.945464Z', 'comments': None}, {'pk': 18, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T09:55:19.684869Z', 'comments': None}], 'main_script': 'Hebrew', 'read_direction': 'rtl', 'line_offset': 1, 'show_confidence_viz': False, 'valid_block_types': [{'pk': 43, 'name': 'Addition'}, {'pk': 42, 'name': 'Center Text'}, {'pk': 1, 'name': 'Header'}, {'pk': 41, 'name': 'Platform'}], 'valid_line_types': [{'pk': 9, 'name': 'default'}, {'pk': 27, 'name': 'Fragmented'}, {'pk': 20, 'name': 'Handwritten'}], 'valid_part_types': [], 'parts_count': 250, 'tags': [{'pk': 6, 'name': 'GroundTruth', 'color': '#763f75'}], 'created_at': '2024-04-10T09:55:19.681656Z', 'updated_at': '2024-10-08T07:06:33.343832Z', 'project_name': 'Gnazim Project', 'project_id': 9}\n",
      "\n",
      "And here are the keys in the document dictionary, their variable types and contents\n",
      "pk                   <class 'int'>        8\n",
      "name                 <class 'str'>        Uri_Train_Set\n",
      "project              <class 'str'>        gnazim-project\n",
      "transcriptions       <class 'list'>       [{'pk': 20, 'name': 'XML Import II', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:15:35.138131Z', 'comments': None}, {'pk': 19, 'name': 'XML Import I', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:07:02.945464Z', 'comments': None}, {'pk': 18, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T09:55:19.684869Z', 'comments': None}]\n",
      "main_script          <class 'str'>        Hebrew\n",
      "read_direction       <class 'str'>        rtl\n",
      "line_offset          <class 'int'>        1\n",
      "show_confidence_viz  <class 'bool'>       False\n",
      "valid_block_types    <class 'list'>       [{'pk': 43, 'name': 'Addition'}, {'pk': 42, 'name': 'Center Text'}, {'pk': 1, 'name': 'Header'}, {'pk': 41, 'name': 'Platform'}]\n",
      "valid_line_types     <class 'list'>       [{'pk': 9, 'name': 'default'}, {'pk': 27, 'name': 'Fragmented'}, {'pk': 20, 'name': 'Handwritten'}]\n",
      "valid_part_types     <class 'list'>       []\n",
      "parts_count          <class 'int'>        250\n",
      "tags                 <class 'list'>       [{'pk': 6, 'name': 'GroundTruth', 'color': '#763f75'}]\n",
      "created_at           <class 'str'>        2024-04-10T09:55:19.681656Z\n",
      "updated_at           <class 'str'>        2024-10-08T07:06:33.343832Z\n",
      "project_name         <class 'str'>        Gnazim Project\n",
      "project_id           <class 'int'>        9\n"
     ]
    }
   ],
   "source": [
    "doc = docs_dict['results'][5]\n",
    "get_basic_info(doc['pk'])\n",
    "print(doc)\n",
    "print()\n",
    "print('And here are the keys in the document dictionary, their variable types and contents')\n",
    "for key in doc:\n",
    "  print(key.ljust(20,' '),str(type(doc[key])).ljust(20,' '),doc[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6192,
     "status": "ok",
     "timestamp": 1728372638966,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "dQ7U7rYbPI9r",
    "outputId": "b1b79f36-056d-46f1-9292-49b7e078d1ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pk': 241, 'name': '', 'filename': 'IDGNAZIM000233.tif', 'title': 'Element 1', 'typology': None, 'image': {'uri': '/media/documents/8/IDGNAZIM000233.tif', 'size': [1193, 833], 'thumbnails': {'card': '/media/documents/8/IDGNAZIM000233.tif.180x180_q85_crop-smart.jpg', 'large': '/media/documents/8/IDGNAZIM000233.tif.1000x1000_q85.jpg'}}, 'image_file_size': 4089, 'original_filename': 'IDGNAZIM000233.tif', 'bw_image': None, 'workflow': {}, 'order': 0, 'recoverable': False, 'transcription_progress': 0, 'source': '', 'max_avg_confidence': None, 'comments': None, 'updated_at': '2024-05-26T19:02:58.731408Z'}\n",
      "241\n",
      "nothing to associate\n",
      "{'pk': 242, 'name': '', 'filename': 'IDGNAZIM000234.tif', 'title': 'Element 2', 'typology': None, 'image': {'uri': '/media/documents/8/IDGNAZIM000234.tif', 'size': [1276, 894], 'thumbnails': {'card': '/media/documents/8/IDGNAZIM000234.tif.180x180_q85_crop-smart.jpg', 'large': '/media/documents/8/IDGNAZIM000234.tif.1000x1000_q85.jpg'}}, 'image_file_size': 5049, 'original_filename': 'IDGNAZIM000234.tif', 'bw_image': None, 'workflow': {}, 'order': 1, 'recoverable': False, 'transcription_progress': 0, 'source': '', 'max_avg_confidence': None, 'comments': None, 'updated_at': '2024-04-10T09:56:18.702268Z'}\n",
      "242\n",
      "nothing to associate\n",
      "{'pk': 243, 'name': '', 'filename': 'IDGNAZIM000235.tif', 'title': 'Element 3', 'typology': None, 'image': {'uri': '/media/documents/8/IDGNAZIM000235.tif', 'size': [1170, 787], 'thumbnails': {'card': '/media/documents/8/IDGNAZIM000235.tif.180x180_q85_crop-smart.jpg', 'large': '/media/documents/8/IDGNAZIM000235.tif.1000x1000_q85.jpg'}}, 'image_file_size': 2825, 'original_filename': 'IDGNAZIM000235.tif', 'bw_image': None, 'workflow': {}, 'order': 2, 'recoverable': False, 'transcription_progress': 0, 'source': '', 'max_avg_confidence': None, 'comments': None, 'updated_at': '2024-04-10T09:56:18.932726Z'}\n",
      "243\n",
      "nothing to associate\n",
      "{'pk': 244, 'name': '', 'filename': 'IDGNAZIM000236.tif', 'title': 'Element 4', 'typology': None, 'image': {'uri': '/media/documents/8/IDGNAZIM000236.tif', 'size': [1219, 871], 'thumbnails': {'card': '/media/documents/8/IDGNAZIM000236.tif.180x180_q85_crop-smart.jpg', 'large': '/media/documents/8/IDGNAZIM000236.tif.1000x1000_q85.jpg'}}, 'image_file_size': 2639, 'original_filename': 'IDGNAZIM000236.tif', 'bw_image': None, 'workflow': {}, 'order': 3, 'recoverable': False, 'transcription_progress': 0, 'source': '', 'max_avg_confidence': None, 'comments': None, 'updated_at': '2024-04-10T09:56:19.154831Z'}\n",
      "244\n",
      "nothing to associate\n",
      "{'pk': 245, 'name': '', 'filename': 'IDGNAZIM000237.tif', 'title': 'Element 5', 'typology': None, 'image': {'uri': '/media/documents/8/IDGNAZIM000237.tif', 'size': [1267, 883], 'thumbnails': {'card': '/media/documents/8/IDGNAZIM000237.tif.180x180_q85_crop-smart.jpg', 'large': '/media/documents/8/IDGNAZIM000237.tif.1000x1000_q85.jpg'}}, 'image_file_size': 3052, 'original_filename': 'IDGNAZIM000237.tif', 'bw_image': None, 'workflow': {}, 'order': 4, 'recoverable': True, 'transcription_progress': 0, 'source': '', 'max_avg_confidence': None, 'comments': None, 'updated_at': '2024-04-10T09:56:19.372517Z'}\n",
      "245\n",
      "nothing to associate\n"
     ]
    }
   ],
   "source": [
    "doc_pk=8\n",
    "parts=get_all_parts(doc_pk)\n",
    "for part in parts[:5]:\n",
    "  print(part)\n",
    "  #associate_lines_with_existing_regions_and_reorder(doc_pk,part['pk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yhhROakZrx6"
   },
   "source": [
    "You can get information about anything concerning parts (=images). The most important for us are the pk (=primary key, i.e. the ID in the database that you need to do anything with anything related to this part, e.g. the regions, lines or transcriptions).\n",
    "\n",
    "For beginners the most important keys are:\n",
    "* pk\n",
    "* filename (the string for the image name, which you can not modify)\n",
    "* image (a dictionary containing among others the height and width in pixels)\n",
    "* order (a integer for the order in the overall document, which you can also\n",
    "modify)\n",
    "\n",
    "For more advanced users there is also:\n",
    "* name (a string for what is normally shown as \"Element N\", which you can modify to what you like, e.g. foliation : \"fol. 46b\")\n",
    "* comments (a string where you can write anything you want that will appear in the part metadata panel)\n",
    "* max_avg_confidence (a float [= a number giving the average precision for the automatic model if this feature is turned on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1728291536997,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "GB7-iB0gPyH0",
    "outputId": "8b43f18d-321f-45ce-9cdf-72546bd48fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pk <class 'int'> 245\n",
      "name <class 'str'> \n",
      "filename <class 'str'> IDGNAZIM000237.tif\n",
      "title <class 'str'> Element 5\n",
      "typology <class 'NoneType'> None\n",
      "image <class 'dict'> {'uri': '/media/documents/8/IDGNAZIM000237.tif', 'size': [1267, 883], 'thumbnails': {'card': '/media/documents/8/IDGNAZIM000237.tif.180x180_q85_crop-smart.jpg', 'large': '/media/documents/8/IDGNAZIM000237.tif.1000x1000_q85.jpg'}}\n",
      "image_file_size <class 'int'> 3052\n",
      "original_filename <class 'str'> IDGNAZIM000237.tif\n",
      "bw_image <class 'NoneType'> None\n",
      "workflow <class 'dict'> {}\n",
      "order <class 'int'> 4\n",
      "recoverable <class 'bool'> False\n",
      "transcription_progress <class 'int'> 0\n",
      "source <class 'str'> \n",
      "max_avg_confidence <class 'NoneType'> None\n",
      "comments <class 'NoneType'> None\n",
      "updated_at <class 'str'> 2024-04-10T09:56:19.372517Z\n"
     ]
    }
   ],
   "source": [
    "for key in part:\n",
    "  print(key,type(part[key]),part[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bukJNEiwdONJ"
   },
   "source": [
    "If you have both the pk of the document AND the pk of the part you can access the regions.\n",
    "\n",
    "There are currently 5 keys in the region dictionary\n",
    "* pk - primary key (ID)\n",
    "* document part - the pk of the part to which the regions belongs\n",
    "* external_id - a string which you can modify to give the region a readable name, e.g. fol_46b_region_001\n",
    "* order - an integer describing the order of regions on this part\n",
    "* box - a list of points describing the polygon of the region\n",
    "* typology - a pk for the ontology of regiontypes\n",
    "\n",
    "All fields except pk are writable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1728291664089,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "s036DOtrcgrs",
    "outputId": "87b1006c-5b4f-4ef3-e3f2-be781831c83f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 245\n",
      "There are 4 region(s) stored in a list of dictionaries that looks as follows.\n",
      "{'pk': 2852, 'document_part': 245, 'external_id': 'eSc_textblock_3f20c0bc', 'order': 0, 'box': [[867, 214], [864, 69], [1232, 69], [1232, 214]], 'typology': 1}\n",
      "{'pk': 2853, 'document_part': 245, 'external_id': 'eSc_textblock_eb6a91d1', 'order': 1, 'box': [[1084, 265], [1084, 394], [637, 394], [117, 394], [117, 265]], 'typology': 42}\n",
      "{'pk': 2854, 'document_part': 245, 'external_id': 'eSc_textblock_3297496c', 'order': 2, 'box': [[1024, 548], [1024, 706], [439, 706], [252, 706], [252, 548]], 'typology': 41}\n",
      "{'pk': 2977, 'document_part': 245, 'external_id': 'eSc_textblock_d163a16a', 'order': 3, 'box': [[1034, 344], [1034, 323], [1027, 323], [1027, 344]], 'typology': None}\n",
      "\n",
      "The first region has the following keys, valuetypes and variable content.\n",
      "pk <class 'int'> 2852\n",
      "document_part <class 'int'> 245\n",
      "external_id <class 'str'> eSc_textblock_3f20c0bc\n",
      "order <class 'int'> 0\n",
      "box <class 'list'> [[867, 214], [864, 69], [1232, 69], [1232, 214]]\n",
      "typology <class 'int'> 1\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "print(doc_pk,part['pk'])\n",
    "regions = get_all_regions_of_part(doc_pk,part['pk'])\n",
    "print('There are',len(regions),'region(s) stored in a list of dictionaries that looks as follows.')\n",
    "for region in regions:\n",
    "  print(region)\n",
    "print()\n",
    "print('The first region has the following keys, valuetypes and variable content.')\n",
    "for key in regions[0]:\n",
    "  print(key,type(regions[0][key]),regions[0][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTkVS8jWfTAd"
   },
   "source": [
    "If you have both the pk of the document AND the pk of the part you can access also the lines.\n",
    "\n",
    "There are currently 8 keys in the lines dictionary\n",
    "* pk - primary key (ID)\n",
    "* document part - the pk of the part to which the line belongs\n",
    "* external_id - a string which you can modify to give the line a readable name, e.g. fol_46b_line_001\n",
    "* order - an integer describing the order of lines on this part\n",
    "* region - the pk of the region with which this line has been linked (\"None\" if it is unlinked)\n",
    "* baseline - a list of points describing the baseline of the line\n",
    "* mask - a list of points describing the polygon around the line\n",
    "* typology - a pk for the ontology of linetypes\n",
    "\n",
    "All fields except pk are writable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1728372695627,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "5fsPrwF4dMn3",
    "outputId": "bf31e8a0-919b-40b3-ef69-94c858ba73f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 245\n",
      "There are 2 lines(s) stored in a list of dictionaries that looks as follows.\n",
      "2\n",
      "{'pk': 6239, 'document_part': 245, 'external_id': 'eSc_line_d38c7e55', 'order': 1, 'region': 2853, 'baseline': [[129, 325], [192, 331], [457, 331], [785, 337], [983, 337], [1015, 337]], 'mask': [[128, 324], [128, 338], [162, 350], [212, 350], [223, 345], [252, 350], [273, 340], [294, 350], [315, 351], [337, 342], [354, 350], [373, 346], [387, 353], [410, 353], [443, 341], [466, 353], [486, 353], [506, 343], [526, 353], [582, 348], [602, 352], [621, 343], [641, 350], [656, 346], [667, 352], [692, 348], [703, 352], [718, 348], [753, 351], [773, 345], [823, 353], [842, 345], [906, 351], [929, 346], [963, 351], [1013, 346], [1014, 336], [1013, 325], [943, 317], [876, 317], [853, 323], [838, 316], [798, 316], [775, 323], [730, 314], [706, 314], [691, 320], [666, 317], [654, 320], [641, 316], [630, 321], [566, 315], [553, 320], [538, 317], [516, 320], [481, 318], [469, 312], [449, 312], [434, 319], [184, 317], [128, 309], [128, 324]], 'typology': 9}\n",
      "{'pk': 6240, 'document_part': 245, 'external_id': 'eSc_line_315d288e', 'order': 2, 'region': 2854, 'baseline': [[306, 613], [328, 615], [712, 615], [867, 615], [949, 615], [971, 615], [983, 615]], 'mask': [[305, 612], [304, 628], [324, 639], [340, 639], [358, 628], [387, 628], [404, 638], [425, 635], [432, 639], [458, 628], [476, 640], [535, 633], [549, 638], [557, 632], [567, 639], [583, 633], [588, 637], [613, 634], [620, 639], [636, 634], [682, 638], [690, 632], [701, 638], [717, 633], [735, 638], [750, 628], [770, 640], [821, 634], [845, 638], [856, 632], [863, 637], [894, 637], [902, 632], [919, 637], [933, 628], [982, 628], [982, 604], [925, 604], [918, 599], [887, 604], [743, 604], [736, 599], [720, 603], [715, 599], [672, 599], [666, 604], [658, 599], [641, 604], [617, 604], [610, 599], [584, 604], [558, 599], [513, 599], [505, 604], [410, 604], [401, 599], [372, 604], [343, 599], [305, 602], [305, 612]], 'typology': 9}\n",
      "\n",
      "The first line has the following keys, valuetypes and variable content.\n",
      "pk <class 'int'> 6239\n",
      "document_part <class 'int'> 245\n",
      "external_id <class 'str'> eSc_line_d38c7e55\n",
      "order <class 'int'> 1\n",
      "region <class 'int'> 2853\n",
      "baseline <class 'list'> [[129, 325], [192, 331], [457, 331], [785, 337], [983, 337], [1015, 337]]\n",
      "mask <class 'list'> [[128, 324], [128, 338], [162, 350], [212, 350], [223, 345], [252, 350], [273, 340], [294, 350], [315, 351], [337, 342], [354, 350], [373, 346], [387, 353], [410, 353], [443, 341], [466, 353], [486, 353], [506, 343], [526, 353], [582, 348], [602, 352], [621, 343], [641, 350], [656, 346], [667, 352], [692, 348], [703, 352], [718, 348], [753, 351], [773, 345], [823, 353], [842, 345], [906, 351], [929, 346], [963, 351], [1013, 346], [1014, 336], [1013, 325], [943, 317], [876, 317], [853, 323], [838, 316], [798, 316], [775, 323], [730, 314], [706, 314], [691, 320], [666, 317], [654, 320], [641, 316], [630, 321], [566, 315], [553, 320], [538, 317], [516, 320], [481, 318], [469, 312], [449, 312], [434, 319], [184, 317], [128, 309], [128, 324]]\n",
      "typology <class 'int'> 9\n"
     ]
    }
   ],
   "source": [
    "print(doc_pk,part['pk'])\n",
    "lines = get_all_lines_of_part(doc_pk,part['pk'])\n",
    "print('There are',len(lines),'lines(s) stored in a list of dictionaries that looks as follows.')\n",
    "print(len(lines))\n",
    "for line in lines:\n",
    "  print(line)\n",
    "print()\n",
    "print('The first line has the following keys, valuetypes and variable content.')\n",
    "for key in lines[0]:\n",
    "  print(key,type(lines[0][key]),lines[0][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1728372758977,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "FNU4db7rbsDS",
    "outputId": "2ad16dc1-bc20-42e5-8213-2fd3441fff42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pk <class 'int'> 6239\n",
      "document_part <class 'int'> 245\n",
      "external_id <class 'str'> eSc_line_d38c7e55\n",
      "order <class 'int'> 1\n",
      "region <class 'int'> 2853\n",
      "baseline <class 'list'> [[129, 325], [192, 331], [457, 331], [785, 337], [983, 337], [1015, 337]]\n",
      "mask <class 'list'> [[128, 324], [128, 338], [162, 350], [212, 350], [223, 345], [252, 350], [273, 340], [294, 350], [315, 351], [337, 342], [354, 350], [373, 346], [387, 353], [410, 353], [443, 341], [466, 353], [486, 353], [506, 343], [526, 353], [582, 348], [602, 352], [621, 343], [641, 350], [656, 346], [667, 352], [692, 348], [703, 352], [718, 348], [753, 351], [773, 345], [823, 353], [842, 345], [906, 351], [929, 346], [963, 351], [1013, 346], [1014, 336], [1013, 325], [943, 317], [876, 317], [853, 323], [838, 316], [798, 316], [775, 323], [730, 314], [706, 314], [691, 320], [666, 317], [654, 320], [641, 316], [630, 321], [566, 315], [553, 320], [538, 317], [516, 320], [481, 318], [469, 312], [449, 312], [434, 319], [184, 317], [128, 309], [128, 324]]\n",
      "typology <class 'int'> 9\n"
     ]
    }
   ],
   "source": [
    "for key in lines[0]:\n",
    "  print(key,type(lines[0][key]),lines[0][key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVKJ7qEfcx_I"
   },
   "source": [
    "##Transcriptions\n",
    "As there can be many transcriptions to one line, transcriptions are different from lines.\n",
    "\n",
    "You can get a list of existing transcription levels via the function \"get_transcription_levels\" with the parameter of the document primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1728295477235,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "P0Ilfm6fqYjK",
    "outputId": "e610d64a-2dfd-455e-d8a8-7f6047773530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pk': 20, 'name': 'XML Import II', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:15:35.138131Z', 'comments': None}\n",
      "{'pk': 19, 'name': 'XML Import I', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T10:07:02.945464Z', 'comments': None}\n",
      "{'pk': 18, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-04-10T09:55:19.684869Z', 'comments': None}\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "tr_levels = get_transcription_levels(doc_pk)\n",
    "for tr_level in tr_levels:\n",
    "  print(tr_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wQyILd-sxcp"
   },
   "source": [
    "You can access ALL transcriptions of one \"part\" just as you accessed regions and lines. However, this will give you ALL transcriptions, i.e. all transcriptions of all transcription levels if you have several transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1728295492286,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "yFhS875jcxYD",
    "outputId": "f57e61d1-f08e-4903-cd00-7de9caeb411b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a list of all transcriptions:\n",
      "{'pk': 6749, 'line': 6238, 'transcription': 18, 'content': 'גורי, חיים', 'graphs': None, 'avg_confidence': None, 'versions': [], 'version_author': '', 'version_source': 'eScriptorium', 'version_updated_at': '2024-06-02T16:16:40.144157Z'}\n",
      "{'pk': 6750, 'line': 6239, 'transcription': 18, 'content': 'שיחה עם שלום: כהן על כוסית עראק', 'graphs': None, 'avg_confidence': None, 'versions': [], 'version_author': '', 'version_source': 'eScriptorium', 'version_updated_at': '2024-06-02T16:16:40.145568Z'}\n",
      "{'pk': 6751, 'line': 6240, 'transcription': 18, 'content': \"למרחב, 17.2.1956, עמ' 3\", 'graphs': None, 'avg_confidence': None, 'versions': [], 'version_author': '', 'version_source': 'eScriptorium', 'version_updated_at': '2024-06-02T16:16:40.146477Z'}\n",
      "\n",
      " a list of keys in one transcription dictionary\n",
      "pk <class 'int'> 6749\n",
      "line <class 'int'> 6238\n",
      "transcription <class 'int'> 18\n",
      "content <class 'str'> גורי, חיים\n",
      "graphs <class 'NoneType'> None\n",
      "avg_confidence <class 'NoneType'> None\n",
      "versions <class 'list'> []\n",
      "version_author <class 'str'> \n",
      "version_source <class 'str'> eScriptorium\n",
      "version_updated_at <class 'str'> 2024-06-02T16:16:40.144157Z\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "part_pk=part['pk']\n",
    "tr_url = get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "transcriptions=get_all_page_transcriptions(doc_pk,part_pk)\n",
    "print('a list of all transcriptions:')\n",
    "for transcription in transcriptions:\n",
    "  print(transcription)\n",
    "\n",
    "print()\n",
    "print(' a list of keys in one transcription dictionary')\n",
    "for key in transcriptions[0]:\n",
    "  print(key,type(transcriptions[0][key]),transcriptions[0][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHjsTU0to0ha"
   },
   "source": [
    "Of the 9 keys in the dictionary, four are important for beginners:\n",
    "\n",
    "* pk [an integer for the ID of the line-transcription]\n",
    "* line [an integer for the ID of the line to which this transcription belongs, i.e. the geometrical data with baseline, mask-polygon etc]\n",
    "* transcription [an integer for the ID of the transcription LEVEL, i.e. \"manual\" or some automatically created transcription. The list of transcription levels can be queried at:\n",
    "* content [the text of the line-transcription]\n",
    "\n",
    "For geeks also the graphs are very interesting as they contain the geometrical information for each automatically transcribed letter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjtwsYE5s_Nh"
   },
   "source": [
    "if you want to filter these transcriptions and just show the transcriptions of a specific level you can either do it yourself or use a function I already created for that, but you first need to get the primary key (ID) of the transcription level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oovpSH2bHk0B"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# replace some unicode by some other\n",
    "tr_level_name = 'WHATEVERNAME of a transcription level you have used in your document'\n",
    "doc_pks = [XXX] # Set XXX to the doc_pk of your document\n",
    "replacements = [('׳',\"'\"),('’',\"'\")] # first value in each tuple is the unicode to be replaced. the second is the replacement.\n",
    "\n",
    "for doc_pk in doc_pks:\n",
    "  master_tr_level = get_transcription_level_pk_by_name(doc_pk,tr_level_name)\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  for i,part in enumerate(parts):\n",
    "    part_pk = part['pk']\n",
    "    print(part_pk,end = ',')\n",
    "    if i % 10 == 0:\n",
    "      print()\n",
    "    trs = get_page_transcription(doc_pk,part_pk,master_tr_level)\n",
    "    updated_trs = []\n",
    "    for tr in trs:\n",
    "      for replacement in replacements:\n",
    "        if replacement[0] in tr['content']:\n",
    "          tr['content'] = tr['content'].replace(replacement[0],replacement[1])\n",
    "          updated_trs.append(tr)\n",
    "    if updated_trs != []:\n",
    "\n",
    "      r = bulk_update_transcriptions(doc_pk,part_pk,updated_trs)\n",
    "      print('trs updated',len(updated_trs),r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zx-IkUvH08sF"
   },
   "source": [
    "#Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwlwEjWmJXo1"
   },
   "source": [
    "Line Typology Analysis and Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OGvstLAi1mKX"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Function to count the occurrence of each typology in the document\n",
    "def count_line_typologies(doc_pk):\n",
    "    # Dictionary to store the count of each typology\n",
    "    typology_counts = defaultdict(int)\n",
    "\n",
    "    # Get all parts of the document\n",
    "    parts = get_all_parts(doc_pk)\n",
    "\n",
    "    for part in parts:\n",
    "        part_pk = part['pk']\n",
    "        #print(f\"Processing part {part_pk}\")\n",
    "\n",
    "        # Fetch all lines in the current part\n",
    "        lines = get_all_lines_of_part(doc_pk, part_pk)\n",
    "\n",
    "        for line in lines:\n",
    "            typology = line.get('typology', None)\n",
    "\n",
    "            # If the line has a typology, count it\n",
    "            if typology:\n",
    "                typology_counts[typology] += 1\n",
    "            else:\n",
    "                typology_counts['None'] += 1  # Count lines with no typology as 'None'\n",
    "\n",
    "    # Convert typology counts to a normal dictionary (optional, for easier reading)\n",
    "    return dict(typology_counts)\n",
    "\n",
    "# # Call the function with the document primary key and print the results\n",
    "# typology_summary = count_line_typologies(doc_pk=8)  # Replace XXX with the actual document primary key\n",
    "\n",
    "# # Print the result\n",
    "# for typology, count in typology_summary.items():\n",
    "#     print(f\"Typology '{typology}': {count} lines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96599,
     "status": "ok",
     "timestamp": 1728463911510,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "bpeh132f4qDK",
    "outputId": "646f385d-2b07-469a-86f9-e2de234140ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line PK: 10740, Document Part: 255, Typology: None, Content: None\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# Function to identify lines with specific typologies (None or 18)\n",
    "def find_specific_typologies(doc_pk, target_typologies):\n",
    "    # List to store lines with matching typologies\n",
    "    matching_lines = []\n",
    "\n",
    "    # Get all parts of the document\n",
    "    parts = get_all_parts(doc_pk)\n",
    "\n",
    "    for part in parts:\n",
    "        part_pk = part['pk']\n",
    "        #print(f\"Processing part {part_pk}\")\n",
    "\n",
    "        # Fetch all lines in the current part\n",
    "        lines = get_all_lines_of_part(doc_pk, part_pk)\n",
    "\n",
    "        for line in lines:\n",
    "            typology = line.get('typology', None)\n",
    "\n",
    "            # Check if the typology is in our target list\n",
    "            if typology in target_typologies or typology is None:\n",
    "                # Collect detailed information about the line\n",
    "                line_info = {\n",
    "                    'line_pk': line['pk'],\n",
    "                    'document_part': line['document_part'],\n",
    "                    'typology': typology,\n",
    "                    'content': None  # Default None for content\n",
    "                }\n",
    "\n",
    "                # Add the line info to the matching list\n",
    "                matching_lines.append(line_info)\n",
    "\n",
    "    return matching_lines\n",
    "\n",
    "# Define the target typologies to search for\n",
    "target_typologies = [18]  # You can add more typologies if needed\n",
    "\n",
    "# Call the function to find lines with typology None or 18\n",
    "specific_lines = find_specific_typologies(doc_pk=8, target_typologies=target_typologies)  # Replace XXX with your document primary key\n",
    "\n",
    "# Print out the details of the lines found\n",
    "for line in specific_lines:\n",
    "    typology_label = 'None' if line['typology'] is None else line['typology']\n",
    "    print(f\"Line PK: {line['line_pk']}, Document Part: {line['document_part']}, Typology: {typology_label}, Content: {line['content']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1728301892042,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "7AqrNRqhluiZ",
    "outputId": "e6b9cfda-84a3-4039-f72c-b36f48a6989b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 10853 typology updated to 'default'\n"
     ]
    }
   ],
   "source": [
    "#change typology of specific line\n",
    "\n",
    "# Set your document primary key, part primary key, and line primary key\n",
    "doc_pk = 8\n",
    "part_pk = 291\n",
    "line_pk = 10853\n",
    "\n",
    "# You already know that the 'default' typology has a PK of 9\n",
    "default_typology_pk = 9\n",
    "\n",
    "# Fetch the specific line data (assuming the get_line function exists)\n",
    "line_data = get_line(doc_pk, part_pk, line_pk)\n",
    "\n",
    "# Check if typology is None and update it, change according to need\n",
    "if line_data['typology'] is None:\n",
    "    line_data['typology'] = default_typology_pk  # Set typology to 'default' PK\n",
    "\n",
    "    # Prepare the payload with the required fields\n",
    "    update_payload = {\n",
    "        'typology': line_data['typology'],  # Use the PK for 'default'\n",
    "        'document_part': line_data['document_part']  # Include the document part PK\n",
    "    }\n",
    "\n",
    "    # Use the update_line function to update this specific line\n",
    "    update_url = get_specific_line_url(doc_pk, part_pk, line_pk)  # Generate the specific URL for the line\n",
    "    result = update_item(update_url, update_payload)  # Send the update\n",
    "\n",
    "    # Check the response\n",
    "    if result.status_code == 200:\n",
    "        print(f\"Line {line_pk} typology updated to 'default'\")\n",
    "    else:\n",
    "        print(f\"Failed to update line {line_pk}, response: {result.status_code}, {result.content}\")\n",
    "else:\n",
    "    print(f\"Line {line_pk} already has typology: {line_data['typology']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 68844,
     "status": "ok",
     "timestamp": 1728300846657,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "a3l5iUMPthhd",
    "outputId": "e36c9c7d-4353-4b32-8112-5b509510ecac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing part 241\n",
      "Successfully updated 4 lines in part 241\n",
      "Processing part 242\n",
      "Line 6223 in part 242 already has typology: 20\n",
      "Line 6224 in part 242 already has typology: 20\n",
      "Line 6372 in part 242 already has typology: 20\n",
      "Line 6226 in part 242 already has typology: 27\n",
      "No lines to update in part 242\n",
      "Processing part 243\n",
      "Successfully updated 4 lines in part 243\n",
      "Processing part 244\n",
      "Successfully updated 3 lines in part 244\n",
      "Processing part 245\n",
      "Line 6238 in part 245 already has typology: 9\n",
      "Successfully updated 2 lines in part 245\n",
      "Processing part 246\n",
      "Successfully updated 4 lines in part 246\n",
      "Processing part 247\n",
      "Line 6249 in part 247 already has typology: 9\n",
      "Line 6253 in part 247 already has typology: 20\n",
      "Line 6254 in part 247 already has typology: 20\n",
      "Successfully updated 2 lines in part 247\n",
      "Processing part 248\n",
      "Line 6256 in part 248 already has typology: 20\n",
      "Line 6257 in part 248 already has typology: 20\n",
      "Line 6258 in part 248 already has typology: 20\n",
      "Line 6259 in part 248 already has typology: 20\n",
      "Line 6260 in part 248 already has typology: 27\n",
      "No lines to update in part 248\n",
      "Processing part 249\n",
      "Successfully updated 4 lines in part 249\n",
      "Processing part 250\n",
      "Successfully updated 3 lines in part 250\n",
      "Processing part 251\n",
      "Successfully updated 4 lines in part 251\n",
      "Processing part 252\n",
      "Successfully updated 4 lines in part 252\n",
      "Processing part 253\n",
      "Successfully updated 4 lines in part 253\n",
      "Processing part 254\n",
      "Line 6286 in part 254 already has typology: 20\n",
      "Line 6287 in part 254 already has typology: 20\n",
      "Line 6288 in part 254 already has typology: 27\n",
      "No lines to update in part 254\n",
      "Processing part 255\n",
      "Line 10743 in part 255 already has typology: 20\n",
      "Line 10742 in part 255 already has typology: 20\n",
      "Line 10741 in part 255 already has typology: 20\n",
      "Line 10740 in part 255 already has typology: 27\n",
      "No lines to update in part 255\n",
      "Processing part 256\n",
      "Successfully updated 4 lines in part 256\n",
      "Processing part 257\n",
      "Successfully updated 4 lines in part 257\n",
      "Processing part 258\n",
      "Successfully updated 3 lines in part 258\n",
      "Processing part 259\n",
      "Line 6308 in part 259 already has typology: 20\n",
      "Line 6309 in part 259 already has typology: 20\n",
      "Line 6310 in part 259 already has typology: 27\n",
      "No lines to update in part 259\n",
      "Processing part 260\n",
      "Successfully updated 5 lines in part 260\n",
      "Processing part 261\n",
      "Line 6316 in part 261 already has typology: 20\n",
      "Line 6317 in part 261 already has typology: 20\n",
      "Line 6318 in part 261 already has typology: 20\n",
      "Line 6319 in part 261 already has typology: 20\n",
      "Line 6320 in part 261 already has typology: 27\n",
      "No lines to update in part 261\n",
      "Processing part 262\n",
      "Successfully updated 3 lines in part 262\n",
      "Processing part 263\n",
      "Successfully updated 4 lines in part 263\n",
      "Processing part 264\n",
      "Successfully updated 3 lines in part 264\n",
      "Processing part 265\n",
      "Successfully updated 3 lines in part 265\n",
      "Processing part 266\n",
      "Successfully updated 4 lines in part 266\n",
      "Processing part 267\n",
      "Successfully updated 4 lines in part 267\n",
      "Processing part 268\n",
      "Successfully updated 3 lines in part 268\n",
      "Processing part 269\n",
      "Successfully updated 4 lines in part 269\n",
      "Processing part 270\n",
      "Successfully updated 4 lines in part 270\n",
      "Processing part 271\n",
      "Successfully updated 4 lines in part 271\n",
      "Processing part 272\n",
      "Line 6399 in part 272 already has typology: 27\n",
      "Successfully updated 8 lines in part 272\n",
      "Processing part 273\n",
      "Successfully updated 4 lines in part 273\n",
      "Processing part 274\n",
      "Successfully updated 3 lines in part 274\n",
      "Processing part 275\n",
      "Successfully updated 4 lines in part 275\n",
      "Processing part 276\n",
      "Successfully updated 3 lines in part 276\n",
      "Processing part 277\n",
      "Successfully updated 4 lines in part 277\n",
      "Processing part 278\n",
      "Successfully updated 5 lines in part 278\n",
      "Processing part 279\n",
      "Successfully updated 4 lines in part 279\n",
      "Processing part 280\n",
      "Successfully updated 4 lines in part 280\n",
      "Processing part 281\n",
      "Line 10676 in part 281 already has typology: 20\n",
      "Line 22585 in part 281 already has typology: 20\n",
      "Line 10679 in part 281 already has typology: 20\n",
      "No lines to update in part 281\n",
      "Processing part 282\n",
      "Successfully updated 4 lines in part 282\n",
      "Processing part 283\n",
      "Successfully updated 3 lines in part 283\n",
      "Processing part 284\n",
      "Successfully updated 5 lines in part 284\n",
      "Processing part 285\n",
      "Successfully updated 6 lines in part 285\n",
      "Processing part 286\n",
      "Line 10703 in part 286 already has typology: 20\n",
      "Line 10704 in part 286 already has typology: 20\n",
      "Line 10705 in part 286 already has typology: 20\n",
      "Line 10708 in part 286 already has typology: 20\n",
      "Line 10710 in part 286 already has typology: 27\n",
      "No lines to update in part 286\n",
      "Processing part 287\n",
      "Line 10715 in part 287 already has typology: 20\n",
      "Line 10717 in part 287 already has typology: 20\n",
      "Line 10718 in part 287 already has typology: 20\n",
      "Successfully updated 4 lines in part 287\n",
      "Processing part 288\n",
      "Line 10719 in part 288 already has typology: 20\n",
      "Line 10720 in part 288 already has typology: 20\n",
      "Line 10721 in part 288 already has typology: 20\n",
      "Line 10724 in part 288 already has typology: 20\n",
      "Line 10725 in part 288 already has typology: 27\n",
      "No lines to update in part 288\n",
      "Processing part 289\n",
      "Successfully updated 4 lines in part 289\n",
      "Processing part 290\n",
      "Successfully updated 3 lines in part 290\n",
      "Processing part 291\n",
      "Line 10853 in part 291 already has typology: 13\n",
      "Successfully updated 3 lines in part 291\n",
      "Processing part 292\n",
      "Successfully updated 4 lines in part 292\n",
      "Processing part 293\n",
      "Successfully updated 3 lines in part 293\n",
      "Processing part 294\n",
      "Successfully updated 4 lines in part 294\n",
      "Processing part 295\n",
      "Line 10876 in part 295 already has typology: 20\n",
      "Successfully updated 4 lines in part 295\n",
      "Processing part 296\n",
      "Successfully updated 4 lines in part 296\n",
      "Processing part 297\n",
      "Successfully updated 4 lines in part 297\n",
      "Processing part 298\n",
      "Successfully updated 3 lines in part 298\n",
      "Processing part 299\n",
      "Successfully updated 3 lines in part 299\n",
      "Processing part 300\n",
      "Line 10954 in part 300 already has typology: 20\n",
      "Line 10955 in part 300 already has typology: 20\n",
      "Line 10956 in part 300 already has typology: 20\n",
      "Line 10957 in part 300 already has typology: 20\n",
      "No lines to update in part 300\n",
      "Processing part 301\n",
      "Successfully updated 4 lines in part 301\n",
      "Processing part 302\n",
      "Successfully updated 5 lines in part 302\n",
      "Processing part 303\n",
      "Successfully updated 5 lines in part 303\n",
      "Processing part 304\n",
      "Line 10974 in part 304 already has typology: 20\n",
      "Line 10975 in part 304 already has typology: 20\n",
      "Line 10976 in part 304 already has typology: 20\n",
      "Line 10977 in part 304 already has typology: 27\n",
      "No lines to update in part 304\n",
      "Processing part 305\n",
      "Successfully updated 4 lines in part 305\n",
      "Processing part 306\n",
      "Successfully updated 4 lines in part 306\n",
      "Processing part 307\n",
      "Line 10996 in part 307 already has typology: 20\n",
      "Successfully updated 3 lines in part 307\n",
      "Processing part 308\n",
      "Line 10997 in part 308 already has typology: 18\n",
      "Successfully updated 3 lines in part 308\n",
      "Processing part 309\n",
      "Successfully updated 4 lines in part 309\n",
      "Processing part 310\n",
      "Successfully updated 2 lines in part 310\n",
      "Processing part 311\n",
      "Successfully updated 4 lines in part 311\n",
      "Processing part 312\n",
      "Line 11013 in part 312 already has typology: 20\n",
      "Line 11015 in part 312 already has typology: 20\n",
      "Line 11016 in part 312 already has typology: 20\n",
      "Line 11021 in part 312 already has typology: 20\n",
      "Line 11022 in part 312 already has typology: 27\n",
      "No lines to update in part 312\n",
      "Processing part 313\n",
      "Successfully updated 4 lines in part 313\n",
      "Processing part 314\n",
      "Successfully updated 4 lines in part 314\n",
      "Processing part 315\n",
      "Successfully updated 3 lines in part 315\n",
      "Processing part 316\n",
      "Successfully updated 4 lines in part 316\n",
      "Processing part 317\n",
      "Successfully updated 3 lines in part 317\n",
      "Processing part 318\n",
      "Successfully updated 3 lines in part 318\n",
      "Processing part 319\n",
      "Successfully updated 4 lines in part 319\n",
      "Processing part 320\n",
      "Successfully updated 4 lines in part 320\n",
      "Processing part 321\n",
      "Successfully updated 3 lines in part 321\n",
      "Processing part 322\n",
      "Successfully updated 3 lines in part 322\n",
      "Processing part 323\n",
      "Successfully updated 3 lines in part 323\n",
      "Processing part 324\n",
      "Successfully updated 4 lines in part 324\n",
      "Processing part 325\n",
      "Successfully updated 3 lines in part 325\n",
      "Processing part 326\n",
      "Line 11084 in part 326 already has typology: 27\n",
      "Successfully updated 4 lines in part 326\n",
      "Processing part 327\n",
      "Successfully updated 4 lines in part 327\n",
      "Processing part 328\n",
      "Line 11092 in part 328 already has typology: 27\n",
      "Successfully updated 2 lines in part 328\n",
      "Processing part 329\n",
      "Line 11096 in part 329 already has typology: 27\n",
      "Successfully updated 3 lines in part 329\n",
      "Processing part 330\n",
      "Successfully updated 3 lines in part 330\n",
      "Processing part 331\n",
      "Successfully updated 3 lines in part 331\n",
      "Processing part 332\n",
      "Successfully updated 5 lines in part 332\n",
      "Processing part 333\n",
      "Line 11111 in part 333 already has typology: 20\n",
      "Successfully updated 3 lines in part 333\n",
      "Processing part 334\n",
      "Line 11115 in part 334 already has typology: 20\n",
      "Successfully updated 4 lines in part 334\n",
      "Processing part 335\n",
      "Successfully updated 4 lines in part 335\n",
      "Processing part 336\n",
      "Line 11121 in part 336 already has typology: 20\n",
      "Line 11122 in part 336 already has typology: 20\n",
      "Line 11123 in part 336 already has typology: 20\n",
      "Line 11124 in part 336 already has typology: 27\n",
      "No lines to update in part 336\n",
      "Processing part 337\n",
      "Successfully updated 4 lines in part 337\n",
      "Processing part 338\n",
      "Successfully updated 3 lines in part 338\n",
      "Processing part 339\n",
      "Successfully updated 4 lines in part 339\n",
      "Processing part 340\n",
      "Successfully updated 4 lines in part 340\n",
      "Processing part 341\n",
      "Successfully updated 4 lines in part 341\n",
      "Processing part 342\n",
      "Successfully updated 4 lines in part 342\n",
      "Processing part 343\n",
      "Successfully updated 4 lines in part 343\n",
      "Processing part 344\n",
      "Successfully updated 4 lines in part 344\n",
      "Processing part 345\n",
      "Successfully updated 4 lines in part 345\n",
      "Processing part 346\n",
      "Successfully updated 4 lines in part 346\n",
      "Processing part 347\n",
      "Successfully updated 5 lines in part 347\n",
      "Processing part 348\n",
      "Successfully updated 3 lines in part 348\n",
      "Processing part 349\n",
      "Successfully updated 4 lines in part 349\n",
      "Processing part 350\n",
      "Successfully updated 4 lines in part 350\n",
      "Processing part 351\n",
      "Successfully updated 4 lines in part 351\n",
      "Processing part 352\n",
      "Successfully updated 4 lines in part 352\n",
      "Processing part 353\n",
      "Successfully updated 4 lines in part 353\n",
      "Processing part 354\n",
      "Successfully updated 4 lines in part 354\n",
      "Processing part 355\n",
      "Successfully updated 5 lines in part 355\n",
      "Processing part 356\n",
      "Line 11220 in part 356 already has typology: 27\n",
      "Successfully updated 2 lines in part 356\n",
      "Processing part 357\n",
      "Successfully updated 6 lines in part 357\n",
      "Processing part 358\n",
      "Successfully updated 4 lines in part 358\n",
      "Processing part 359\n",
      "Line 11234 in part 359 already has typology: 20\n",
      "Line 22617 in part 359 already has typology: 20\n",
      "Line 11240 in part 359 already has typology: 20\n",
      "Line 11238 in part 359 already has typology: 20\n",
      "Line 11239 in part 359 already has typology: 20\n",
      "No lines to update in part 359\n",
      "Processing part 360\n",
      "Line 11244 in part 360 already has typology: 27\n",
      "Successfully updated 3 lines in part 360\n",
      "Processing part 361\n",
      "Successfully updated 3 lines in part 361\n",
      "Processing part 362\n",
      "Successfully updated 4 lines in part 362\n",
      "Processing part 363\n",
      "Successfully updated 3 lines in part 363\n",
      "Processing part 364\n",
      "Successfully updated 4 lines in part 364\n",
      "Processing part 365\n",
      "Successfully updated 4 lines in part 365\n",
      "Processing part 366\n",
      "Successfully updated 3 lines in part 366\n",
      "Processing part 367\n",
      "Line 18742 in part 367 already has typology: 20\n",
      "Successfully updated 3 lines in part 367\n",
      "Processing part 368\n",
      "Successfully updated 3 lines in part 368\n",
      "Processing part 369\n",
      "Successfully updated 4 lines in part 369\n",
      "Processing part 370\n",
      "Successfully updated 3 lines in part 370\n",
      "Processing part 371\n",
      "Successfully updated 4 lines in part 371\n",
      "Processing part 372\n",
      "Line 18769 in part 372 already has typology: 20\n",
      "Successfully updated 3 lines in part 372\n",
      "Processing part 373\n",
      "Successfully updated 4 lines in part 373\n",
      "Processing part 374\n",
      "Successfully updated 4 lines in part 374\n",
      "Processing part 375\n",
      "Successfully updated 4 lines in part 375\n",
      "Processing part 376\n",
      "Successfully updated 4 lines in part 376\n",
      "Processing part 377\n",
      "Successfully updated 4 lines in part 377\n",
      "Processing part 378\n",
      "Line 18792 in part 378 already has typology: 20\n",
      "Line 22632 in part 378 already has typology: 20\n",
      "Line 18795 in part 378 already has typology: 20\n",
      "Line 18796 in part 378 already has typology: 20\n",
      "No lines to update in part 378\n",
      "Processing part 379\n",
      "Successfully updated 4 lines in part 379\n",
      "Processing part 380\n",
      "Successfully updated 3 lines in part 380\n",
      "Processing part 381\n",
      "Successfully updated 4 lines in part 381\n",
      "Processing part 382\n",
      "Successfully updated 4 lines in part 382\n",
      "Processing part 383\n",
      "Successfully updated 5 lines in part 383\n",
      "Processing part 384\n",
      "Successfully updated 4 lines in part 384\n",
      "Processing part 385\n",
      "Successfully updated 4 lines in part 385\n",
      "Processing part 386\n",
      "Line 18830 in part 386 already has typology: 20\n",
      "Line 18831 in part 386 already has typology: 20\n",
      "Line 18832 in part 386 already has typology: 20\n",
      "Line 18833 in part 386 already has typology: 27\n",
      "No lines to update in part 386\n",
      "Processing part 387\n",
      "Successfully updated 3 lines in part 387\n",
      "Processing part 388\n",
      "Successfully updated 4 lines in part 388\n",
      "Processing part 389\n",
      "Successfully updated 6 lines in part 389\n",
      "Processing part 390\n",
      "Successfully updated 4 lines in part 390\n",
      "Processing part 391\n",
      "Successfully updated 4 lines in part 391\n",
      "Processing part 392\n",
      "Successfully updated 4 lines in part 392\n",
      "Processing part 393\n",
      "Line 18866 in part 393 already has typology: 20\n",
      "Line 18869 in part 393 already has typology: 20\n",
      "Successfully updated 4 lines in part 393\n",
      "Processing part 394\n",
      "Successfully updated 3 lines in part 394\n",
      "Processing part 395\n",
      "Successfully updated 4 lines in part 395\n",
      "Processing part 396\n",
      "Successfully updated 4 lines in part 396\n",
      "Processing part 397\n",
      "Successfully updated 4 lines in part 397\n",
      "Processing part 398\n",
      "Line 18888 in part 398 already has typology: 20\n",
      "Line 18889 in part 398 already has typology: 20\n",
      "Line 18890 in part 398 already has typology: 20\n",
      "Line 18891 in part 398 already has typology: 20\n",
      "No lines to update in part 398\n",
      "Processing part 399\n",
      "Successfully updated 3 lines in part 399\n",
      "Processing part 400\n",
      "Line 18896 in part 400 already has typology: 20\n",
      "Line 22659 in part 400 already has typology: 20\n",
      "Line 18899 in part 400 already has typology: 20\n",
      "No lines to update in part 400\n",
      "Processing part 401\n",
      "Successfully updated 3 lines in part 401\n",
      "Processing part 402\n",
      "Successfully updated 4 lines in part 402\n",
      "Processing part 403\n",
      "Line 18908 in part 403 already has typology: 20\n",
      "Line 18909 in part 403 already has typology: 20\n",
      "Line 18910 in part 403 already has typology: 20\n",
      "Line 18911 in part 403 already has typology: 27\n",
      "No lines to update in part 403\n",
      "Processing part 404\n",
      "Successfully updated 4 lines in part 404\n",
      "Processing part 405\n",
      "Successfully updated 3 lines in part 405\n",
      "Processing part 406\n",
      "Line 18919 in part 406 already has typology: 20\n",
      "Line 18920 in part 406 already has typology: 20\n",
      "Line 18921 in part 406 already has typology: 20\n",
      "No lines to update in part 406\n",
      "Processing part 407\n",
      "Successfully updated 5 lines in part 407\n",
      "Processing part 408\n",
      "Successfully updated 4 lines in part 408\n",
      "Processing part 409\n",
      "Successfully updated 4 lines in part 409\n",
      "Processing part 410\n",
      "Successfully updated 4 lines in part 410\n",
      "Processing part 411\n",
      "Line 18942 in part 411 already has typology: 20\n",
      "Line 18943 in part 411 already has typology: 20\n",
      "Line 18944 in part 411 already has typology: 20\n",
      "Line 18945 in part 411 already has typology: 20\n",
      "Line 18946 in part 411 already has typology: 27\n",
      "No lines to update in part 411\n",
      "Processing part 412\n",
      "Line 18949 in part 412 already has typology: 20\n",
      "Line 18952 in part 412 already has typology: 20\n",
      "Line 18950 in part 412 already has typology: 20\n",
      "Successfully updated 3 lines in part 412\n",
      "Processing part 413\n",
      "Successfully updated 4 lines in part 413\n",
      "Processing part 414\n",
      "Successfully updated 5 lines in part 414\n",
      "Processing part 415\n",
      "Successfully updated 4 lines in part 415\n",
      "Processing part 416\n",
      "Successfully updated 3 lines in part 416\n",
      "Processing part 417\n",
      "Line 18971 in part 417 already has typology: 20\n",
      "Line 18972 in part 417 already has typology: 20\n",
      "Line 18973 in part 417 already has typology: 20\n",
      "Line 18974 in part 417 already has typology: 20\n",
      "Line 18975 in part 417 already has typology: 27\n",
      "No lines to update in part 417\n",
      "Processing part 418\n",
      "Successfully updated 3 lines in part 418\n",
      "Processing part 419\n",
      "Line 18979 in part 419 already has typology: 20\n",
      "Line 18980 in part 419 already has typology: 20\n",
      "Line 18981 in part 419 already has typology: 20\n",
      "Line 18982 in part 419 already has typology: 20\n",
      "Line 18983 in part 419 already has typology: 27\n",
      "No lines to update in part 419\n",
      "Processing part 420\n",
      "Successfully updated 3 lines in part 420\n",
      "Processing part 421\n",
      "Line 18988 in part 421 already has typology: 20\n",
      "Line 18989 in part 421 already has typology: 20\n",
      "Line 18990 in part 421 already has typology: 20\n",
      "Line 18991 in part 421 already has typology: 20\n",
      "Line 18992 in part 421 already has typology: 27\n",
      "No lines to update in part 421\n",
      "Processing part 422\n",
      "Successfully updated 4 lines in part 422\n",
      "Processing part 423\n",
      "Successfully updated 7 lines in part 423\n",
      "Processing part 424\n",
      "Successfully updated 8 lines in part 424\n",
      "Processing part 425\n",
      "Successfully updated 4 lines in part 425\n",
      "Processing part 426\n",
      "Successfully updated 4 lines in part 426\n",
      "Processing part 427\n",
      "Line 19022 in part 427 already has typology: 20\n",
      "Line 19023 in part 427 already has typology: 20\n",
      "Line 19024 in part 427 already has typology: 20\n",
      "Line 19025 in part 427 already has typology: 20\n",
      "Line 19026 in part 427 already has typology: 20\n",
      "Line 19027 in part 427 already has typology: 20\n",
      "No lines to update in part 427\n",
      "Processing part 428\n",
      "Successfully updated 4 lines in part 428\n",
      "Processing part 429\n",
      "Successfully updated 4 lines in part 429\n",
      "Processing part 430\n",
      "Successfully updated 3 lines in part 430\n",
      "Processing part 431\n",
      "Successfully updated 4 lines in part 431\n",
      "Processing part 432\n",
      "Successfully updated 4 lines in part 432\n",
      "Processing part 433\n",
      "Successfully updated 4 lines in part 433\n",
      "Processing part 434\n",
      "Successfully updated 4 lines in part 434\n",
      "Processing part 435\n",
      "Successfully updated 4 lines in part 435\n",
      "Processing part 436\n",
      "Successfully updated 3 lines in part 436\n",
      "Processing part 437\n",
      "Successfully updated 4 lines in part 437\n",
      "Processing part 438\n",
      "Successfully updated 4 lines in part 438\n",
      "Processing part 439\n",
      "Line 19076 in part 439 already has typology: 20\n",
      "Successfully updated 3 lines in part 439\n",
      "Processing part 440\n",
      "Successfully updated 5 lines in part 440\n",
      "Processing part 441\n",
      "Line 19970 in part 441 already has typology: 20\n",
      "Line 19971 in part 441 already has typology: 20\n",
      "Line 19972 in part 441 already has typology: 20\n",
      "Line 19973 in part 441 already has typology: 20\n",
      "Line 19974 in part 441 already has typology: 27\n",
      "No lines to update in part 441\n",
      "Processing part 442\n",
      "Successfully updated 3 lines in part 442\n",
      "Processing part 443\n",
      "Successfully updated 5 lines in part 443\n",
      "Processing part 444\n",
      "Successfully updated 4 lines in part 444\n",
      "Processing part 445\n",
      "Successfully updated 5 lines in part 445\n",
      "Processing part 446\n",
      "Line 19992 in part 446 already has typology: 20\n",
      "Line 19993 in part 446 already has typology: 20\n",
      "Line 19994 in part 446 already has typology: 27\n",
      "No lines to update in part 446\n",
      "Processing part 447\n",
      "Successfully updated 3 lines in part 447\n",
      "Processing part 448\n",
      "Line 19999 in part 448 already has typology: 20\n",
      "Line 20000 in part 448 already has typology: 20\n",
      "Line 20001 in part 448 already has typology: 20\n",
      "No lines to update in part 448\n",
      "Processing part 449\n",
      "Line 20005 in part 449 already has typology: 20\n",
      "Successfully updated 4 lines in part 449\n",
      "Processing part 450\n",
      "Successfully updated 6 lines in part 450\n",
      "Processing part 451\n",
      "Successfully updated 4 lines in part 451\n",
      "Processing part 452\n",
      "Successfully updated 5 lines in part 452\n",
      "Processing part 453\n",
      "Successfully updated 5 lines in part 453\n",
      "Processing part 454\n",
      "Successfully updated 3 lines in part 454\n",
      "Processing part 455\n",
      "Line 20031 in part 455 already has typology: 20\n",
      "Line 20032 in part 455 already has typology: 20\n",
      "Line 20033 in part 455 already has typology: 27\n",
      "No lines to update in part 455\n",
      "Processing part 456\n",
      "Successfully updated 3 lines in part 456\n",
      "Processing part 457\n",
      "Successfully updated 4 lines in part 457\n",
      "Processing part 458\n",
      "Line 20041 in part 458 already has typology: 20\n",
      "Line 20042 in part 458 already has typology: 20\n",
      "Line 20043 in part 458 already has typology: 27\n",
      "No lines to update in part 458\n",
      "Processing part 459\n",
      "Line 20044 in part 459 already has typology: 20\n",
      "Line 22634 in part 459 already has typology: 20\n",
      "Line 20051 in part 459 already has typology: 20\n",
      "No lines to update in part 459\n",
      "Processing part 460\n",
      "Successfully updated 4 lines in part 460\n",
      "Processing part 461\n",
      "Successfully updated 3 lines in part 461\n",
      "Processing part 462\n",
      "Successfully updated 5 lines in part 462\n",
      "Processing part 463\n",
      "Line 20064 in part 463 already has typology: 20\n",
      "Line 20065 in part 463 already has typology: 20\n",
      "Line 20067 in part 463 already has typology: 27\n",
      "Line 20068 in part 463 already has typology: 27\n",
      "Line 20066 in part 463 already has typology: 20\n",
      "No lines to update in part 463\n",
      "Processing part 464\n",
      "Line 20072 in part 464 already has typology: 20\n",
      "Successfully updated 3 lines in part 464\n",
      "Processing part 465\n",
      "Successfully updated 4 lines in part 465\n",
      "Processing part 466\n",
      "Successfully updated 4 lines in part 466\n",
      "Processing part 467\n",
      "Successfully updated 5 lines in part 467\n",
      "Processing part 468\n",
      "Successfully updated 3 lines in part 468\n",
      "Processing part 469\n",
      "Successfully updated 4 lines in part 469\n",
      "Processing part 470\n",
      "Successfully updated 3 lines in part 470\n",
      "Processing part 471\n",
      "Successfully updated 4 lines in part 471\n",
      "Processing part 472\n",
      "Successfully updated 4 lines in part 472\n",
      "Processing part 473\n",
      "Successfully updated 5 lines in part 473\n",
      "Processing part 474\n",
      "Line 20114 in part 474 already has typology: 20\n",
      "Line 20115 in part 474 already has typology: 20\n",
      "Line 20116 in part 474 already has typology: 20\n",
      "Line 20117 in part 474 already has typology: 20\n",
      "No lines to update in part 474\n",
      "Processing part 475\n",
      "Successfully updated 3 lines in part 475\n",
      "Processing part 476\n",
      "Successfully updated 5 lines in part 476\n",
      "Processing part 477\n",
      "Successfully updated 5 lines in part 477\n",
      "Processing part 478\n",
      "Line 20137 in part 478 already has typology: 27\n",
      "Successfully updated 3 lines in part 478\n",
      "Processing part 479\n",
      "Successfully updated 4 lines in part 479\n",
      "Processing part 480\n",
      "Successfully updated 4 lines in part 480\n",
      "Processing part 481\n",
      "Successfully updated 3 lines in part 481\n",
      "Processing part 482\n",
      "Successfully updated 3 lines in part 482\n",
      "Processing part 483\n",
      "Successfully updated 4 lines in part 483\n",
      "Processing part 484\n",
      "Line 20161 in part 484 already has typology: 27\n",
      "Successfully updated 3 lines in part 484\n",
      "Processing part 485\n",
      "Successfully updated 3 lines in part 485\n",
      "Processing part 486\n",
      "Line 20167 in part 486 already has typology: 9\n",
      "Line 22654 in part 486 already has typology: 9\n",
      "Line 20170 in part 486 already has typology: 9\n",
      "Line 20171 in part 486 already has typology: 9\n",
      "No lines to update in part 486\n",
      "Processing part 487\n",
      "Line 20172 in part 487 already has typology: 9\n",
      "Line 22655 in part 487 already has typology: 9\n",
      "Line 20175 in part 487 already has typology: 9\n",
      "No lines to update in part 487\n",
      "Processing part 488\n",
      "Line 20176 in part 488 already has typology: 9\n",
      "Line 22658 in part 488 already has typology: 9\n",
      "Line 20179 in part 488 already has typology: 9\n",
      "Line 20180 in part 488 already has typology: 9\n",
      "No lines to update in part 488\n",
      "Processing part 489\n",
      "Line 20181 in part 489 already has typology: 9\n",
      "Line 20182 in part 489 already has typology: 9\n",
      "Line 20183 in part 489 already has typology: 9\n",
      "No lines to update in part 489\n",
      "Processing part 490\n",
      "Line 20184 in part 490 already has typology: 9\n",
      "Line 20186 in part 490 already has typology: 9\n",
      "Line 20187 in part 490 already has typology: 9\n",
      "Line 20188 in part 490 already has typology: 9\n",
      "No lines to update in part 490\n"
     ]
    }
   ],
   "source": [
    "#Bulk change the typology of all lines with a specific name\n",
    "\n",
    "# Define the default typology PK\n",
    "default_typology_pk = 9  # 'default' typology has a PK of 9\n",
    "\n",
    "# Function to process all lines in a document and update typology if it's None\n",
    "def process_document_lines_bulk(doc_pk):\n",
    "    # Get all parts of the document\n",
    "    parts = get_all_parts(doc_pk)  # Assuming this function exists to fetch all parts of the document\n",
    "\n",
    "    for part in parts:\n",
    "        part_pk = part['pk']\n",
    "        print(f\"Processing part {part_pk}\")\n",
    "\n",
    "        # Fetch all lines in the current part\n",
    "        lines = get_all_lines_of_part(doc_pk, part_pk)  # Assuming this function exists to fetch all lines for the part\n",
    "        updated_lines = []  # To store the lines that need updating\n",
    "\n",
    "        for line in lines:\n",
    "            line_pk = line['pk']\n",
    "\n",
    "            # Check if the line's typology is None\n",
    "            if line['typology'] is None:\n",
    "                # Update the line's typology to 'default'\n",
    "                line['typology'] = default_typology_pk  # Set to 'default' PK\n",
    "\n",
    "                # Prepare the updated line data with required fields\n",
    "                updated_lines.append({\n",
    "                    'pk': line_pk,\n",
    "                    'typology': line['typology'],\n",
    "                    'document_part': line['document_part']\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Line {line_pk} in part {part_pk} already has typology: {line['typology']}\")\n",
    "\n",
    "        # If we have lines to update, send them in bulk\n",
    "        if updated_lines:\n",
    "            result = bulk_update_lines(doc_pk, part_pk, updated_lines)\n",
    "            if result.status_code == 200:\n",
    "                print(f\"Successfully updated {len(updated_lines)} lines in part {part_pk}\")\n",
    "            else:\n",
    "                print(f\"Failed to update lines in part {part_pk}, response: {result.status_code}, {result.content}\")\n",
    "        else:\n",
    "            print(f\"No lines to update in part {part_pk}\")\n",
    "\n",
    "# Call the function with the document primary key\n",
    "process_document_lines_bulk(doc_pk=8) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD9xSTSnM4Qb"
   },
   "source": [
    "Repolygonization Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y_m3h-XOU0r"
   },
   "source": [
    "Functions that simplify baselines, align them horizontally and contain them within their regions, and then repolygonize, with custom values per line type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3697,
     "status": "ok",
     "timestamp": 1732776510942,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "YWQ3bxkt3Hqa",
    "outputId": "89c78319-2cd3-4fdd-9ed0-6a977fe901b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repoygonized 1 lines based on custom ascender and descender values.\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import LineString, Polygon\n",
    "\n",
    "# Function to contain lines within region boundaries and track adjusted lines\n",
    "def contain_lines_within_region_boundaries(doc_pk, part_pk):\n",
    "    # Fetch page height and width\n",
    "    page_height, page_width = get_page_height_width(doc_pk, part_pk)\n",
    "\n",
    "    # Fetch all regions and lines for the part\n",
    "    regions = get_all_regions_of_part(doc_pk, part_pk)\n",
    "    lines = get_all_lines_of_part(doc_pk, part_pk)\n",
    "\n",
    "    adjusted_lines = []\n",
    "\n",
    "    # Create a circumference (boundary) for each region\n",
    "    for region in regions:\n",
    "        region['Circumference'] = Polygon(region['box'])\n",
    "\n",
    "    # Iterate through each line\n",
    "    for line in lines:\n",
    "        baseline = line['baseline']\n",
    "\n",
    "        # Ensure the baseline is correctly oriented\n",
    "        if baseline[0][0] > baseline[-1][0]:\n",
    "            baseline.reverse()\n",
    "\n",
    "        # Find the corresponding region for this line\n",
    "        this_region = search(line['region'], regions)\n",
    "        region_boundary = this_region['Circumference']\n",
    "\n",
    "        # Create a LineString for the baseline\n",
    "        baseline_line = LineString(baseline)\n",
    "\n",
    "        # If the baseline extends beyond the region, clip it to the boundary\n",
    "        if not region_boundary.contains(baseline_line):\n",
    "            # Find the intersection between the baseline and region boundary\n",
    "            intersection_line = region_boundary.intersection(baseline_line)\n",
    "\n",
    "            if not intersection_line.is_empty:\n",
    "                # Extract the new baseline from the intersection points\n",
    "                if isinstance(intersection_line, LineString):\n",
    "                    new_baseline = [[int(round(coord[0])), int(round(coord[1]))] for coord in intersection_line.coords]\n",
    "                else:\n",
    "                    # Handle cases where the intersection is a collection of points/lines\n",
    "                    new_baseline = [[int(round(coord[0])), int(round(coord[1]))] for geom in intersection_line.geoms for coord in geom.coords]\n",
    "\n",
    "                # Reverse baseline if originally reversed\n",
    "                if baseline[0][0] > baseline[-1][0]:\n",
    "                    new_baseline.reverse()\n",
    "\n",
    "                # Add the adjusted line to the list\n",
    "                adjusted_lines.append(line['pk'])\n",
    "                new_line = {'pk': line['pk'], 'baseline': new_baseline}\n",
    "\n",
    "                # Bulk update the lines with the new baselines\n",
    "                bulk_update_lines(doc_pk, part_pk, [new_line])\n",
    "\n",
    "    return adjusted_lines\n",
    "\n",
    "# Main function to contain lines, then apply custom repolygonization based on line typology\n",
    "def contain_and_repolygonize_with_custom_ascenders_descenders(doc_pk, part_pk, headers):\n",
    "    # Step 1: Contain lines within region boundaries and track adjusted lines\n",
    "    adjusted_lines = contain_lines_within_region_boundaries(doc_pk, part_pk)\n",
    "\n",
    "    # Step 2: Apply custom repolygonization for specific line typologies (only for adjusted lines)\n",
    "    if adjusted_lines:\n",
    "        # Repolygonize for line type 9\n",
    "        create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "            doc_pk=doc_pk, part_pk=part_pk, method='fix', linetypelist=[9],\n",
    "            ascender=30, descender=45, safetydistance=3\n",
    "        )\n",
    "\n",
    "        # Repolygonize for line types 20 and 27\n",
    "        create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "            doc_pk=doc_pk, part_pk=part_pk, method='fix', linetypelist=[20, 27],\n",
    "            ascender=65, descender=60, safetydistance=3\n",
    "        )\n",
    "\n",
    "        print(f\"Repoygonized {len(adjusted_lines)} lines based on custom ascender and descender values.\")\n",
    "    else:\n",
    "        print(\"No lines were out of bounds or needed repolygonization.\")\n",
    "\n",
    "# Example usage\n",
    "doc_pk = 8  # Replace with actual document primary key\n",
    "part_pk = 393  # Replace with actual part primary key\n",
    "\n",
    "# Call the function to contain and repolygonize lines\n",
    "contain_and_repolygonize_with_custom_ascenders_descenders(doc_pk, part_pk, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D04tIuUS4lRA"
   },
   "outputs": [],
   "source": [
    "\n",
    "from shapely.geometry import LineString, Polygon\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Function to gently align handwritten baselines without flattening them too much\n",
    "def smooth_handwritten_baseline(baseline):\n",
    "    # Calculate a gentle adjustment for handwritten lines: average y, but retain some slope\n",
    "    smoothed_baseline = []\n",
    "    x_coords = [point[0] for point in baseline]\n",
    "    y_coords = [point[1] for point in baseline]\n",
    "\n",
    "    # Fit a simple linear regression (y = mx + b) to preserve some slope\n",
    "    if len(x_coords) > 1:\n",
    "        m = (y_coords[-1] - y_coords[0]) / (x_coords[-1] - x_coords[0])  # Slope\n",
    "        b = y_coords[0] - m * x_coords[0]  # Intercept\n",
    "        # Generate smoothed baseline points\n",
    "        smoothed_baseline = [[x, int(round(m * x + b))] for x in x_coords]\n",
    "    else:\n",
    "        # If only one point, keep the original baseline\n",
    "        smoothed_baseline = baseline\n",
    "\n",
    "    return smoothed_baseline\n",
    "\n",
    "# Function to make the baseline more horizontally aligned\n",
    "def make_baseline_horizontal(baseline):\n",
    "    # Calculate the average y-coordinate of the baseline points\n",
    "    avg_y = sum([point[1] for point in baseline]) / len(baseline)\n",
    "\n",
    "    # Adjust the y-coordinates of the baseline to be closer to the average\n",
    "    horizontal_baseline = [[x, int(round(avg_y))] for x, y in baseline]\n",
    "\n",
    "    return horizontal_baseline\n",
    "\n",
    "# Function to simplify, align, contain, and repolygonize lines according to their typology\n",
    "def simplify_align_contain_and_repolygonize_lines(doc_pk, part_pk, simplification=10):\n",
    "    # Fetch the URL for lines in the part\n",
    "    lines_url = get_lines_url(doc_pk, part_pk)\n",
    "\n",
    "    # Fetch all lines and regions in the part\n",
    "    lines = loop_through_itempages_and_get_all(lines_url)\n",
    "    regions = get_all_regions_of_part(doc_pk, part_pk)\n",
    "\n",
    "    # Create polygons for each region (boundary)\n",
    "    for region in regions:\n",
    "        region['Circumference'] = Polygon(region['box'])\n",
    "\n",
    "    # List to track lines that need repolygonization by typology\n",
    "    repolygonize_lines_by_type = {\n",
    "        9: [],      # Line type 9\n",
    "        20: [],     # Line type 20\n",
    "        27: []      # Line type 27\n",
    "    }\n",
    "\n",
    "    # Iterate through each line for simplification, alignment, containment, and repolygonization\n",
    "    for line in lines:\n",
    "        baseline = line['baseline']\n",
    "        baseline_line = LineString(baseline)  # Convert the baseline to LineString object\n",
    "\n",
    "        # Step 1: Simplify the baseline\n",
    "        simplified_baseline = baseline_line.simplify(simplification)\n",
    "        new_baseline = [[int(round(x)), int(round(y))] for x, y in simplified_baseline.coords]\n",
    "\n",
    "        # Step 2: Align the baseline based on typology\n",
    "        typology = line.get('typology', None)\n",
    "        if typology in [20, 27]:\n",
    "            # For handwritten lines (20, 27), apply gentle smoothing, retaining slope\n",
    "            aligned_baseline = smooth_handwritten_baseline(new_baseline)\n",
    "        else:\n",
    "            # For other lines, flatten more aggressively\n",
    "            aligned_baseline = make_baseline_horizontal(new_baseline)\n",
    "\n",
    "        # Step 3: Contain the baseline within the region if needed\n",
    "        region = next((r for r in regions if r['pk'] == line['region']), None)\n",
    "        if region:\n",
    "            region_boundary = region['Circumference']\n",
    "            baseline_line = LineString(aligned_baseline)  # Update baseline to horizontally aligned one\n",
    "\n",
    "            if not region_boundary.contains(baseline_line):\n",
    "                # If baseline extends beyond the region, intersect it with the region boundary\n",
    "                intersection_line = region_boundary.intersection(baseline_line)\n",
    "\n",
    "                if not intersection_line.is_empty:\n",
    "                    if isinstance(intersection_line, LineString):\n",
    "                        aligned_baseline = [[int(round(x)), int(round(y))] for x, y in intersection_line.coords]\n",
    "                    else:\n",
    "                        # Handle cases where intersection might be a collection of points\n",
    "                        aligned_baseline = [[int(round(x)), int(round(y))] for geom in intersection_line.geoms for x, y in geom.coords]\n",
    "\n",
    "        # Step 4: Send updated baseline to the API\n",
    "        line_url = lines_url + str(line['pk']) + '/'\n",
    "        line_json = {'baseline': aligned_baseline}\n",
    "        res_line = requests.patch(line_url, headers=headers, data=json.dumps(line_json))\n",
    "\n",
    "        if res_line.status_code == 200:\n",
    "            #print(f\"Simplified, aligned, and contained baseline for line {line['pk']} updated successfully.\")\n",
    "\n",
    "            # After baseline is simplified, aligned, and contained, check the typology for custom repolygonization\n",
    "            if typology in repolygonize_lines_by_type:\n",
    "                repolygonize_lines_by_type[typology].append(line['pk'])\n",
    "        else:\n",
    "            print(f\"Failed to update line {line['pk']}. Status: {res_line.status_code}\")\n",
    "\n",
    "    # Apply custom repolygonization based on line typology\n",
    "    # Repolygonize for line type 9\n",
    "    if repolygonize_lines_by_type[9]:\n",
    "        create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "            doc_pk=doc_pk, part_pk=part_pk, method='fix', linetypelist=[9],\n",
    "            ascender=30, descender=45, safetydistance=3\n",
    "        )\n",
    "        #print(f\"Repoygonized {len(repolygonize_lines_by_type[9])} lines of type 9.\")\n",
    "\n",
    "    # Repolygonize for line types 20 and 27\n",
    "    if repolygonize_lines_by_type[20] or repolygonize_lines_by_type[27]:\n",
    "        create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "            doc_pk=doc_pk, part_pk=part_pk, method='fix', linetypelist=[20, 27],\n",
    "            ascender=65, descender=60, safetydistance=3\n",
    "        )\n",
    "        #print(f\"Repoygonized {len(repolygonize_lines_by_type[20]) + len(repolygonize_lines_by_type[27])} lines of types 20 and 27.\")\n",
    "\n",
    "# Example usage\n",
    "doc_pk = 8  # Replace with actual document primary key\n",
    "part_pk = 393  # Replace with actual part primary key\n",
    "simplification_tolerance = 20  # Adjust simplification tolerance as needed\n",
    "simplify_align_contain_and_repolygonize_lines(doc_pk, part_pk, simplification=simplification_tolerance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9e7_0WbsOx0-"
   },
   "outputs": [],
   "source": [
    "doc_pk = 8\n",
    "parts = get_all_parts(doc_pk)\n",
    "\n",
    "for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    simplify_align_contain_and_repolygonize_lines(doc_pk, part_pk, simplification=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG81MDUUKM16"
   },
   "outputs": [],
   "source": [
    "#loop thru all parts and repolygonize according to the fixed ascender\\descender parameters (default= 30,45)\n",
    "# Get all parts of the document\n",
    "doc_pk = 8\n",
    "parts = get_all_parts(doc_pk)\n",
    "\n",
    "for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "    doc_pk,part_pk,method='fix',regiontypelist=[], ascender=30,descender=45, safetydistance=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104797,
     "status": "ok",
     "timestamp": 1728401016828,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "PTysGps-mqv7",
    "outputId": "1e853ac5-9e56-4819-def6-5b80239730fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    }
   ],
   "source": [
    "#loop thru all parts and repolygonize according to the fixed ascender\\descender parameters (handwritten\\fragmented= 65,60)\n",
    "# Get all parts of the document\n",
    "doc_pk = 8\n",
    "parts = get_all_parts(doc_pk)\n",
    "\n",
    "for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    try:\n",
    "      create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "      doc_pk=doc_pk,part_pk=part_pk,method='fix',linetypelist=[20,27], ascender=65,descender=60, safetydistance=3)\n",
    "    except: #lower the values\n",
    "      print(part_pk)\n",
    "      create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "      doc_pk=doc_pk,part_pk=part_pk,method='fix',linetypelist=[20,27], ascender=58,descender=60, safetydistance=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1828,
     "status": "ok",
     "timestamp": 1728461444627,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "bneY5hICEqZu",
    "outputId": "b4952299-c3cc-4336-89d3-924bcc0486e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "    8,393,method='fix', regiontypelist=[],linetypelist=[20],ascender=30,descender=45, safetydistance=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1728399171921,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "0ubtuz8yAUvT",
    "outputId": "739c7cdb-a844-48e5-80a7-a849a1c9a20f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(8,255,method='average_line_height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1728390834122,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "4_5OL56O7m4T",
    "outputId": "112cc24b-9f18-4559-9983-6cfe902df51e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repolygonize line :  6333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repolygonize_line(8, 265, 6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1728391308094,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "TSAhKM-JJUMn",
    "outputId": "a401d49b-4ec1-4afe-9385-a07fcd880633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repolygonize :  266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repolygonize(8,266)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvEQuJmJasby"
   },
   "source": [
    "line association and repolygonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95725,
     "status": "ok",
     "timestamp": 1728461888504,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "ZPGCyVAiVufW",
    "outputId": "814e5cf5-2fb2-4614-9c04-26c6ded39327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found unassociated line in doc 8 , on part 307 , line: 2\n",
      "found unassociated line in doc 8 , on part 307 , line: 3\n",
      "found unassociated line in doc 8 , on part 337 , line: 2\n",
      "found unassociated line in doc 8 , on part 337 , line: 3\n",
      "found unassociated line in doc 8 , on part 338 , line: 2\n",
      "found unassociated line in doc 8 , on part 339 , line: 3\n",
      "found unassociated line in doc 8 , on part 340 , line: 3\n",
      "found unassociated line in doc 8 , on part 342 , line: 3\n",
      "found unassociated line in doc 8 , on part 344 , line: 1\n",
      "found unassociated line in doc 8 , on part 344 , line: 2\n",
      "found unassociated line in doc 8 , on part 349 , line: 3\n",
      "found unassociated line in doc 8 , on part 358 , line: 2\n",
      "found unassociated line in doc 8 , on part 367 , line: 1\n",
      "found unassociated line in doc 8 , on part 367 , line: 2\n",
      "found unassociated line in doc 8 , on part 389 , line: 4\n"
     ]
    }
   ],
   "source": [
    "# check whether ther are any non-associated lines\n",
    "doc_pk = 8\n",
    "parts = get_all_parts(doc_pk)\n",
    "\n",
    "for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    find_or_delete_unlinked_lines(doc_pk, part_pk, do_delete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3130,
     "status": "ok",
     "timestamp": 1728462695329,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "kXQfKNyzW40I",
    "outputId": "c7bd4c19-987f-4eca-e8a1-4e8cc073722e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n",
      "associating lines\n",
      "[{'pk': 18849, 'document_part': 389, 'external_id': 'eSc_line_843c3d86', 'order': 4, 'region': 5487, 'baseline': [[784, 486], [1112, 486]], 'mask': [[783, 483], [782, 489], [799, 490], [808, 499], [817, 496], [824, 501], [848, 501], [859, 490], [895, 497], [904, 491], [916, 491], [925, 501], [949, 501], [960, 492], [973, 492], [983, 501], [997, 494], [1003, 501], [1020, 501], [1029, 494], [1037, 501], [1049, 496], [1054, 501], [1078, 497], [1099, 501], [1110, 492], [1111, 476], [1102, 470], [1053, 470], [1050, 473], [1044, 470], [1026, 473], [1003, 469], [1000, 473], [995, 469], [981, 469], [974, 476], [953, 476], [934, 469], [922, 475], [843, 475], [833, 471], [825, 475], [783, 475], [783, 483]], 'typology': 9}]\n",
      "reordering\n",
      "reorder  389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "part_pk = 389\n",
    "associate_lines_with_existing_regions_and_reorder(doc_pk,part_pk)\n",
    "create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "    8,part_pk,method='fix', regiontypelist=[],linetypelist=[9,20],ascender=30,descender=45, safetydistance=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_UMY2rOJkiE"
   },
   "source": [
    "Region Typology Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93428,
     "status": "ok",
     "timestamp": 1728475456730,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "NJS2vA9w6fun",
    "outputId": "85b4ca42-737e-4b85-8c66-27def8ac8464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typology '100': 207 regions\n",
      "Typology '101': 40 regions\n",
      "Typology '104': 455 regions\n",
      "Typology '102': 100 regions\n",
      "Typology '103': 43 regions\n",
      "Typology '105': 36 regions\n",
      "Typology '106': 32 regions\n",
      "Typology '107': 1 regions\n"
     ]
    }
   ],
   "source": [
    "# Function to count the occurrence of each region typology in the document\n",
    "def count_region_typologies(doc_pk):\n",
    "    # Dictionary to store the count of each typology\n",
    "    region_typology_counts = defaultdict(int)\n",
    "\n",
    "    # Get all parts of the document\n",
    "    parts = get_all_parts(doc_pk)\n",
    "\n",
    "    for part in parts:\n",
    "        part_pk = part['pk']\n",
    "        #print(f\"Processing part {part_pk}\")\n",
    "\n",
    "        # Fetch all regions in the current part\n",
    "        regions = get_all_regions_of_part(doc_pk, part_pk)  # Assuming this function fetches all regions for the part\n",
    "\n",
    "        for region in regions:\n",
    "            region_typology = region.get('typology', None)\n",
    "\n",
    "            # If the region has a typology, count it\n",
    "            if region_typology:\n",
    "                region_typology_counts[region_typology] += 1\n",
    "            else:\n",
    "                region_typology_counts['None'] += 1  # Count regions with no typology as 'None'\n",
    "\n",
    "    # Convert the region typology counts to a normal dictionary (optional, for easier reading)\n",
    "    return dict(region_typology_counts)\n",
    "\n",
    "# Call the function with the document primary key and print the results\n",
    "region_typology_summary = count_region_typologies(doc_pk=555)  # Replace XXX with the actual document primary key\n",
    "\n",
    "# Print the result\n",
    "for typology, count in region_typology_summary.items():\n",
    "    print(f\"Typology '{typology}': {count} regions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1732774348749,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "ovPBieoX9SVC",
    "outputId": "8609a8a0-a0e0-4ac8-afe8-d7c31a5058c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get document segmentation ontology for document:  555\n",
      "https://env-9828609.us.reclaim.cloud/api/documents/555/\n",
      "Document: 555  with  210  parts\n",
      "region types: [{'pk': 104, 'name': 'Column'}, {'pk': 103, 'name': 'Date(s)'}, {'pk': 105, 'name': 'footer'}, {'pk': 100, 'name': 'header'}, {'pk': 102, 'name': 'Metadata'}, {'pk': 106, 'name': 'Motto'}, {'pk': 101, 'name': 'Pricing'}]\n",
      "line types: []\n",
      "transcription_level_list: [{'pk': 591, 'name': 'transcript', 'archived': False, 'avg_confidence': None, 'created_at': '2024-10-09T11:33:22.224457Z', 'comments': None}, {'pk': 590, 'name': 'manual', 'archived': False, 'avg_confidence': None, 'created_at': '2024-10-09T11:29:11.212256Z', 'comments': None}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(210,\n",
       " [{'pk': 591,\n",
       "   'name': 'transcript',\n",
       "   'archived': False,\n",
       "   'avg_confidence': None,\n",
       "   'created_at': '2024-10-09T11:33:22.224457Z',\n",
       "   'comments': None},\n",
       "  {'pk': 590,\n",
       "   'name': 'manual',\n",
       "   'archived': False,\n",
       "   'avg_confidence': None,\n",
       "   'created_at': '2024-10-09T11:29:11.212256Z',\n",
       "   'comments': None}],\n",
       " [{'pk': 104, 'name': 'Column'},\n",
       "  {'pk': 103, 'name': 'Date(s)'},\n",
       "  {'pk': 105, 'name': 'footer'},\n",
       "  {'pk': 100, 'name': 'header'},\n",
       "  {'pk': 102, 'name': 'Metadata'},\n",
       "  {'pk': 106, 'name': 'Motto'},\n",
       "  {'pk': 101, 'name': 'Pricing'}],\n",
       " [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_basic_info(555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93801,
     "status": "ok",
     "timestamp": 1728475698398,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "vmrl_zr9-F1C",
    "outputId": "a351f524-7989-4882-9a39-54ce842b8f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found regions with specific typologies:\n",
      "Region PK: 7589, Document Part: 5739, Typology: 107\n"
     ]
    }
   ],
   "source": [
    "# Function to find regions with specific typologies (37, 58, None)\n",
    "def find_specific_region_typologies(doc_pk, target_typologies):\n",
    "    # List to store regions with matching typologies\n",
    "    matching_regions = []\n",
    "\n",
    "    # Get all parts of the document\n",
    "    parts = get_all_parts(doc_pk)\n",
    "\n",
    "    for part in parts:\n",
    "        part_pk = part['pk']\n",
    "        #print(f\"Processing part {part_pk}\")\n",
    "\n",
    "        # Fetch all regions in the current part\n",
    "        regions = get_all_regions_of_part(doc_pk, part_pk)\n",
    "\n",
    "        for region in regions:\n",
    "            region_typology = region.get('typology', None)\n",
    "\n",
    "            # Check if the region's typology is in our target list\n",
    "            if region_typology in target_typologies or region_typology is None:\n",
    "                # Collect detailed information about the region\n",
    "                region_info = {\n",
    "                    'region_pk': region['pk'],\n",
    "                    'document_part': part_pk,\n",
    "                    'typology': region_typology,\n",
    "                    #'external_id': region.get('external_id', 'No ID'),\n",
    "                    #'box': region.get('box', 'No box info')\n",
    "                }\n",
    "\n",
    "                # Add the region info to the matching list\n",
    "                matching_regions.append(region_info)\n",
    "\n",
    "    return matching_regions\n",
    "\n",
    "# Define the target typologies to search for\n",
    "target_typologies = [107]  # Adding None as a separate case\n",
    "\n",
    "# Call the function to find regions with typology 37, 58, or None\n",
    "specific_regions = find_specific_region_typologies(doc_pk=555, target_typologies=target_typologies)  # Replace XXX with the actual document primary key\n",
    "\n",
    "# Print out the details of the regions found\n",
    "if specific_regions:\n",
    "    print(\"Found regions with specific typologies:\")\n",
    "    for region in specific_regions:\n",
    "        typology_label = 'None' if region['typology'] is None else region['typology']\n",
    "        print(f\"Region PK: {region['region_pk']}, Document Part: {region['document_part']}, Typology: {typology_label}\") #External ID: {region['external_id']}, Box: {region['box']}\")\n",
    "else:\n",
    "    print(\"No regions found with the specified typologies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1728464613509,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "WfN0tZVyhJL0",
    "outputId": "0ede3667-28b3-4823-cb9a-5d6aa0223ae1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [204]>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_region(8,241,2638)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86883,
     "status": "ok",
     "timestamp": 1728453065858,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "hG6JrG9Sz9RD",
    "outputId": "f4284ae7-6049-43b4-fb49-3f444d32386f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate region types found in any part.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to check for duplicate region types within each part of the document\n",
    "def check_duplicate_region_typologies(doc_pk):\n",
    "    # Get all parts of the document\n",
    "    parts = get_all_parts(doc_pk)\n",
    "\n",
    "    # List to store parts with duplicate region types\n",
    "    parts_with_duplicates = []\n",
    "\n",
    "    for part in parts:\n",
    "        part_pk = part['pk']\n",
    "        # Dictionary to store the count of each typology in the part\n",
    "        region_typology_counts = defaultdict(int)\n",
    "\n",
    "        # Fetch all regions in the current part\n",
    "        regions = get_all_regions_of_part(doc_pk, part_pk)  # Assuming this function fetches all regions for the part\n",
    "\n",
    "        # Count the occurrence of each region typology in the part\n",
    "        for region in regions:\n",
    "            region_typology = region.get('typology', None)\n",
    "            if region_typology:\n",
    "                region_typology_counts[region_typology] += 1\n",
    "            else:\n",
    "                region_typology_counts['None'] += 1  # Count regions with no typology as 'None'\n",
    "\n",
    "        # Check for duplicates (i.e., any region type with a count > 1)\n",
    "        duplicate_typologies = [typology for typology, count in region_typology_counts.items() if count > 1]\n",
    "\n",
    "        # If duplicates are found, record the part PK and the duplicate types\n",
    "        if duplicate_typologies:\n",
    "            parts_with_duplicates.append({\n",
    "                'part_pk': part_pk,\n",
    "                'duplicate_typologies': duplicate_typologies\n",
    "            })\n",
    "\n",
    "    # Report parts with duplicate region types\n",
    "    if parts_with_duplicates:\n",
    "        print(\"Found duplicate region types in the following parts:\")\n",
    "        for part_info in parts_with_duplicates:\n",
    "            print(f\"Part PK: {part_info['part_pk']}, Duplicate Region Types: {part_info['duplicate_typologies']}\")\n",
    "    else:\n",
    "        print(\"No duplicate region types found in any part.\")\n",
    "\n",
    "# Example usage\n",
    "doc_pk = 8  # Replace with your actual document primary key\n",
    "check_duplicate_region_typologies(doc_pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1728305316435,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "NgYrQdsJBH6S",
    "outputId": "6b2267c6-de8e-4eb9-f7c2-d31ac772f254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [204]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_region(8,292,3367)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TrDLZAMKg59"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182038,
     "status": "ok",
     "timestamp": 1728464917612,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "LFedRG0iKfqn",
    "outputId": "7a364072-d2a7-43b3-f4ea-f0722b27565e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 1 lines with empty or no transcription in part 245:\n",
      "Line PK: 23301\n"
     ]
    }
   ],
   "source": [
    "# Function to identify empty lines across all parts of the document\n",
    "def identify_empty_lines_in_document(doc_pk, tr_level, min_length=1):\n",
    "    # Fetch all parts of the document\n",
    "    parts = get_all_parts(doc_pk)\n",
    "\n",
    "    # Iterate over each part\n",
    "    for part in parts:\n",
    "        part_pk = part['pk']\n",
    "        #print(f\"Processing part {part_pk}...\")\n",
    "\n",
    "        # Fetch transcriptions and all lines for the part\n",
    "        transcriptions = get_page_transcription(doc_pk, part_pk, tr_level)\n",
    "        lines = get_all_lines_of_part(doc_pk, part_pk)\n",
    "        line_pks = get_pks_of_dict_list(lines)\n",
    "\n",
    "        # List to store lines that have empty or short transcriptions\n",
    "        lines2identify = list()\n",
    "\n",
    "        # Identify lines with short transcription content\n",
    "        for tr in transcriptions:\n",
    "            if len(tr['content'].strip()) < min_length:  # Ensure spaces are counted as empty\n",
    "                lines2identify.append(tr['line'])  # Store line pk\n",
    "\n",
    "        # Identify lines that have no transcriptions\n",
    "        for line_pk in line_pks:\n",
    "            if line_pk not in [tr['line'] for tr in transcriptions]:  # Lines with no transcription\n",
    "                lines2identify.append(line_pk)\n",
    "\n",
    "        # Report the identified lines instead of deleting them\n",
    "        if len(lines2identify) > 0:\n",
    "            print(f\"Identified {len(lines2identify)} lines with empty or no transcription in part {part_pk}:\")\n",
    "            for line_pk in lines2identify:\n",
    "                print(f\"Line PK: {line_pk}\")\n",
    "\n",
    "# Example usage for document pk 8\n",
    "doc_pk = 8  # Document primary key\n",
    "tr_level = 18  # Replace with actual transcription level pk (e.g., 'manual' = 18')\n",
    "min_length = 1  # Minimum length to consider a transcription not empty\n",
    "\n",
    "# Identify empty lines across all parts of the document\n",
    "identify_empty_lines_in_document(doc_pk, tr_level, min_length=min_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85081,
     "status": "ok",
     "timestamp": 1728477933860,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -180
    },
    "id": "313sdWzAPTqa",
    "outputId": "f1319319-4c43-40a4-e8b9-8464e378ff33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts without regions found:\n",
      "Part PK: 5698\n",
      "Part PK: 5699\n",
      "Part PK: 5700\n",
      "Part PK: 5701\n",
      "Part PK: 5702\n",
      "Part PK: 5703\n",
      "Part PK: 5704\n",
      "Part PK: 5712\n",
      "Part PK: 5713\n",
      "Part PK: 5778\n",
      "Part PK: 5779\n"
     ]
    }
   ],
   "source": [
    "def find_parts_without_regions(doc_pk):\n",
    "    \"\"\"\n",
    "    Find and report parts of a document that have no regions at all.\n",
    "\n",
    "    Parameters:\n",
    "    - doc_pk: Primary key of the document to check.\n",
    "    \"\"\"\n",
    "    # Fetch all parts of the document\n",
    "    parts = get_all_parts(doc_pk)\n",
    "\n",
    "    # List to track parts without regions\n",
    "    parts_without_regions = []\n",
    "\n",
    "    # Iterate through each part and check for regions\n",
    "    for part in parts:\n",
    "        part_pk = part['pk']\n",
    "\n",
    "        # Fetch regions for the part\n",
    "        regions = get_all_regions_of_part(doc_pk, part_pk)\n",
    "\n",
    "        # If no regions are found, add the part to the report list\n",
    "        if not regions:\n",
    "            parts_without_regions.append(part_pk)\n",
    "\n",
    "    # Report the parts that have no regions\n",
    "    if parts_without_regions:\n",
    "        print(\"Parts without regions found:\")\n",
    "        for part_pk in parts_without_regions:\n",
    "            print(f\"Part PK: {part_pk}\")\n",
    "    else:\n",
    "        print(\"All parts have regions.\")\n",
    "    return parts_without_regions\n",
    "\n",
    "# Example usage\n",
    "doc_pk = 555  # Replace with actual document primary key\n",
    "\n",
    "parts_without_regions  = find_parts_without_regions(doc_pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMRWYy45UWeq"
   },
   "outputs": [],
   "source": [
    "for part in parts_without_regions:\n",
    "    delete_part(555,part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu-yVjZLamng"
   },
   "source": [
    "##Multi-layered Model Segmentation and Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5qo5aHYa8xd"
   },
   "source": [
    "explore the segment and transcribe models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1138,
     "status": "ok",
     "timestamp": 1742387213934,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "ig1SFzHPascn",
    "outputId": "db5295f9-a350-4e89-a6e3-fa153ab4af99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pk': 15,\n",
       "  'name': 'seg_news_1.0',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/ef38940a/seg_news_10.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 5,\n",
       "  'name': 'syr_expo_25',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/b09fec28/syr_expo_25.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 29,\n",
       "  'name': 'padded_1800_region_model',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/d61bb8e5/padded_1800_region_model_92.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 27,\n",
       "  'name': 'padded_1700_baseline_model',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/286e05c3/padded_trial_model_49.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 26,\n",
       "  'name': 'default_seg',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/e946a736/blla.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 28,\n",
       "  'name': 'Persian_Journals_LA',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/81b2cd4d/region_model_11_10_24_68.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 30,\n",
       "  'name': 'padded_1800_region_model_2',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/4e8f3540/padded_1800_region_model_42.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 31,\n",
       "  'name': 'padded_1800_baseline_model',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/0dc0b0c9/padded_1800_model_84.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 21,\n",
       "  'name': 'region_model_03_08_24',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/133ed94f/region_model_03_08_24.mlmodel',\n",
       "  'job': 'Segment',\n",
       "  'owner': 'admin',\n",
       "  'training': False}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_seg_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1742387336274,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "ilfLUxEf4tZg",
    "outputId": "5963dd91-a3c9-4fea-a049-4ab6f6eff2af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pk': 10,\n",
       "  'name': 'Persian (persian_best base)',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/f9746068/persian-persian_best-base.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 1,\n",
       "  'name': 'trial farsi model',\n",
       "  'file': None,\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 13,\n",
       "  'name': 'catmus-print-fondue-tiny-2024-01-31',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/d5e72571/catmus-print-fondue-tiny-2024-01-31.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 12,\n",
       "  'name': 'catmus-print-fondue-small-2024-01-31',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/220c2566/catmus-print-fondue-small-2024-01-31.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 7,\n",
       "  'name': 'All Arabic Scripts',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/bd6a7ef5/all_arabic_scripts.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 6,\n",
       "  'name': 'BiblIA_01',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/56478c34/biblia_01.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 18,\n",
       "  'name': 'Persian_third_model_best',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/f0df58c1/second_model_08_07_24_best.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 32,\n",
       "  'name': 'finetuned_dijest_model_13_11_24_best',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/bb2d214d/finetuned_dijest_model_13_11_24_best.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 20,\n",
       "  'name': 'Persian_fifth_model_best',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/a5557eab/fifth_model_best.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 14,\n",
       "  'name': 'catmus-print-fondue-large',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/08f5f8c5/catmus-print-fondue-large.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 2,\n",
       "  'name': 'CATMUS Medieval 1.0.0',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/9715b680/catmus-medieval-100.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 19,\n",
       "  'name': 'Persian_fourth_model_best',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/07f6332d/fourth_model_08_07_24_best.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 8,\n",
       "  'name': 'arabic_best',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/fb17343d/arabic_best.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 17,\n",
       "  'name': 'Persian_second_model_best',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/cd2f9574/second_model_best.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 3,\n",
       "  'name': 'german_print',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/daffd2c4/german_print.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False},\n",
       " {'pk': 4,\n",
       "  'name': 'persian_best',\n",
       "  'file': 'http://env-9828609.us.reclaim.cloud/media/models/6bab8c12/persian_best.mlmodel',\n",
       "  'job': 'Recognize',\n",
       "  'owner': 'admin',\n",
       "  'training': False}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_trans_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1742388233942,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "1CzSNE4_411Z",
    "outputId": "c61bb73f-88a9-42c4-e9d9-23dbf9f1a9e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pk': 318,\n",
       "  'name': 'manual',\n",
       "  'archived': False,\n",
       "  'avg_confidence': None,\n",
       "  'created_at': '2024-07-08T12:32:53.718884Z',\n",
       "  'comments': None}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_transcription_levels(290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_RnBBqiZ9KC"
   },
   "source": [
    "now use info to run only regions model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1742387519129,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "2aTolTrNbARi",
    "outputId": "9012726e-1685-403f-820e-8656e15b7aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom regions model is Persian_Journals_LA = 28\n",
    "segment(290, 28, [4038], steps = 'regions',override = True,print_status = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIi5lCFWaAKP"
   },
   "source": [
    "now run only lines (and masks) model, without overriding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1742387639165,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "sR098JwTcuTh",
    "outputId": "c54f19bd-73f8-48f6-bdab-786e7c080a58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#default blla = 26\n",
    "segment(290, 26, [4038], steps = 'lines',override = False, print_status = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCpZKNWUZzU_"
   },
   "source": [
    "now remove unassociated lines.\n",
    "\n",
    "\n",
    "potentially could add more preprocessing, e.g., if region is column and there are less than 5 lines, delete region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3703,
     "status": "ok",
     "timestamp": 1742387921209,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "u9nFAY9jaN6M",
    "outputId": "16090dcd-7757-4318-f98b-79887d88f5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found unassociated line in doc 290 , on part 4038 , line: 16\n",
      "found unassociated line in doc 290 , on part 4038 , line: 17\n",
      "found unassociated line in doc 290 , on part 4038 , line: 18\n",
      "found unassociated line in doc 290 , on part 4038 , line: 19\n",
      "found unassociated line in doc 290 , on part 4038 , line: 20\n",
      "found unassociated line in doc 290 , on part 4038 , line: 85\n",
      "found unassociated line in doc 290 , on part 4038 , line: 86\n",
      "found unassociated line in doc 290 , on part 4038 , line: 87\n",
      "found unassociated line in doc 290 , on part 4038 , line: 88\n",
      "found unassociated line in doc 290 , on part 4038 , line: 89\n",
      "found unassociated line in doc 290 , on part 4038 , line: 91\n",
      "found unassociated line in doc 290 , on part 4038 , line: 92\n",
      "found unassociated line in doc 290 , on part 4038 , line: 93\n",
      "found unassociated line in doc 290 , on part 4038 , line: 94\n",
      "found unassociated line in doc 290 , on part 4038 , line: 95\n",
      "found unassociated line in doc 290 , on part 4038 , line: 96\n",
      "found unassociated line in doc 290 , on part 4038 , line: 97\n",
      "found unassociated line in doc 290 , on part 4038 , line: 98\n",
      "found unassociated line in doc 290 , on part 4038 , line: 99\n",
      "found unassociated line in doc 290 , on part 4038 , line: 100\n",
      "found unassociated line in doc 290 , on part 4038 , line: 101\n",
      "found unassociated line in doc 290 , on part 4038 , line: 102\n",
      "reorder  4038\n"
     ]
    }
   ],
   "source": [
    "find_or_delete_unlinked_lines(290,4038,do_delete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVAvnSFohjTb"
   },
   "outputs": [],
   "source": [
    "#if region is column and number of lines is less than 5, delete region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr-ezDPKatZ5"
   },
   "source": [
    "and now transcribe with custom HTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1742388254872,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "V9F9otHx3oWw",
    "outputId": "1a08aac5-44f2-4cdb-ba49-36649b1d26a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom HTR model is fifth_persian_model = 20, make sure to also get transcription level\n",
    "transcribe(290, 20, [4038], 318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21926,
     "status": "ok",
     "timestamp": 1732795041201,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "8K6GWyJ_mVUe",
    "outputId": "ecf4035e-8c7d-4854-e7ba-c931be9acf57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: i want to run a segment of all images (parts) of a document (its pk is 129)\n",
    "\n",
    "segment(562, 30, get_part_pk_list(561), steps = 'regions',override = True,print_status = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20966,
     "status": "ok",
     "timestamp": 1732795153980,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "aZR79t-4orfg",
    "outputId": "e3d3b42c-f56b-4c75-f484-3a28c416a882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(561, 31, get_part_pk_list(561), steps = 'lines',override = True,print_status = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6MvJ26Yi8lu"
   },
   "source": [
    "can run double segmentation models, now want to check if can associate lines with closest region, and then trim them to the polygon boundary. if it doesnt work, can just delete the unassociated lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4952,
     "status": "ok",
     "timestamp": 1732775849219,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "8_zRutL1eLmE",
    "outputId": "bc13900f-ea2a-41b4-b8c4-19698ab187be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3388\n",
      "associating lines\n",
      "[{'pk': 30356, 'document_part': 3388, 'external_id': 'eSc_line_30b18195', 'order': 13, 'region': 8749, 'baseline': [[363, 682], [774, 697], [1069, 700]], 'mask': [[363, 682], [362, 696], [530, 698], [534, 700], [547, 706], [548, 706], [549, 706], [579, 701], [583, 701], [590, 707], [592, 707], [593, 707], [594, 707], [632, 702], [637, 702], [639, 702], [658, 709], [659, 709], [661, 709], [693, 705], [698, 704], [701, 705], [712, 710], [713, 710], [714, 710], [715, 710], [731, 706], [781, 712], [782, 712], [821, 708], [843, 714], [844, 714], [845, 714], [846, 714], [859, 708], [865, 715], [866, 715], [868, 715], [869, 715], [870, 715], [882, 709], [921, 716], [922, 716], [972, 711], [987, 718], [988, 718], [989, 718], [1068, 712], [1069, 700], [1068, 676], [1002, 671], [1001, 671], [1000, 671], [996, 674], [979, 680], [959, 672], [954, 670], [953, 670], [952, 670], [951, 670], [945, 671], [921, 679], [899, 670], [895, 669], [894, 669], [893, 669], [885, 670], [865, 674], [846, 669], [837, 667], [836, 667], [835, 667], [834, 667], [830, 668], [812, 677], [777, 677], [743, 674], [736, 666], [734, 665], [734, 663], [733, 663], [732, 663], [731, 663], [720, 666], [701, 669], [691, 665], [686, 662], [685, 662], [684, 662], [683, 662], [678, 665], [659, 671], [572, 671], [517, 660], [516, 660], [515, 660], [489, 668], [363, 656], [363, 682]], 'typology': 35}]\n",
      "reordering\n",
      "reorder  3388\n"
     ]
    }
   ],
   "source": [
    "# associate_line_with_existing_region(129,3388,line)\n",
    "associate_lines_with_existing_regions_and_reorder(129,3388)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1744,
     "status": "ok",
     "timestamp": 1732775937371,
     "user": {
      "displayName": "Noam Maeir",
      "userId": "16506007319421436294"
     },
     "user_tz": -120
    },
    "id": "_gArY3Nnfbgk",
    "outputId": "63aae4ea-7333-4824-e871-7a2d3f88f468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found unassociated line in doc 129 , on part 3388 , line: 11\n",
      "found unassociated line in doc 129 , on part 3388 , line: 12\n",
      "found unassociated line in doc 129 , on part 3388 , line: 14\n",
      "found unassociated line in doc 129 , on part 3388 , line: 15\n",
      "found unassociated line in doc 129 , on part 3388 , line: 42\n",
      "reorder  3388\n"
     ]
    }
   ],
   "source": [
    "find_or_delete_unlinked_lines(129,3388,do_delete=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1hyBDa55mPolZve4Ih1qpXd94i6f8Xno-",
     "timestamp": 1728302198247
    },
    {
     "file_id": "1Olx6XjOCKTGQfPaLZpoEt0twAUhrLYXZ",
     "timestamp": 1715343863418
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
