{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCN4Fu2EchQn"
   },
   "source": [
    "Click on your name and then profile to arrive your eScriptorium instance. If you are working on the main instance, that is `https://msia.escriptorium.fr/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhBZ3GdTceXd"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhFgWj6-c6lM"
   },
   "source": [
    "Then click on API and copy the alphanumeric code after \"Token\" (after and not including the :). This is your secret \"token\" which you should not share with anyone as it gives access to all data to which you have access whether your own or whether shared with you.\n",
    "\n",
    "If you work on the msia instance replace `abe8934572784dea` with your code in the following cell.\n",
    "\n",
    "If you work on another instance, you need to give it a name instead of `YOUR_SERVERNAME2`, replace YOURTOKEN2 with your token and replace \"URL2\" with the url (until but not including the first slash) of your instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6SU61rMc-6t"
   },
   "outputs": [],
   "source": [
    "#@title initialization (seize your token and server and then run it)\n",
    "%%writefile __init__.py\n",
    "import copy\n",
    "serverconnections = [{'servername':'$YOUR_INSTANCENAME_HERE', #choose freely\n",
    "  'headers' : {'Authorization':'Token $YOURTOKEN_HERE'}, # as in UI\n",
    "  'root_url' : 'YOUR_INSTANCE_URL_HERE'}]\n",
    "  # you can add further instance or account connections if you have them\n",
    "\n",
    "for serverconnection in serverconnections:\n",
    "    serverconnection['headersbrief'] = copy.deepcopy(serverconnection['headers'])\n",
    "    serverconnection['headers']['Content-type'] = 'application/json'\n",
    "    serverconnection['headers']['Accept']= 'application/json'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  print(\"<> Running the module as a script! <>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "l7uFT1sgw3tQ"
   },
   "outputs": [],
   "source": [
    "#@title import modules - please run this cell to import functions\n",
    "\n",
    "from __init__ import serverconnections\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import numpy\n",
    "from numpy import rad2deg,deg2rad,array,atleast_2d,cos,sin,arctan2,squeeze,cross,linalg\n",
    "import math\n",
    "from math import sqrt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import networkx as nx\n",
    "import collections\n",
    "from statistics import mean\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely import LineString\n",
    "from shapely import intersection\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import split as poly_split\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "from typing import Any, Tuple, Union, List, Dict, Type, TypeVar\n",
    "from imageio import imread\n",
    "\n",
    "!pip install Levenshtein\n",
    "import Levenshtein\n",
    "#!pip install bidict\n",
    "#import bidict\n",
    "#!pip install xlsxwriter\n",
    "#import xlsxwriter\n",
    "!pip install xmltodict\n",
    "import xmltodict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_4IS2IZ7wggh"
   },
   "outputs": [],
   "source": [
    "# @title helper functions\n",
    "\n",
    "def loop_through_itempages_and_get_all(base_url,suffix=''):\n",
    "    result_list=[]\n",
    "    def get_page(page,result_list):\n",
    "        data = False\n",
    "        #url = urljoin(base_url, '?page=%d' % page)\n",
    "        if suffix=='':\n",
    "          url = base_url+'?page='+str(page)\n",
    "        else:\n",
    "          url = base_url+'?page='+str(page)+'&'+suffix\n",
    "        res = requests.get(url, headers=headers)\n",
    "        #print(res.json())\n",
    "        try:\n",
    "            data = res.json()\n",
    "        except json.decoder.JSONDecodeError as e:\n",
    "            print('exception')\n",
    "            print(res)\n",
    "        else:\n",
    "          if data:\n",
    "            #print(data)\n",
    "            result_list += [item for item in data['results']]\n",
    "        if data:\n",
    "          if data['next']:\n",
    "            get_page(page+1,result_list)\n",
    "    get_page(1,result_list)\n",
    "    return(result_list)\n",
    "\n",
    "def get_pks_of_dict_list(list_of_dictionaries):\n",
    "    pks =[item['pk'] for item in list_of_dictionaries]\n",
    "    return pks\n",
    "\n",
    "def get_sthg_from_dict_list(list_of_dictionaries,sthg):\n",
    "  return([item[sthg] for item in list_of_dictionaries])\n",
    "\n",
    "# get_urls--------------------------------------------------------------------------------------------\n",
    "def get_documents_url():\n",
    "  return root_url+'/api/documents/'\n",
    "\n",
    "def get_specific_doc_url(doc_pk):\n",
    "  return get_documents_url()+str(doc_pk)+'/'\n",
    "\n",
    "def get_parts_url(doc_pk):\n",
    "  return get_specific_doc_url(doc_pk)+'parts/'\n",
    "\n",
    "def get_specific_part_url(doc_pk,part_pk):\n",
    "  return get_parts_url(doc_pk)+str(part_pk)+'/'\n",
    "\n",
    "def get_metadata_url(doc_pk,part_pk):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'metadata/'\n",
    "\n",
    "def get_regions_url(doc_pk,part_pk):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'blocks/'\n",
    "\n",
    "def get_specific_region_url(doc_pk,part_pk,region_pk):\n",
    "  return get_regions_url(doc_pk,part_pk)+str(region_pk)+'/'\n",
    "\n",
    "def get_lines_url(doc_pk,part_pk):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'lines/'\n",
    "\n",
    "def get_specific_line_url(doc_pk,part_pk,line_pk):\n",
    "  return get_lines_url(doc_pk,part_pk)+str(line_pk)+'/'\n",
    "\n",
    "def get_doc_transcriptions_url(doc_pk):\n",
    "  return get_specific_doc_url(doc_pk)+'transcriptions/'\n",
    "\n",
    "def get_specific_transcription_level_url(doc_pk,tr_level):\n",
    "  return get_doc_transcriptions_url(doc_pk)+str(tr_level)+'/'\n",
    "\n",
    "def get_part_transcriptions_base_url(doc_pk,part_pk):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'transcriptions/'\n",
    "\n",
    "def get_part_transcriptions_url(doc_pk,part_pk,tr_level):\n",
    "  return get_specific_part_url(doc_pk,part_pk)+'transcriptions/?transcription='+str(tr_level)+'&'\n",
    "\n",
    "def get_specific_transcription_url(doc_pk,part_pk,tr_pk):\n",
    "  return get_part_transcriptions_base_url+str(tr_pk)+'/'\n",
    "\n",
    "def get_models_url():\n",
    "  return root_url+'/api/models/'\n",
    "\n",
    "def get_specific_model_url(model_pk):\n",
    "  return get_models_url()+str(model_pk)+'/'\n",
    "\n",
    "#def get_projects_url():\n",
    "#  return root_url+'/api/projects/'\n",
    "\n",
    "#def get_linetypes_url():\n",
    "#  return root_url+'/api/types/lines/'\n",
    "\n",
    "#def get_regiontypes_url():\n",
    "#  return root_url+'/api/types/regions/'\n",
    "\n",
    "#def get_annotationtypes_url():\n",
    "#  return root_url+'/api/types/annotations/'\n",
    "\n",
    "#def get_annotationtypes_url():\n",
    "#  return root_url+'/api/types/part/'\n",
    "\n",
    "\n",
    "def get_segmentation_url(doc_pk):\n",
    "  return get_specific_doc_url(doc_pk)+'segment/'\n",
    "\n",
    "def get_transcribe_url(doc_pk):\n",
    "  return get_specific_doc_url(doc_pk)+'transcribe/'\n",
    "\n",
    "\n",
    "\n",
    "  # search-and-sort-----------------------------------------------------------------------\n",
    "def search(pk, list_of_dictionaries):\n",
    "    result_list=[element for element in list_of_dictionaries if element['pk'] == pk]\n",
    "    if len(result_list)==1:\n",
    "      return result_list  [0]\n",
    "    elif len(result_list)==0:\n",
    "      return None\n",
    "    else:\n",
    "      return result_list\n",
    "\n",
    "def search_any(key, value, list_of_dictionaries):\n",
    "    return [element for element in list_of_dictionaries if element[key] == value]\n",
    "\n",
    "def filter_dict_list(key,key_val_list,list_of_dictionaries):\n",
    "  if (isinstance(key_val_list,str)) or (not(isinstance(key_val_list,list))):\n",
    "    return search_any(key, key_val_list, list_of_dictionaries)\n",
    "  else:\n",
    "    return [row for row in list_of_dictionaries if row.get(key) in key_val_list]\n",
    "\n",
    "def find_substr_in_str(str,substr):\n",
    "  return [m for m in re.finditer(substr, str)]\n",
    "\n",
    "def replace_substr_in_str(str,substr,replacement):\n",
    "  return re.sub(substr, replacement, str)\n",
    "\n",
    "def get_idx(pk, list_of_dictionaries):\n",
    "  result_list=[n for n,element in enumerate(list_of_dictionaries) if element['pk'] == pk]\n",
    "  if len(result_list)==1:\n",
    "    return result_list[0]\n",
    "  elif len(result_list)==0:\n",
    "    return None\n",
    "  else:\n",
    "    return result_list\n",
    "\n",
    "def check_token_contains(key,chars,list_of_dictionaries):\n",
    "  return [n for n,element in enumerate(list_of_dictionaries) if any((c in chars) for c in element[key])]\n",
    "\n",
    "def find_string_in_page_transcription(doc_pk,part_pk,tr_level,query_string):\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "    transcription_txt = get_sthg_from_dict_list(transcriptions,'content')\n",
    "    page_transcription = ' '.join(transcription_txt)\n",
    "    return find_substr_in_str(page_transcription,query_string)\n",
    "\n",
    "\n",
    "#-------------maths-----------------------------------------------------------------------------------------\n",
    "\n",
    "def get_line_beg_point(line,direction='RTL'):\n",
    "  if direction == 'RTL':\n",
    "    return line['baseline'][-1]\n",
    "  else:\n",
    "    return line['baseline'][0]\n",
    "def get_line_end_point(line,direction='RTL'):\n",
    "  if direction == 'RTL':\n",
    "    return line['baseline'][0]\n",
    "  else:\n",
    "    return line['baseline'][-1]\n",
    "def get_line_mid_point(line):\n",
    "  baseline = line['baseline']\n",
    "  line_length=get_baseline_length(baseline)\n",
    "  nu_segments = len(baseline)-1\n",
    "  segment_length_list=list()\n",
    "  p0 = baseline[0]\n",
    "  for p1 in baseline[1:]:\n",
    "    segment = [p0,p1]\n",
    "    segment_length_list.append(get_baseline_length(segment))\n",
    "    p0 = p1\n",
    "  way2go = line_length\n",
    "  i=-1\n",
    "  while way2go > line_length/2:\n",
    "    i += 1\n",
    "    way2go -= segment_length_list[i]\n",
    "  angle = get_angle([baseline[i],baseline[i+1]])\n",
    "  h,w = get_vector(angle,-way2go)\n",
    "  return [baseline[i][0]-h,baseline[i][1]-w]\n",
    "\n",
    "def get_line_beg_angle(line,direction='RTL'):\n",
    "  p1 = get_line_beg_point(line,direction)\n",
    "  if direction == 'RTL':\n",
    "    p2 = line['baseline'][-2]\n",
    "  else:\n",
    "    p2 = line['baseline'][1]\n",
    "  return get_angle([p1,p2])\n",
    "def get_line_end_angle(line,direction='RTL'):\n",
    "  p1 = get_line_end_point(line,direction)\n",
    "  if direction == 'RTL':\n",
    "    p2 = line['baseline'][1]\n",
    "  else:\n",
    "    p2 = line['baseline'][-2]\n",
    "  return get_angle([p2,p1])\n",
    "def get_baseline_length(baseline):\n",
    "  total_length=0\n",
    "  x0,y0 = baseline[0]\n",
    "  for x1,y1 in baseline[1:]:\n",
    "    total_length += dist2points(x0,y0,x1,y1)\n",
    "    x0 = x1\n",
    "    y0 = y1\n",
    "  return round(total_length,1)\n",
    "def get_mean_line_height(line):\n",
    "  mask=line['mask']\n",
    "  line_area=Polygon(mask).area\n",
    "  line_length=get_baseline_length(line['baseline'])\n",
    "  if line_length>0:\n",
    "    return round(line_area/line_length,1)\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def get_average_line_distance(lines):\n",
    "  nu_lines = len(lines)\n",
    "  if nu_lines>1:\n",
    "    distmapsize = (nu_lines-1,nu_lines-1)\n",
    "    distmap = numpy.ones(distmapsize,dtype = int)*1000000\n",
    "    for i1,line1 in enumerate(lines):\n",
    "      linestr1 = LineString(line1['baseline'])\n",
    "      for i2,line2 in enumerate(lines[i1+1:]):\n",
    "        linestr2 = LineString(line2['baseline'])\n",
    "        distmap[i1-1][i1+i2] = int(round(linestr1.distance(linestr2),0))\n",
    "    row_ind, col_ind = linear_sum_assignment(distmap)\n",
    "    dist_sum=0\n",
    "    for r,c in zip(row_ind,col_ind):\n",
    "      dist_sum+=distmap[r][c]\n",
    "    return int(round(dist_sum/nu_lines,0))\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def dist2points(x1,y1,x2,y2):\n",
    "  dist=sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "  return dist\n",
    "\n",
    "def get_centroid(line):\n",
    "  return [int(round((line[0][0]+line[-1][0])/2,0)),int(round((line[0][1]+line[-1][1])/2,0))]\n",
    "\n",
    "def get_angle(line):\n",
    "    return rad2deg(arctan2(line[-1][-1] - line[0][-1], line[-1][0] - line[0][0]))\n",
    "\n",
    "def get_vector(angle,length):\n",
    "  #from numpy import rad2deg,deg2rad,cos,sin\n",
    "  #return rad2deg(arctan2(line[-1][-1] - line[0][-1], line[-1][0] - line[0][0]))\n",
    "  #length=hypothenuse\n",
    "  #ankathete=x\n",
    "  #gegenkathete=y\n",
    "  y=int(round(sin(deg2rad(angle))*length,0))\n",
    "  x=int(round(cos(deg2rad(angle))*length,0))\n",
    "  return x,y\n",
    "\n",
    "def rotateNP(p, origin=(0, 0), degrees=0):\n",
    "    angle = deg2rad(degrees)\n",
    "    R = array([[cos(angle), -sin(angle)],\n",
    "                  [sin(angle),  cos(angle)]])\n",
    "    o = atleast_2d(origin)\n",
    "    p = atleast_2d(p)\n",
    "    return squeeze((R @ (p.T-o.T) + o.T).T)\n",
    "\n",
    "def line_intersection(line1, line2):\n",
    "    xdiff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])\n",
    "    ydiff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1])\n",
    "\n",
    "    def det(a, b):\n",
    "        return a[0] * b[1] - a[1] * b[0]\n",
    "\n",
    "    div = det(xdiff, ydiff)\n",
    "    if div == 0:\n",
    "        print('lines do not intersect')\n",
    "        x = line1[0][0]\n",
    "        y = 100000\n",
    "    else:\n",
    "        d = (det(*line1), det(*line2))\n",
    "        x = det(d, xdiff) / div\n",
    "        y = det(d, ydiff) / div\n",
    "    return x, y\n",
    "\n",
    "def float2int(x):\n",
    "  return int(round(x,0))\n",
    "\n",
    "def str2int(x):\n",
    "  return int(round(float(x),0))\n",
    "\n",
    "def list_of_str_points2int(list_of_points):\n",
    "  upper_int = []\n",
    "  for p in list_of_points:\n",
    "    p_int = list()\n",
    "    for c in p:\n",
    "      p_int.append(str2int(c))\n",
    "    upper_int.append(p_int)\n",
    "  return upper_int\n",
    "\n",
    "def plot_polygon(Polygon):\n",
    "  matplot(*Polygon.exterior.xy)\n",
    "\n",
    "# check_validity--------------------------------------------------------------------------------\n",
    "def check_point_in_image(p,imgHeight,imgWidth,safetydistance):\n",
    "    # deal with x:\n",
    "    p[0]=min(imgWidth-safetydistance,max(p[0],safetydistance))\n",
    "    p[1]=min(imgHeight-safetydistance,max(p[1],safetydistance))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LHhTd_vwlwlD"
   },
   "outputs": [],
   "source": [
    "#@title basic\n",
    "import json\n",
    "import requests\n",
    "from numpy import rad2deg,deg2rad,array,atleast_2d,cos,sin,arctan2,squeeze,cross,linalg\n",
    "from shapely import LineString\n",
    "from shapely.ops import split as poly_split\n",
    "\n",
    "\n",
    "#----------------get-info------------------------------\n",
    "def get_serverinfo(servername,serverconnections):\n",
    "  serverconnection=search_any('servername', servername, serverconnections)[0]\n",
    "  print('switching to ',servername)\n",
    "  headers = serverconnection['headers']\n",
    "  headersbrief = serverconnection['headersbrief']\n",
    "  root_url = serverconnection['root_url']\n",
    "  return root_url,headers,headersbrief\n",
    "\n",
    "\n",
    "def get_basic_info(doc_pk):\n",
    "  '''\n",
    "  input: document ID\n",
    "  output:\n",
    "  nu of parts\n",
    "  transcription_level_list (list of different transcription levels)\n",
    "  region_type_list, line_type_list (segmentation ontologies)\n",
    "  '''\n",
    "  print('get document segmentation ontology for document: ',doc_pk)\n",
    "  doc_url = get_documents_url()+str(doc_pk)+'/'\n",
    "  print(doc_url)\n",
    "  document_dict=requests.get(url=doc_url,headers=headers).json()\n",
    "  nu_parts = document_dict['parts_count']\n",
    "  print('Document:', doc_pk,' with ',nu_parts,' parts')\n",
    "  region_type_list=document_dict['valid_block_types']\n",
    "  print('region types:',region_type_list)\n",
    "  line_type_list=document_dict['valid_line_types']\n",
    "  print('line types:',line_type_list)\n",
    "  transcription_level_list=document_dict['transcriptions']\n",
    "  print('transcription_level_list:',transcription_level_list)\n",
    "  return nu_parts,transcription_level_list,region_type_list,line_type_list\n",
    "\n",
    "def get_margin_and_paratext_type_pks(doc_pk):\n",
    "    region_type_list,line_type_list=get_document_segmentation_ontology(doc_pk)\n",
    "    for region_type in region_type_list:\n",
    "        if region_type['name']=='Margin':\n",
    "            margin_type_pk=region_type['pk']\n",
    "        elif region_type['name']=='Paratext':\n",
    "            paratext_type_pk=region_type['pk']\n",
    "    return margin_type_pk,paratext_type_pk\n",
    "\n",
    "def get_default_line_type_pks(doc_pk):\n",
    "  region_types,line_types = get_document_segmentation_ontology(doc_pk)\n",
    "  default_line_types = search_any('name','default',line_types)\n",
    "  default_line_type_pks = [line_type['pk'] for line_type in default_line_types]\n",
    "  return default_line_type_pks\n",
    "\n",
    "#-get-items----------------------------------------------------------------------\n",
    "def get_item(url):\n",
    "  return(requests.get(url,headers=headers).json())\n",
    "\n",
    "def get_all_docs():\n",
    "  docs_url = get_documents_url()\n",
    "  return loop_through_itempages_and_get_all(docs_url)\n",
    "\n",
    "def get_doc(doc_pk):\n",
    "  doc_url = get_specific_doc_url(doc_pk)\n",
    "  return(get_item(doc_url))\n",
    "\n",
    "def get_all_projects():\n",
    "  projects_url = get_projects_url()\n",
    "  return loop_through_itempages_and_get_all(projects_url)\n",
    "\n",
    "def get_projects_url():\n",
    "  return root_url+'/api/projects/'\n",
    "\n",
    "def get_part(doc_pk,part_pk):\n",
    "  part_url = get_specific_part_url(doc_pk,part_pk)\n",
    "  return(get_item(part_url))\n",
    "\n",
    "def get_region(doc_pk,part_pk,region_pk):\n",
    "  region_url = get_specific_region_url(doc_pk,part_pk,region_pk)\n",
    "  return(get_item(region_url))\n",
    "\n",
    "def get_line(doc_pk,part_pk,line_pk):\n",
    "  line_url = get_specific_line_url(doc_pk,part_pk,line_pk)\n",
    "  return(get_item(line_url))\n",
    "\n",
    "def get_all_metadata_of_part(doc_pk,part_pk):\n",
    "  metadata_url = get_metadata_url(doc_pk,part_pk)\n",
    "  return(loop_through_itempages_and_get_all(metadata_url))\n",
    "\n",
    "def get_all_parts(doc_pk):\n",
    "    parts_url = get_parts_url(doc_pk)\n",
    "    return(loop_through_itempages_and_get_all(parts_url))\n",
    "\n",
    "def get_all_regions_of_part(doc_pk,part_pk):\n",
    "    regions_url = get_regions_url(doc_pk,part_pk)\n",
    "    return(loop_through_itempages_and_get_all(regions_url))\n",
    "\n",
    "def get_all_lines_of_part(doc_pk,part_pk):\n",
    "    lines_url = get_lines_url(doc_pk,part_pk)\n",
    "    return(loop_through_itempages_and_get_all(lines_url))\n",
    "\n",
    "def get_page_height_width(doc_pk,part_pk):\n",
    "  part_dict = get_part(doc_pk,part_pk)\n",
    "  pagewidth=part_dict['image']['size'][0]\n",
    "  pageheight=part_dict['image']['size'][1]\n",
    "  return pageheight,pagewidth\n",
    "\n",
    "def get_img_from_url(img_url):\n",
    "  img = imread(img_url)\n",
    "  return img\n",
    "\n",
    "def get_part_img(doc_pk,part):\n",
    "  imgname = part['filename']\n",
    "  img_url = root_url+'/media/documents/'+str(doc_pk)+'/'+imgname\n",
    "  #print(img_url)\n",
    "  img = get_img_from_url(img_url)\n",
    "  return img\n",
    "\n",
    "def write_doc_list_from_instance2file(fname='results.tsv'):\n",
    "  docs = get_all_docs()\n",
    "  f = open(fname,mode='wt')\n",
    "  for doc in docs:\n",
    "    doc_pk=doc['pk']\n",
    "    doc_name=doc['name']\n",
    "    doc_img_nu=doc['parts_count']\n",
    "    f.write(str(doc_pk)+'\\t'+doc_name+'\\t'+str(doc_img_nu)+'\\n')\n",
    "  f.close()\n",
    "\n",
    "def get_part_pk_list(doc_pk):\n",
    "    parts = get_all_parts(doc_pk)\n",
    "    pks = get_pks_of_dict_list(parts)\n",
    "    return pks\n",
    "\n",
    "def get_line_pk_list(doc_pk,part_pk):\n",
    "    lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    pks = get_pks_of_dict_list(lines)\n",
    "    return pks\n",
    "#def get_linetranscription_pk_list(doc_pk, part_pk,tr_pk):\n",
    "#    tr_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'\n",
    "#    return(loop_through_itempages(tr_url,'pk','results'))\n",
    "\n",
    "def get_region_pk_list(doc_pk,part_pk):\n",
    "    regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "    pks = get_pks_of_dict_list(regions)\n",
    "    return pks\n",
    "\n",
    "def get_inhabited_region_pk_list(doc_pk,part_pk):\n",
    "    all_lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    inhabited_regions = list(set([line['region'] for line in all_lines]))\n",
    "    return(inhabited_regions)\n",
    "\n",
    "def get_region_pk_list_of_type(doc_pk,part_pk,region_type):\n",
    "    regions_url = get_regions_url(doc_pk,part_pk)\n",
    "    all_regions=get_all_regions_of_part(doc_pk,part_pk)\n",
    "    regions=[region['pk'] for region in all_regions if region['typology'] == region_type]\n",
    "    return regions\n",
    "\n",
    "def get_regions_and_lines(doc_pk,part_pk):\n",
    "  lines_url=get_lines_url(doc_pk,part_pk)\n",
    "  lines=loop_through_itempages_and_get_all(lines_url)\n",
    "  regions_url=get_regions_url(doc_pk,part_pk)\n",
    "  regions=loop_through_itempages_and_get_all(regions_url)\n",
    "  return regions,lines\n",
    "\n",
    "def get_regions_and_lines_and_transcriptions(doc_pk,part_pk,tr_level):\n",
    "  lines_url=get_lines_url(doc_pk,part_pk)\n",
    "  lines=loop_through_itempages_and_get_all(lines_url)\n",
    "  regions_url=get_regions_url(doc_pk,part_pk)\n",
    "  regions=loop_through_itempages_and_get_all(regions_url)\n",
    "  tr_url = get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "  tr_suffix = 'transcription='+str(tr_level)\n",
    "  transcriptions=loop_through_itempages_and_get_all(tr_url,tr_suffix)\n",
    "  return regions,lines,transcriptions\n",
    "\n",
    "def get_main_region_pk_list(doc_pk,part_pk):\n",
    "    regions = get_region_pk_list_of_type(doc_pk,part_pk,2)\n",
    "    return regions\n",
    "\n",
    "def get_region_box_list(doc_pk,part_pk):\n",
    "    regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "    return(get_sthg_from_dict_list(regions,'box'))\n",
    "\n",
    "def get_all_page_transcriptions(doc_pk,part_pk):\n",
    "    tr_url = get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "    return(loop_through_itempages_and_get_all(tr_url))\n",
    "\n",
    "def get_page_transcription(doc_pk,part_pk,tr_level):\n",
    "    tr_url = get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "    trs =  loop_through_itempages_and_get_all(tr_url)\n",
    "    return [tr for tr in trs if tr['transcription']==tr_level]\n",
    "\n",
    "def get_transcription_level_pk_by_name(doc_pk,tr_level_name):\n",
    "    tr_levels = get_transcription_levels(doc_pk)\n",
    "    tr_levels = search_any('name',tr_level_name,tr_levels)\n",
    "    if not(tr_levels == []):\n",
    "      return tr_levels[0]['pk']\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "def get_page_transcription_tr_pks(doc_pk,part_pk,tr_level):\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "    return(get_sthg_from_dict_list(transcriptions,'pk'))\n",
    "\n",
    "def get_page_transcription_line_pks(doc_pk,part_pk,tr_level):\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "    return(get_sthg_from_dict_list(transcriptions,'line'))\n",
    "\n",
    "def get_all_lines_of_region(doc_pk,part_pk,region_pk):\n",
    "    lines_url = get_lines_url(doc_pk,part_pk)\n",
    "    all_lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    lines = [line for line in all_lines if line['region'] == region_pk]\n",
    "    return lines\n",
    "\n",
    "def get_all_line_pks_of_region(doc_pk,part_pk,region_pk):\n",
    "    lines_url = get_lines_url(doc_pk,part_pk)\n",
    "    all_lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    line_pks = [line['pk'] for line in all_lines if line['region'] == region_pk]\n",
    "    return line_pks\n",
    "\n",
    "def get_document_segmentation_ontology(doc_pk,print_status=False):\n",
    "    doc_url = get_specific_doc_url(doc_pk)\n",
    "    document_dict=get_item(doc_url)\n",
    "    region_type_list=document_dict['valid_block_types']\n",
    "    line_type_list=document_dict['valid_line_types']\n",
    "    if print_status:\n",
    "      print('get document segmentation ontology for document: ',doc_pk)\n",
    "      print(document_dict)\n",
    "      for region_type in region_type_list:\n",
    "          print(region_type)\n",
    "      for line_type in line_type_list:\n",
    "          print(line_type)\n",
    "    return region_type_list,line_type_list\n",
    "\n",
    "def get_transcription_levels(doc_pk):\n",
    "    tr_level_url = get_doc_transcriptions_url(doc_pk)\n",
    "    tr_levels = get_item(tr_level_url)\n",
    "    return tr_levels\n",
    "\n",
    "def get_specific_tr_level_pk(doc_pk,tr_level_name):\n",
    "  all_tr_levels = get_transcription_levels(doc_pk)\n",
    "  tr_levels = search_any('name',tr_level_name,all_tr_levels)\n",
    "  if not(tr_levels) == None:\n",
    "    if len(tr_levels)==1:\n",
    "      return tr_levels[0]['pk']\n",
    "    else:\n",
    "      return [tr_level['pk'] for tr_level in tr_levels]\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def get_set_of_all_characters_in_transcription_level(doc_pk,part_list,tr_level):\n",
    "    print('get char set of document for document: ',doc_pk)\n",
    "    chars = set()\n",
    "    for n,part_pk in enumerate(part_list):\n",
    "        if n % 100 == 0:\n",
    "            print(n,' parts finished')\n",
    "        transcriptions_this_part=get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "        for line in transcriptions_this_part:\n",
    "            chars=chars.union(set(line['content']))\n",
    "    chars=list(chars)\n",
    "    chars.sort()\n",
    "    return chars\n",
    "\n",
    "def get_imgname_list(doc_pk):\n",
    "    parts = get_all_parts(doc_pk)\n",
    "    return(get_sthg_from_dict_list(parts,'filename'))\n",
    "\n",
    "def get_all_models():\n",
    "  models_url = get_models_url()\n",
    "  models = loop_through_itempages_and_get_all(models_url)\n",
    "  simplified_models = [{'pk':model['pk'],'name':model['name'],'file':model['file'],'job':model['job'],'owner':model['owner'],'training':model['training']} for model in models]\n",
    "  unique_models =  [dict(t) for t in {tuple(d.items()) for d in simplified_models}]\n",
    "  return(unique_models)\n",
    "\n",
    "def get_specific_model(model_pk):\n",
    "  model_url = get_specific_model_url(model_pk)\n",
    "  return(get_item(model_url))\n",
    "\n",
    "def get_all_seg_models():\n",
    "  all_models = get_all_models()\n",
    "  return [model for model in all_models if model['job'] == 'Segment']\n",
    "\n",
    "def get_all_trans_models():\n",
    "  all_models = get_all_models()\n",
    "  return [model for model in all_models if model['job'] == 'Recognize']\n",
    "\n",
    "\"\"\"\n",
    "def get_doc_annotation_taxonomies(doc_pk):\n",
    "  tax_url = get_specific_doc_url(doc_pk)+'taxonomies/annotations/'\n",
    "  return loop_through_itempages_and_get_all(tax_url)\n",
    "\"\"\"\n",
    "def get_doc_annotation_components(doc_pk):\n",
    "  comp_url = get_specific_doc_url(doc_pk)+'taxonomies/components/'\n",
    "  return loop_through_itempages_and_get_all(comp_url)\n",
    "\n",
    "def get_doc_txt_annotation_taxonomies(doc_pk):\n",
    "  text_tax_url = get_specific_doc_url(doc_pk)+'taxonomies/annotations/text/'\n",
    "  return loop_through_itempages_and_get_all(text_tax_url)\n",
    "\n",
    "def get_doc_img_annotation_taxonomies(doc_pk):\n",
    "  img_tax_url = get_specific_doc_url(doc_pk)+'taxonomies/annotations/image/'\n",
    "  return loop_through_itempages_and_get_all(img_tax_url)\n",
    "\n",
    "def get_part_txt_annotations(doc_pk,part_pk):\n",
    "  text_tax_url = get_specific_part_url(doc_pk,part_pk)+'annotations/text/'\n",
    "  return loop_through_itempages_and_get_all(text_tax_url)\n",
    "\n",
    "def get_part_img_annotations(doc_pk,part_pk):\n",
    "  img_tax_url = get_specific_part_url(doc_pk,part_pk)+'annotations/image/'\n",
    "  return loop_through_itempages_and_get_all(img_tax_url)\n",
    "\n",
    "def get_specific_part_txt_annotations(doc_pk,part_pk,annot_pk):\n",
    "  text_tax_url = get_specific_part_url(doc_pk,part_pk)+'annotations/text/'+str(annot_pk)+'/'\n",
    "  return get_item(text_tax_url)\n",
    "\n",
    "def get_specific_part_img_annotations(doc_pk,part_pk,annot_pk):\n",
    "  img_tax_url = get_specific_part_url(doc_pk,part_pk)+'annotations/image/'+str(annot_pk)+'/'\n",
    "  return get_item(img_tax_url)\n",
    "\n",
    "def get_all_doc_txt_annotations(doc_pk):\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  all_annots = list()\n",
    "  for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    all_annots += get_part_txt_annotations(doc_pk,part_pk)\n",
    "  return all_annots\n",
    "\n",
    "def get_all_doc_img_annotations(doc_pk):\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  all_annots = list()\n",
    "  for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    all_annots += get_part_img_annotations(doc_pk,part_pk)\n",
    "  return all_annots\n",
    "\n",
    "def get_all_doc_annotations(doc_pk):\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  all_annots = list()\n",
    "  for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    all_annots += get_part_txt_annotations(doc_pk,part_pk)\n",
    "    all_annots += get_part_img_annotations(doc_pk,part_pk)\n",
    "  return all_annots\n",
    "\n",
    "def get_all_img_names(doc_pk):\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  return [(part['pk'], part['filename']) for part in parts]\n",
    "\n",
    "#---------------create-things-------------------------------------------------------------------------------\n",
    "\n",
    "def create_new_document(doc_name,project,script_pk=\"Hebrew\",read_direction=\"rtl\",line_offset=1, show_conf = False):\n",
    "    docs_url = get_documents_url()\n",
    "    data=json.dumps({\"name\": doc_name,\"main_script\": script_pk,\n",
    "                     \"project\": project,\n",
    "                     \"read_direction\":read_direction,\n",
    "                     \"line_offset\": line_offset,\n",
    "                     \"show_confidence_viz\": show_conf,\n",
    "                     \"valid_block_types\": [{\"pk\": 3,\"name\": \"Commentary\"},\n",
    "                     {\"pk\": 4,\"name\": \"Illustration\"},{\"pk\": 2,\"name\": \"Main\"},\n",
    "                     {\"pk\": 1,\"name\": \"Title\"}]})\n",
    "    res = requests.post(docs_url,headers=headers,data=data)\n",
    "    print(res.status_code, res.content)\n",
    "    jsonResponse=json.loads(res.content.decode('utf-8'))\n",
    "    doc_pk=jsonResponse['pk']\n",
    "    return doc_pk\n",
    "\n",
    "def create_transcription_levels(doc_pk,tr_name):\n",
    "    doc_tr_url=get_doc_transcriptions_url(doc_pk)\n",
    "    transcription_level_list = requests.get(doc_tr_url, headers=headers).json()\n",
    "    print('existing transcription levels: ',transcription_level_list)\n",
    "    existing_tr_level_names = [tr_level['name'] for tr_level in transcription_level_list]\n",
    "    if tr_name in existing_tr_level_names:\n",
    "      print(tr_name,'already exists')\n",
    "      break_this_code\n",
    "    else:\n",
    "      datajson={'name':tr_name}\n",
    "      print(datajson)\n",
    "      print(doc_tr_url)\n",
    "      data=json.dumps(datajson) #add check whether tr_level exists\n",
    "      res=requests.post(doc_tr_url,headers=headers,data=data)\n",
    "      print(res)\n",
    "      jsonResponse=json.loads(res.content.decode('utf-8'))\n",
    "      tr_pk=jsonResponse['pk']\n",
    "      return tr_pk\n",
    "\n",
    "\n",
    "def create_part(doc_pk,dirname,fname,part_name_in_UI = '',comments=None):\n",
    "  # part_name is what will appear in the UI left to the image filename. If left empty it is given automatically such as \"Element 1\" according to the position of the part in the document.\n",
    "  # e.g. pk = create_part(2299,r'/content','38774_jpg_000068_C.jpg','trial3','some_comment')\n",
    "  parts_url = get_parts_url(doc_pk)\n",
    "#mydir=(r\"A:\\openITI\\testset\")\n",
    "#myfile='15.png'\n",
    "  file = os.path.join(dirname, fname)\n",
    "  data = dict()\n",
    "  with open(file, 'rb') as fh:\n",
    "    if not(part_name_in_UI == ''):\n",
    "      data['name'] = part_name_in_UI\n",
    "    if not(comments == None):\n",
    "      data['comments'] = comments\n",
    "    res = requests.post(parts_url,data=data, files={'image': fh},headers=headersbrief)\n",
    "  print(res.status_code, res.content)\n",
    "  jsonResponse=json.loads(res.content.decode('utf-8'))\n",
    "  part_pk=jsonResponse['pk']\n",
    "  return part_pk\n",
    "\n",
    "def create_part_metadata(doc_pk,part_pk,eScr_metadata):\n",
    "  metadata_url = get_metadata_url(doc_pk,part_pk)\n",
    "  r = requests.post(url=metadata_url,headers=headers,data=json.dumps(eScr_metadata))\n",
    "  return r\n",
    "\n",
    "def create_region(doc_pk,part_pk,reg_polygon,regiontype_pk = None):\n",
    "  region_url = get_regions_url(doc_pk,part_pk)\n",
    "  data = {'document_part':part_pk,'box':reg_polygon,'typology':regiontype_pk}\n",
    "  r = requests.post(url=region_url,headers=headers,data=json.dumps(data))\n",
    "  return r\n",
    "\n",
    "def create_line(doc_pk,part_pk,baseline,mask = None, repolygonize = False, linetype_pk = None):\n",
    "  line_url = get_lines_url(doc_pk,part_pk)\n",
    "  data = {'document_part':part_pk,'baseline':baseline,'mask' : mask, 'typology':linetype_pk}\n",
    "  r = requests.post(url=line_url,headers=headers,data=json.dumps(data))\n",
    "  line_pk = r.json().get('pk')\n",
    "  if repolygonize:\n",
    "    repolygonize_line(doc_pk,part_pk,line_pk)\n",
    "  return r\n",
    "\n",
    "def bulk_create_lines(doc_pk,part_pk,lines):\n",
    "  bulk_create_url = get_lines_url(doc_pk,part_pk)+'bulk_create/'\n",
    "  data = {'lines' : lines}\n",
    "  r = requests.post(url=bulk_create_url,headers=headers,data=json.dumps(data))\n",
    "  return r\n",
    "\n",
    "def bulk_create_transcriptions(doc_pk,part_pk,trs):\n",
    "  bulk_create_url = get_part_transcriptions_base_url(doc_pk,part_pk)+'bulk_create/'\n",
    "  data = {'lines' : trs}\n",
    "  r = requests.post(url=bulk_create_url,headers=headers,data=json.dumps(data))\n",
    "  return r\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_txt_annotation(doc_pk,part_pk,tr_level_pk,start_line_pk,start_offset,end_line_pk,end_offset,annot_type,comments=''):\n",
    "  url = get_specific_part_url+'annotations/text/'\n",
    "  # integrate comments\n",
    "  data = {'taxonomy':annot_type,'transcription': tr_level_pk,'components':[],'start_line':start_line_pk,'start_offset':start_offset,'end_line':end_line_pk,'end_offset':end_offset}\n",
    "  r = requests.post(url,headers=headers,data = json.dumps(data))\n",
    "  return r\n",
    "\"\"\"\n",
    "\n",
    "def merge_lines(doc_pk,part_pk,lines):\n",
    "  merge_url = get_lines_url(doc_pk,part_pk)+'merge/'\n",
    "  # data should look like: json.dumps({'lines':[pk1,pk2,pk3]})\n",
    "  r = requests.post(merge_url,lines,headers = headers)\n",
    "  return r\n",
    "\n",
    "#-DELETE items-------------------------------------------------------------------\n",
    "def delete_item(delete_url):\n",
    "  r = requests.delete(delete_url,headers=headers)\n",
    "  return r\n",
    "\n",
    "def delete_part(doc_pk,part_pk):\n",
    "  delete_url = get_specific_part_url(doc_pk,part_pk)\n",
    "  r = delete_item(delete_url)\n",
    "  return r\n",
    "\n",
    "def delete_region(doc_pk,part_pk,region_pk):\n",
    "  delete_url = get_specific_region_url(doc_pk,part_pk,region_pk)\n",
    "  r = delete_item(delete_url)\n",
    "  return r\n",
    "\n",
    "def delete_all_regions_of_part(doc_pk,part_pk):\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  for region in regions:\n",
    "    delete_region(part_pk,region['pk'])\n",
    "\n",
    "def delete_line(doc_pk,part_pk,line_pk):\n",
    "  delete_url = get_specific_line_url(doc_pk,part_pk,line_pk)\n",
    "  r = delete_item(delete_url)\n",
    "  return r\n",
    "\n",
    "def bulk_delete_lines(doc_pk,part_pk,line_pk_list):\n",
    "  delete_url = get_lines_url(doc_pk,part_pk)+'bulk_delete/'\n",
    "  bulk_json=json.dumps({'lines': line_pk_list})\n",
    "  r=requests.post(delete_url,headers=headers,data=bulk_json)\n",
    "  return r\n",
    "\n",
    "def delete_all_lines_of_part(doc_pk,part_pk):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  del_lines = [line['pk'] for line in lines]\n",
    "  r = bulk_delete_lines(doc_pk,part_pk,del_lines)\n",
    "  return r\n",
    "\n",
    "def delete_all_lines_of_linetype(doc_pk,part_pk,line_type):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  del_line_pk_list = [line['pk'] for line in lines if line['typology']==line_type]\n",
    "  r = bulk_delete_lines(doc_pk,part_pk,del_line_pk_list)\n",
    "  return r\n",
    "\n",
    "def delete_all_lines_in_region(doc_pk,part_pk,region_pk):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  del_line_pk_list = [line['pk'] for line in lines if line['typology']==region_pk]\n",
    "  r = bulk_delete_lines(doc_pk,part_pk,del_line_pk_list)\n",
    "  return r\n",
    "\n",
    "def delete_all_lines_in_region_types(doc_pk,part_pk,region_type_pk_list):\n",
    "  if isinstance(region_type_pk_list,int):\n",
    "    region_type_pk_list = [region_type_pk_list]\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  del_line_pk_list = [line['pk'] for line in lines if line['region'] in [region['pk'] for region in regions if region['typology'] in region_type_pk_list]]\n",
    "  r = bulk_delete_lines(doc_pk,part_pk,del_line_pk_list)\n",
    "  return r\n",
    "\n",
    "def delete_all_transcriptions_of_tr_levels_in_linetypes(doc_pk,part_pk,tr_level_pk_list,line_type_pk_list):\n",
    "  if isinstance(line_type_pk_list,int):\n",
    "    line_type_pk_list = [line_type_pk_list]\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  transcriptions = delete_all_part_transcriptions(doc_pk,part_pk)\n",
    "  del_transcriptions = [tr for tr in transcriptions if (tr['transcription '] in tr_level_pk_list) and (tr['line'] in [line['pk'] for line in lines if line['typology'] in line_type_pk_list])]\n",
    "  r = bulk_delete_transcriptions(doc_pk,part_pk,del_transcriptions)\n",
    "  return r\n",
    "\n",
    "def delete_all_transcriptions_of_tr_levels_in_regiontypes(doc_pk,part_pk,tr_level_pk_list,region_type_pk_list):\n",
    "  if isinstance(region_type_pk_list,int):\n",
    "    region_type_pk_list = [region_type_pk_list]\n",
    "  if isinstance(tr_level_pk_list,int):\n",
    "    tr_level_pk_list = [tr_level_pk_list]\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  transcriptions = delete_all_part_transcriptions(doc_pk,part_pk)\n",
    "  del_regions = [region['pk'] for region in regions if region['typology'] in region_type_pk_list]\n",
    "  del_lines = [line['pk'] for line in lines if line['region'] in del_regions]\n",
    "  del_transcriptions = [tr for tr in transcriptions if (tr['transcription '] in tr_level_pk_list) and (tr['line'] in del_lines)]\n",
    "  r = bulk_delete_transcriptions(doc_pk,part_pk,del_transcriptions)\n",
    "  return r\n",
    "\n",
    "def bulk_delete_transcriptions(doc_pk,part_pk,transcription_pk_list):\n",
    "  delete_url = get_part_transcriptions_base_url(doc_pk,part_pk)+'bulk_delete/'\n",
    "  bulk_json=json.dumps({'lines': transcription_pk_list})\n",
    "  r=requests.post(delete_url,headers=headers,data=bulk_json)\n",
    "  return r\n",
    "\n",
    "def delete_part_transcription(doc_pk,part_pk,tr_level):\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  tr_pks = [tr['pk'] for tr in transcriptions]\n",
    "  r = bulk_delete_transcriptions(doc_pk,part_pk,tr_pks)\n",
    "  return r\n",
    "\n",
    "def delete_all_part_transcriptions(doc_pk,part_pk):\n",
    "  transcriptions = get_all_page_transcriptions(doc_pk,part_pk)\n",
    "  tr_pks = [tr['pk'] for tr in transcriptions]\n",
    "  r = bulk_delete_transcriptions(doc_pk,part_pk,tr_pks)\n",
    "  return r\n",
    "\n",
    "def delete_all_part_regions_lines_transcriptions(doc_pk,part_pk):\n",
    "  delete_all_part_transcriptions(doc_pk,part_pk)\n",
    "  delete_all_lines_of_part(doc_pk,part_pk)\n",
    "  delete_all_regions_of_part(doc_pk,part_pk)\n",
    "\n",
    "\n",
    "\n",
    "# modify-basic-data-------------------------------------------------------------------------\n",
    "\n",
    "def rename_tr_level(doc_pk,tr_level,new_name):\n",
    "  tr_level_url=get_doc_transcriptions_url(doc_pk)+str(tr_level)+'/'\n",
    "  rjson=get_item(tr_level_url)\n",
    "  rjson['name']=new_name\n",
    "  r=requests.put(tr_level_url,headers=headers,data=json.dumps(rjson))\n",
    "  return r\n",
    "\n",
    "def update_item(update_url, item):\n",
    "  r=requests.put(update_url,headers=headers,data=json.dumps(item))\n",
    "  return r\n",
    "\n",
    "def update_part(doc_pk,part_pk,part):\n",
    "  update_url = get_specific_part_url(doc_pk,part['pk'])\n",
    "  r = update_item(update_url,part)\n",
    "  return r\n",
    "\n",
    "def update_region(doc_pk,part_pk,region):\n",
    "  update_url = get_specific_region_url(doc_pk,part_pk,region['pk'])\n",
    "  r = update_item(update_url, region)\n",
    "  return r\n",
    "\n",
    "def update_line(doc_pk,part_pk,line):\n",
    "  update_url = get_specific_line_url(doc_pk,part_pk,line['pk'])\n",
    "  r = update_item(update_url, line)\n",
    "  return r\n",
    "\n",
    "def bulk_update_lines(doc_pk,part_pk,lines):\n",
    "  bulk_update_url = get_lines_url(doc_pk,part_pk)+'bulk_update/'\n",
    "  bulk_json=json.dumps({'lines': lines})\n",
    "  res_bulk=requests.put(bulk_update_url,headers=headers,data=bulk_json)\n",
    "  return res_bulk\n",
    "\n",
    "def bulk_update_transcriptions(doc_pk,part_pk,transcriptions):\n",
    "  bulk_update_url = get_part_transcriptions_base_url(doc_pk,part_pk)+'bulk_update/'\n",
    "  bulk_json=json.dumps({'lines': transcriptions}) # or should it say lines????\n",
    "  res_bulk=requests.put(bulk_update_url,headers=headers,data=bulk_json)\n",
    "  return res_bulk\n",
    "\n",
    "def modify_part_name(doc_pk,part_pk,part_name_in_UI):\n",
    "  part_url = get_specific_part_url(doc_pk,part_pk)\n",
    "  data={'name':part_name_in_UI}\n",
    "  res = requests.patch(part_url,data=json.dumps(data), headers=headers)\n",
    "  return res.status_code\n",
    "\n",
    "def add_comments2part(doc_pk,part_pk,comments,overwrite = False):\n",
    "  #e.g. sc = add_comments2part(2299,455702,comments='comment4',overwrite = False)\n",
    "  part_url = get_specific_part_url(doc_pk,part_pk)\n",
    "  part = get_item(part_url)\n",
    "  if overwrite:\n",
    "    data={'comments':comments}\n",
    "  else:\n",
    "    data={'comments':part['comments'] + comments}\n",
    "  res = requests.patch(part_url,data=json.dumps(data), headers=headers)\n",
    "  return res.status_code\n",
    "\n",
    "# function-endpoints------------------------------------------------------------------------------\n",
    "\n",
    "def import_xml(doc_pk,dirname,fname,tr_level_name,override = False):\n",
    "  \"\"\"\n",
    "  e.g. import_xml(3221,r'/content/','export_doc3221_trial_alto_202302231124.zip','AT')\n",
    "  \"\"\"\n",
    "  data={'name':tr_level_name,'document':doc_pk, 'task':'import-xml'}\n",
    "  if override:\n",
    "    data['override'] = True\n",
    "  file = os.path.join(dirname, fname)\n",
    "  import_url=get_specific_doc_url(doc_pk)+'imports/'\n",
    "  with open(file,'rb') as fh:\n",
    "    file_handler={'upload_file':fh}\n",
    "    res=requests.post(import_url,headers=headersbrief,data=data,files=file_handler)\n",
    "    return res\n",
    "\n",
    "\n",
    "def export_xml(doc_pk,part_pk_list,tr_level_pk,region_type_pk_list,include_undefined = True, include_orphan = True, file_format = 'alto',include_images = False, print_status = True):\n",
    "  export_url = get_specific_doc_url(doc_pk)+'export/' # e.g. https://escriptorium.openiti.org/api/documents/3221/export/\n",
    "  if include_undefined:\n",
    "    region_type_pk_list += 'Undefined'\n",
    "  if include_orphan:\n",
    "    region_type_pk_list += 'Orphan'\n",
    "  data = {'parts': part_pk_list, 'transcription': tr_level_pk,'task': 'export','region_types':region_type_pk_list,'include_images':include_images,'file_format':file_format}\n",
    "  #e.g. {\"parts\": [755434], \"transcription\": 5631, \"task\": \"export\", \"region_types\": [2,'Undefined','Orphan'], \"include_images\" : False, \"file_format\": \"alto\"}\n",
    "  res = requests.post(export_url,data=data,headers=headersbrief)\n",
    "  if print_status:\n",
    "    print(res.status_code)\n",
    "    if round(res.status_code/100,0) != 2:\n",
    "      print(res.content)\n",
    "  return res\n",
    "\n",
    "def segment(doc_pk,segmodel_pk,part_pk_list,steps = 'both',override = True,print_status = True):\n",
    "  if not(steps in ['lines','both','regions','masks']):\n",
    "    print('steps are not valid')\n",
    "    juststop\n",
    "  segment_url = get_segmentation_url(doc_pk)\n",
    "  data = {'parts':part_pk_list,'model':segmodel_pk,'steps':steps,'override':override}\n",
    "  r = requests.post(segment_url,data = json.dumps(data),headers = headers)\n",
    "  if print_status:\n",
    "    print(r.status_code)\n",
    "  return r\n",
    "\n",
    "def transcribe(doc_pk,trans_model_pk,part_pk_list,tr_level,print_status = False):\n",
    "  trans_url = get_transcribe_url(doc_pk)\n",
    "  # '.../api/documents/{DOC_PK}/transcribe/'\n",
    "  data = {'parts':part_pk_list,'model':trans_model_pk,'transcription':tr_level}\n",
    "  #{parts: [1, 2], model: 1} {headers only with authorization}\n",
    "  r = requests.post(trans_url,data = json.dumps(data),headers = headers)\n",
    "  if print_status:\n",
    "    print(r.status_code)\n",
    "  return r\n",
    "\n",
    "def txt2img_alignment(doc_pk,part_pk_list,trans_model_pk,tr_level):\n",
    "  doc_url = get_documents_url()\n",
    "  align_url = doc_url+str(doc_pk)+'/forced_align/'\n",
    "  if part_pk_list=='':\n",
    "    jsondata = {'model': trans_model_pk, 'transcription' : tr_level}\n",
    "  else:\n",
    "    jsondata = {'parts': part_pk_list, 'model': trans_model_pk, 'transcription' : tr_level}\n",
    "  r = requests.post(url=align_url,headers=headers,data=json.dumps(jsondata))\n",
    "  return r\n",
    "\n",
    "def move_part(doc_pk,part_pk,place):\n",
    "    print('move part_pk '+str(part_pk)+'to place '+str(place))\n",
    "    url=get_specific_part_url(doc_pk,part_pk)+'move/'\n",
    "    data='{\"index\":'+str(place)+'}'\n",
    "    res=requests.post(url,headers=headers,data=data)\n",
    "    return res\n",
    "\n",
    "def crop_part_img(doc_pk,part_pk,top,left,bottom,right,print_status = False):\n",
    "  data = json.dumps({'x1':left, 'y1':top, 'x2' : right, 'y2':bottom})\n",
    "  crop_url=get_specific_part_url(doc_pk,part_pk)+'crop/'\n",
    "  r = requests.post(url = crop_url,headers = headers,data = data)\n",
    "  if print_status:\n",
    "    print(r.status_code)\n",
    "  return r\n",
    "\n",
    "def rotate_img(doc_pk,part_pk,angle,print_status = False):\n",
    "  rotate_url=get_specific_part_url(doc_pk,part_pk)+'rotate/'\n",
    "  data={'angle':angle}\n",
    "  res=requests.post(url=rotate_url,headers=headers,data=json.dumps(data))\n",
    "  if print_status:\n",
    "    print(res.status_code)\n",
    "  return res\n",
    "\n",
    "def repolygonize(doc_pk,part_pk, print_status= False):\n",
    "    print('repolygonize : ',part_pk)\n",
    "    url_repoly=get_specific_part_url(doc_pk,part_pk)+'reset_masks/'\n",
    "    res=requests.post(url_repoly,headers=headers)\n",
    "    if print_status:\n",
    "      print(res.status_code)\n",
    "    return res\n",
    "\n",
    "def repolygonize_line(doc_pk,part_pk,line_pk):\n",
    "    print('repolygonize line : ',line_pk)\n",
    "    url_repoly=get_specific_part_url(doc_pk,part_pk)+'reset_masks/?only='+str(line_pk)\n",
    "    r=requests.post(url_repoly,headers=headers)\n",
    "    return r\n",
    "\n",
    "def reorder(doc_pk,part_pk,print_status = False):\n",
    "    print('reorder ',part_pk)\n",
    "    reorder_url=get_specific_part_url(doc_pk,part_pk)+'recalculate_ordering/'\n",
    "    res = requests.post(reorder_url,headers=headers)\n",
    "    if print_status:\n",
    "      print(res.status_code)\n",
    "    return res\n",
    "\n",
    "def train(doc_pk,part_pk_list,model_name,tr_level_pk,on_top_model_pk,override = False):\n",
    "  train_url = get_specific_doc_url(doc_url)+'train/'\n",
    "  data = {'parts': part_pk_list,'model': on_top_model_pk, 'model_name': model_name,'transcription': tr_level_pk,'override': override  }\n",
    "  r=requests.post(url= train_url,data=json.dumps(data),headers=headers)\n",
    "  return r\n",
    "\n",
    "def segtrain(doc_pk,part_pk_list,model_name,on_top_model_pk,override = False):\n",
    "  segtrain_url = get_specific_doc_url(doc_url)+'segtrain/'\n",
    "  data = {'parts': part_pk_list,'model': on_top_model_pk, 'model_name': model_name,'override': override  }\n",
    "  r=requests.post(url= segtrain_url,data=json.dumps(data),headers=headers)\n",
    "  return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CbryrfBIYuN0"
   },
   "outputs": [],
   "source": [
    "#@title complex\n",
    "#%%writefile complex_api_functions.py\n",
    "\n",
    "# find_and_replace_text_or_trim, transfer_txt2level, get_pages_without_transcription_level, get_pages_with_wordstring_in_transcription_level,\n",
    "# delete_linefree_regions, keep_only_biggest_region, associate_lines_with_existing_regions, delete_unlinked_lines, delete_1p_lines,\n",
    "# find_lines_without_mask, extend_lines, create_transcription_table, delete_empty_lines\n",
    "\n",
    "def associate_line_with_existing_region(doc_pk,part_pk,line):\n",
    "  r = None\n",
    "  centroidx,centroidy=get_line_mid_point(line)\n",
    "  p = Point(centroidx, centroidy)\n",
    "\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  for region in regions:\n",
    "    region['poly'] = Polygon(region['box'])\n",
    "    if p.within(region['poly']):\n",
    "      line['region']=region['pk']\n",
    "      r = update_line(doc_pk,part_pk,line)\n",
    "      break\n",
    "  return r\n",
    "\n",
    "def calculate_average_line_distance(lines,pageheight,pagewidth):\n",
    "\n",
    "  average_line_height_list=[]\n",
    "  midpointList = [get_line_mid_point(line) for line in lines]\n",
    "  print(midpointList)\n",
    "  lineBegList = [get_line_beg_point(line) for line in lines]\n",
    "  lineEndList = [get_line_end_point(line) for line in lines]\n",
    "  d_list=[]\n",
    "  for n,midpoint in enumerate(midpointList):\n",
    "    if n>0:\n",
    "      p1=array(lineBegList[n-1])\n",
    "      p2=array(lineEndList[n-1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append(-cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
    "    elif len(midpointList)>1:\n",
    "      p1=array(lineBegList[n+1])\n",
    "      p2=array(lineEndList[n+1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append((cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
    "    average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
    "    average_line_height_list.append(average_line_height)\n",
    "    print(average_line_height)\n",
    "  if average_line_height_list:\n",
    "    return int(round(mean(average_line_height_list)))\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "def get_average_line_distance(lines):\n",
    "  nu_lines = len(lines)\n",
    "  if nu_lines>1:\n",
    "    distmapsize = (nu_lines-1,nu_lines-1)\n",
    "    distmap = numpy.ones(distmapsize,dtype = int)*1000000\n",
    "    for i1,line1 in enumerate(lines):\n",
    "      linestr1 = LineString(line1['baseline'])\n",
    "      for i2,line2 in enumerate(lines[i1+1:]):\n",
    "        linestr2 = LineString(line2['baseline'])\n",
    "        distmap[i1-1][i1+i2] = int(round(linestr1.distance(linestr2),0))\n",
    "    row_ind, col_ind = linear_sum_assignment(distmap)\n",
    "    dist_sum=0\n",
    "    for r,c in zip(row_ind,col_ind):\n",
    "      dist_sum+=distmap[r][c]\n",
    "    return int(round(dist_sum/nu_lines,0))\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "\n",
    "def simplify_line_polygons_on_part(doc_pk,part_pk,simplification=10):\n",
    "  lines_url=get_lines_url(doc_pk,part_pk)\n",
    "  lines=loop_through_itempages_and_get_all(lines_url)\n",
    "  for line in lines:\n",
    "    mask=line['mask']\n",
    "    #print(len(mask),mask)\n",
    "    maskPoly=Polygon(mask)\n",
    "    maskSimple=maskPoly.simplify(10)\n",
    "    #print(maskSimple)\n",
    "    x,y = maskSimple.exterior.coords.xy\n",
    "    new_mask = [list(x) for x in zip(x,y)]\n",
    "    #print(len(new_mask),new_mask)\n",
    "    line_url = lines_url+str(line['pk'])+'/'\n",
    "    line_json={'mask' : new_mask}\n",
    "    res_line = requests.patch(line_url,headers=headers,data=json.dumps(line_json))\n",
    "\n",
    "def calculate_CER(doc_pk,part_pk,master_tr,CER_tr,get_lines):\n",
    "  master_trs = get_page_transcription(doc_pk,part_pk,master_tr)\n",
    "  CER_trs = get_page_transcription(doc_pk,part_pk,CER_tr)\n",
    "\n",
    "  cumul_dist = 0\n",
    "  cumul_len = 0\n",
    "  comparison_table =[]\n",
    "  for mtr in master_trs:\n",
    "    ctr = search_any('line',mtr['line'],CER_trs)\n",
    "    mtr_line_len = len(mtr['content'])\n",
    "    included = False\n",
    "    lev_dist = None\n",
    "    ctr_content = None\n",
    "    ctr_pk = None\n",
    "    if len(ctr)==1:\n",
    "        ctr_content = ctr[0]['content']\n",
    "        ctr_pk = ctr[0]['pk']\n",
    "    if (mtr_line_len>0) and (len(ctr)==1):\n",
    "      included = True\n",
    "      lev_dist = Levenshtein.distance(mtr['content'],ctr_content)\n",
    "      cumul_dist += lev_dist\n",
    "      cumul_len += mtr_line_len\n",
    "    if get_lines:\n",
    "      comparison_table.append({'line_pk':mtr['line'],\n",
    "                               'mtr_pk':mtr['pk'],\n",
    "                               'mtr_content':mtr['content'],\n",
    "                               'ctr_pk':ctr_pk,\n",
    "                               'ctr_content':ctr_content,\n",
    "                               'levenshtein_dist':lev_dist,\n",
    "                               'mtr_line_len':mtr_line_len,\n",
    "                               'included':included})\n",
    "  if get_lines:\n",
    "    for ctr in CER_trs:\n",
    "      mtr_cands = search_any('line_pk',ctr['line'],comparison_table)\n",
    "      if len(ctr)==0:\n",
    "        comparison_table.append({'line_pk':mtr['line'],\n",
    "                                'mtr_pk':None,\n",
    "                                'mtr_content':None,\n",
    "                                'ctr_pk':ctr['pk'],\n",
    "                                'ctr_content':ctr['content'],\n",
    "                                'levenshtein_dist':lev_dist,\n",
    "                                'mtr_line_len':None,\n",
    "                                'included':included})\n",
    "    return cumul_dist,cumul_len,comparison_table\n",
    "  else:\n",
    "    return cumul_dist,cumul_len\n",
    "\n",
    "def find_and_replace_chars(doc_pk,part_pk,tr_level,chars,replacement_char,do_replace):\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  if do_replace:\n",
    "    print('replacing '+chars+' with '+replacement_char)\n",
    "    for m,line_tr in enumerate(transcriptions):\n",
    "      oldstr=line_tr['content']\n",
    "      if (any((c in chars) for c in oldstr)):\n",
    "        content=line_tr['content']\n",
    "        content=content.replace(chars,replacement_char)\n",
    "        if not(oldstr==content):\n",
    "          line_tr['content']=content\n",
    "          tr_url = get_specific_transcription_url(doc_pk,part_pk,line_tr['pk'])\n",
    "          res_patch=requests.patch(tr_url,headers=headers,data=json.dumps(line_tr['content']))\n",
    "    return ''\n",
    "  else:\n",
    "    print('find pages with '+chars)\n",
    "    chars=set(chars)\n",
    "    for m,line_tr in enumerate(transcriptions):\n",
    "      linestr=line_tr['content']\n",
    "      if any((c in chars) for c in linestr):\n",
    "        print('have a look for character in ',part_pk,' line :',m,)\n",
    "    return part_pk\n",
    "\n",
    "def replace_string_or_trim(doc_pk,part_pk,tr_level,search_str,replacement_str,do_replace,do_trim=False):\n",
    "  if do_replace:\n",
    "    print('replacing '+search_str+' with '+replacement_str)\n",
    "  if do_trim:\n",
    "    print('trimming')\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  for line_tr in transcriptions:\n",
    "    oldstr = line_tr['content']\n",
    "    if do_trim:\n",
    "      line_tr['content'] = line_tr['content'].strip()\n",
    "    if do_replace:\n",
    "      line_tr['content'] = replace_substr_in_str(line_tr['content'],search_str,replacement_str)\n",
    "    if not(oldstr == tr['content']):\n",
    "      tr_url = get_specific_transcription_url(doc_pk,part_pk,line_tr['pk'])\n",
    "      res_patch=requests.patch(tr_url,headers=headers,data=json.dumps(line_tr['content']))\n",
    "\n",
    "def transfer_txt2level(doc_pk,part_pk,source_tr_level,target_tr_level,overwrite):\n",
    "  target_tr_url=get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "  source_transcriptions = get_page_transcription(doc_pk,part_pk,source_tr_level)\n",
    "  target_transcriptions = get_page_transcription(doc_pk,part_pk,target_tr_level)\n",
    "  target_tr_line_pks = get_sthg_from_dict_list(target_transcriptions,'line') # create list of existing target transcription line pks.\n",
    "  post_transcriptions = []\n",
    "  for line_tr in source_transcriptions:\n",
    "    line_pk = line_tr['line']\n",
    "    data = {'line': line_pk, 'transcription': target_tr_level, 'content': line_tr['content']}\n",
    "    if line_pk in target_tr_line_pks: # check if exists\n",
    "      if overwrite:\n",
    "        updated_data = search_any('line',line_pk,target_transcriptions)[0]\n",
    "        updated_data['content'] = data['content']\n",
    "        post_transcriptions.append(updated_data)\n",
    "    else:\n",
    "      post_transcriptions.append(data)\n",
    "  bulk_update_transcriptions(doc_pk,part_pk,post_transcriptions)\n",
    "\n",
    "def get_lines_without_transcription_level(doc_pk,part_pk,tr_level,also_empty_lines=False):\n",
    "  print('check pages without transcription in transcription level for pk', part_pk)\n",
    "  line_pks = set(get_line_pk_list(doc_pk,part_pk))\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  if also_empty_lines:\n",
    "    transcriptions = [tr for tr in transcriptions if len(tr['content'].strip())>0]\n",
    "  tr_line_pks = set(get_sthg_from_dict_list(transcriptions,'line')) # create set of existing transcription line pks.\n",
    "  missing=line_pks.difference(tr_line_pks)\n",
    "  return missing\n",
    "\n",
    "def find_or_delete_linefree_regions(doc_pk,part_pk,do_delete=False):\n",
    "  inhabited_region_list=set(get_inhabited_region_pk_list(doc_pk,part_pk))\n",
    "  region_list=set(get_region_pk_list(doc_pk,part_pk))\n",
    "  uninhabited_region_list=region_list.difference(inhabited_region_list)\n",
    "  for region in uninhabited_region_list:\n",
    "    if do_delete:\n",
    "      delete_region(doc_pk,part_pk,region)\n",
    "      print('found and deleted linefree region:',region)\n",
    "    else:\n",
    "      print('found linefree region:',region)\n",
    "  if not(do_delete):\n",
    "    return uninhabited_region_list\n",
    "\n",
    "\n",
    "def keep_only_n_biggest_regions(doc_pk,part_pk,n):\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  for region in regions:\n",
    "    region['area'] = Polygon(region['box']).area\n",
    "  sorted_regions = sorted(regions,key=lambda i: i['area'], reverse = True)\n",
    "  if len(sorted_regions)>n:\n",
    "    for region in sorted_regions[n:]:\n",
    "      print('delete ',region['pk'])\n",
    "      delete_region(doc_pk,part_pk,region['pk'])\n",
    "\n",
    "def associate_lines_with_existing_regions_and_reorder(doc_pk,part_pk):\n",
    "  print(part_pk)\n",
    "  line_url=get_lines_url(doc_pk,part_pk)\n",
    "  region_url=get_regions_url(doc_pk,part_pk)\n",
    "  lines=loop_through_itempages_and_get_all(line_url)\n",
    "  regions=loop_through_itempages_and_get_all(region_url)\n",
    "  nu_regions=len(regions)\n",
    "  region_pk_list = [item['pk'] for item in regions]\n",
    "  updated_lines=list()\n",
    "  for region in regions:\n",
    "    region['poly'] = Polygon(region['box'])\n",
    "  #print('associating')\n",
    "  for n,line in enumerate(lines):\n",
    "    baseline=line['baseline']\n",
    "    centroidx,centroidy=get_centroid(baseline)\n",
    "    p = Point(centroidx, centroidy)\n",
    "    for region in regions:\n",
    "      if p.within(region['poly']) and not(line['region']==region['pk']):\n",
    "        line['region']=region['pk']\n",
    "        updated_lines.append(line)\n",
    "        break\n",
    "  if len(updated_lines)>0:\n",
    "    print('associating lines')\n",
    "    print(updated_lines)\n",
    "    bulk_update_lines(doc_pk,part_pk,updated_lines)\n",
    "    print('reordering')\n",
    "    reorder(doc_pk,part_pk)\n",
    "  else:\n",
    "    print('nothing to associate')\n",
    "\n",
    "def find_or_delete_unlinked_lines(doc_pk,part_pk,do_delete=False):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  lines2delete = list()\n",
    "  for line_n,line in enumerate(lines):\n",
    "    if line['region']==None:\n",
    "      print('found unassociated line in doc',doc_pk,', on part',part_pk,', line:',line_n)\n",
    "      if do_delete:\n",
    "        lines2delete.append(line['pk'])\n",
    "  if len(lines2delete)>0:\n",
    "    bulk_delete_lines(doc_pk,part_pk,lines2delete)\n",
    "    reorder(doc_pk,part_pk)\n",
    "\n",
    "def delete_1p_lines(doc_pk,part_pk):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  lines2delete = list()\n",
    "  for line_n,line in enumerate(lines):\n",
    "    if (len(line['baseline'])<2):\n",
    "      print('need to delete line')\n",
    "      lines2delete.append(line['pk'])\n",
    "  if len(lines2delete)>0:\n",
    "    bulk_delete_lines(doc_pk,part_pk,lines2delete)\n",
    "    reorder(doc_pk,part_pk)\n",
    "\n",
    "def find_lines_without_mask(doc_pk,part_pk):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  maskless_lines = list()\n",
    "  for line_n,line in enumerate(lines):\n",
    "    try:\n",
    "      len_mask=len(line['mask'])\n",
    "    except:\n",
    "      len_mask=0\n",
    "    if len_mask<4:\n",
    "      print('line in doc',doc_pk,'part',part_pk,'line',line_n,'with only',len_mask,'points')\n",
    "      maskless_lines.append(line)\n",
    "  return maskless_lines\n",
    "\n",
    "def extend_lines(doc_pk,part_pk,extension=15,left_also=False,baseline2topline=False,y_decrease=10,repoly=True):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  pageheight,pagewidth=page_height_width(doc_pk,part_pk)\n",
    "  for line in lines:\n",
    "    baseline=line['baseline']\n",
    "    line_id=line['pk']\n",
    "    if baseline[0][0]<baseline[-1][0]:\n",
    "      if left_also:\n",
    "        baseline[0][0]=max(5,int(round((baseline[0][0])))-extension)\n",
    "      baseline[-1][0]=min(int(round((baseline[-1][0])))+extension,pagewidth-5)\n",
    "    else:\n",
    "      if left_also:\n",
    "        baseline[0][0]=min(int(round((baseline[0][0])))+extension,pagewidth-5)\n",
    "      baseline[-1][0]=max(5,int(round((baseline[-1][0])))-extension)\n",
    "    if baseline2topline:\n",
    "      baseline=[[pt[0], max(5,pt[1]-y_decrease)] for pt in baseline]\n",
    "    line['baseline'] = baseline\n",
    "  bulk_update_lines(doc_pk,part_pk,lines)\n",
    "  if repoly:\n",
    "    repolygonize(doc_pk,part_pk)\n",
    "\n",
    "def create_transcription_table(doc_pk,part_list,tr_level):\n",
    "  pages=[]\n",
    "  regions=[]\n",
    "  line_pks=[]\n",
    "  transcription_pks=[]\n",
    "  linenumbers=[]\n",
    "  txt=[]\n",
    "  regions=[]\n",
    "  for n,part_pk in enumerate(part_list):\n",
    "\n",
    "    if n % 10 == 0:\n",
    "      print(n)\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "    lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "    regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "    line_pk_list_this_transcription = [tr.get('line') for tr in transcriptions]\n",
    "    line_pk_list_this_part = [line.get('pk') for line in lines]\n",
    "    line2region_list_this_part = [line.get('region') for line in lines]\n",
    "    region_pk_list_this_part = [region.get('pk') for region in regions]\n",
    "\n",
    "    for line in lines:\n",
    "      pages.append(n+1)\n",
    "      line_pks.append(line['pk'])\n",
    "      linenumbers.append(line['order']+1)\n",
    "      try:\n",
    "        regions.append(region_pk_list_this_part.index(line['region'])+1)\n",
    "      except:\n",
    "        regions.append(None)\n",
    "      try:\n",
    "        index=line_pk_list_this_transcription.index(line['pk'])\n",
    "        transcription_pks.append(transcriptions[index]['pk'])\n",
    "        txt.append(transcriptions[index]['content'])\n",
    "      except:\n",
    "        transcription_pks.append(None)\n",
    "        txt.append('')\n",
    "  print('done')\n",
    "\n",
    "  columntitles = ['page', 'region', 'line','line_pk', 'transcription_pk','txt']\n",
    "  data =[columntitles] + list(zip(pages, regions, linenumbers,line_pks,transcription_pks,txt))\n",
    "\n",
    "  for i, d in enumerate(data):\n",
    "    tableline = ' | '.join(str(x).ljust(4) for x in d)\n",
    "    print(tableline)\n",
    "    if i == 0:\n",
    "      print('-' * len(tableline))\n",
    "\n",
    "def delete_empty_lines(doc_pk,part_pk,tr_level,min_length=1):\n",
    "  transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  line_pks = get_pks_of_dict_list(lines)\n",
    "  lines2delete = list()\n",
    "  for tr in transcriptions:\n",
    "    if len(tr['content'])<min_length:\n",
    "      lines2delete.append(tr['line'])\n",
    "    line_pks.remove(tr['line'])\n",
    "  for line_pk in line_pks:\n",
    "    lines2delete.append(line_pk)\n",
    "  if len(lines2delete)>0:\n",
    "    bulk_delete_lines(doc_pk,part_pk,lines2delete,print_status = True)\n",
    "\n",
    "def calculate_average_line_distance(baselineCoordsList,pageheight,pagewidth):\n",
    "  average_line_height_list=[]\n",
    "  midpointList = [[(line[0][0]+line[-1][0])/2,(line[0][1]+line[-1][1])/2] for line in baselineCoordsList]\n",
    "  lineBegList = [line_intersection([[0,0],[0,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
    "  lineEndList = [line_intersection([[pagewidth,0],[pagewidth,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
    "  d_list=[]\n",
    "  for n,midpoint in enumerate(midpointList):\n",
    "    if n>0:\n",
    "      p1=array(lineBegList[n-1])\n",
    "      p2=array(lineEndList[n-1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append(cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
    "    elif len(midpointList)>1:\n",
    "      p1=array(lineBegList[n+1])\n",
    "      p2=array(lineEndList[n+1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append(-(cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
    "    average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
    "    average_line_height_list.append(average_line_height)\n",
    "  if average_line_height_list:\n",
    "    return int(round(mean(average_line_height_list)))\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "def restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,average_line_distance=0):\n",
    "\n",
    "  def repoly_extreme_line(line,direction):\n",
    "    lines_url = get_lines_url(doc_pk,part_pk)\n",
    "    # create dummy baseline above or below to limit extension of first or last line\n",
    "    phantombaseline=[[pt[0], min(pageheight-5,max(5,pt[1]+(average_line_distance+5)*direction))] for pt in line[2]]\n",
    "    phantom_data=json.dumps({'document_part':part_pk,'region':region_pk,'baseline':phantombaseline})\n",
    "    phantom_line_pk=requests.post(lines_url,headers=headers,data=phantom_data).json()['pk']\n",
    "    # repolygonize\n",
    "    res=repolygonize_line(doc_pk,part_pk,line[0])\n",
    "    print('repoly-response: ',res)\n",
    "    # delete dummy line\n",
    "    phantom_line_del=delete_line(doc_pk,part_pk,phantom_line_pk,headers=headers)\n",
    "  print(average_line_distance)\n",
    "  pageheight,pagewidth=page_height_width(doc_pk,part_pk)\n",
    "  all_lines_this_region=get_all_lines_of_region(doc_pk,part_pk,region_pk)\n",
    "# main lines only\n",
    "  main_lines_this_region =[line for line in all_lines_this_region if line['typology'==None]]\n",
    "  baselineCoordsList=[line[2] for line in main_lines_this_region]\n",
    "# calculate average line distance\n",
    "  if average_line_distance==0:\n",
    "    average_line_distance=calculate_average_line_distance(baselineCoordsList,pageheight,pagewidth)\n",
    "  if (average_line_distance>5) and (len(main_lines_this_region)>0):\n",
    "# treat first line\n",
    "    index_first = min(lines[1] for lines in main_lines_this_region)\n",
    "    line_first=[line for line in main_lines_this_region if line[1]==index_first]\n",
    "    line_first=line_first[0]\n",
    "    repoly_extreme_line(line_first,-1)\n",
    "# treat last line\n",
    "    index_last = max(lines[1] for lines in main_lines_this_region)\n",
    "    if not(index_last==index_first):\n",
    "      line_last=[line for line in main_lines_this_region if line[1]==index_last]\n",
    "      line_last=line_last[0]\n",
    "      repoly_extreme_line(line_last,1)\n",
    "      print('repolygonized extremes of part: ',part_pk)\n",
    "  else:\n",
    "    print('not able to calculate average line distance for part: ',part_pk)\n",
    "  return average_line_distance\n",
    "\n",
    "def create_normalized_polygons_around_typelist_lines(doc_pk,part_pk,regiontypelist,linetypelist,include_none_linetype=True,ascender=12,descender=24,safetydistance=5):\n",
    "  if include_none_linetype:\n",
    "    check_none=None\n",
    "  else:\n",
    "    check_none=False\n",
    "  pageheight,pagewidth=get_page_height_width(doc_pk,part_pk)\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  regions=get_all_regions_of_part(doc_pk,part_pk)\n",
    "  selected_region_pks = [region['pk'] for region in regions if region['typology'] in regiontypelist]\n",
    "  selected_lines = [line for line in lines if ((line['typology'] in linetypelist) or (line['typology']==check_none)) and line['region'] in selected_region_pks]\n",
    "  for line in selected_lines:\n",
    "    baseline = line['baseline']\n",
    "    old_pt=baseline[0]\n",
    "    for o,pt in enumerate(baseline[1:]):\n",
    "      angle=get_angle([old_pt,pt])\n",
    "      if o==0:\n",
    "        inside_above=False\n",
    "        inside_below=False\n",
    "        new_pt_above=rotateNP((old_pt[0],old_pt[1]-ascender),origin=tuple(old_pt),degrees=angle)\n",
    "        new_pt_below=rotateNP((old_pt[0],old_pt[1]+descender),origin=tuple(old_pt),degrees=angle)\n",
    "        new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "        new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "        polygon_above=[[int(coordinate) for coordinate in new_pt_above]]\n",
    "        polygon_below=[[int(coordinate) for coordinate in new_pt_below]]\n",
    "      new_pt_above=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]-ascender),origin=tuple(pt),degrees=angle)]\n",
    "      new_pt_below=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]+descender),origin=tuple(pt),degrees=angle)]\n",
    "      new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "      new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "      if o>0:\n",
    "        # if point is inside new mask and is not the last point, delete previous point, dont insert next point either but insert instead the intersection of (p-2:p-1) and (p:p+1)\n",
    "        if inside_above and not(o==len(line[1:])):\n",
    "          replace_pt=list(line_intersection([polygon_above[-3],polygon_above[-2]],[polygon_above[-1],new_pt_above]))\n",
    "          del polygon_above[-2:-1]\n",
    "          polygon_above.append(replace_pt)\n",
    "        if inside_below:\n",
    "          replace_pt=list(line_intersection([polygon_below[2],polygon_below[1]],[polygon_below[0],new_pt_below]))\n",
    "          del polygon_below[0:1]\n",
    "          polygon_below.insert(0,replace_pt)\n",
    "        pol=Polygon(polygon_above+polygon_below)\n",
    "        inside_above=pol.contains(Point(new_pt_above))\n",
    "        inside_below=pol.contains(Point(new_pt_below))\n",
    "      polygon_above.append(new_pt_above) # if point is outside add it to boundary\n",
    "      polygon_below.insert(0,new_pt_below) # if point is outside add it to boundary\n",
    "      old_pt=pt\n",
    "    if inside_above:\n",
    "      del polygon_above[-1] # if point is inside and is last point delete it\n",
    "    if inside_below:\n",
    "      del polygon_below[0] # if point is inside and is last point delete it\n",
    "    polygon_boundary=[baseline[0]]+polygon_above+[baseline[-1]]+polygon_below\n",
    "    line['mask']=polygon_boundary\n",
    "  return bulk_update_lines(doc_pk,part_pk,selected_lines)\n",
    "\n",
    "\n",
    "def simplify_hebrew(doc_nu,part_list,source_tr_level,target_tr_level):\n",
    "    vocalization=[1425,1427,1430,1436,1438,1443,1446,1453,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1471,1473,1474,1476,1477,1478,1479]\n",
    "    singlequot=[1523,8216,8217,8219,8242]\n",
    "    doublequot=[1524,8220,8221,8222,8223,8243]\n",
    "    spacechars=[9,160,8201,8195,8192]\n",
    "    deletechars=[1565,8299,8205,8300,8302]\n",
    "    toreplacechars=[(64296,1514),(64298,1513),(64299,1513),(64300,1513),(64302,1488),(64303,1488),(64305,1489),(64306,1490),(64307,1491),(64309,1493),(64315,1499),(64324,1508),(64327,1511),(64330,1514),(64331,1493),(64332,1489),(64333,1499),(64334,1508),(8277,42),(8283,46),(11799,61)]\n",
    "    for n,part in enumerate(part_list[41:]):\n",
    "        target_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'\n",
    "        #https://www.escriptorium.fr/api/documents/39/parts/14480/transcriptions/?page=2&transcription=46\n",
    "        page_nu=0\n",
    "        while True:\n",
    "            page_nu+=1\n",
    "            source_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(source_tr_level)\n",
    "            target_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(target_tr_level)\n",
    "            res = requests.get(source_tr_url, headers=headers).json()\n",
    "            print(source_tr_url)\n",
    "            res_target=requests.get(target_tr_url, headers=headers).json()\n",
    "            if res_target!={'detail': 'Invalid page.'}:\n",
    "                target_line_pk_list = [line['line'] for line in res_target['results']]\n",
    "                target_tr_pk_list = [line['pk'] for line in res_target['results']]\n",
    "                print(target_line_pk_list)\n",
    "            else:\n",
    "                target_line_pk_list=[]\n",
    "                target_tr_pk_list=[]\n",
    "            #print(res_target)\n",
    "            for m,line in enumerate(res['results']):\n",
    "                #'line': 674381, 'transcription': 46, 'content': 'מדביק מרחם שערים הליכות מצויינים',\n",
    "                line_pk=line['line']\n",
    "                print(line_pk)\n",
    "                print(line['pk'])\n",
    "                content=line['content']\n",
    "                for u in vocalization:\n",
    "                    content=content.replace(chr(u),'')\n",
    "                for u in spacechars:\n",
    "                    content=content.replace(chr(u),' ')\n",
    "                for u in deletechars:\n",
    "                    content=content.replace(chr(u),'')\n",
    "                for u in singlequot:\n",
    "                    content=content.replace(chr(u),\"'\")\n",
    "                for u in doublequot:\n",
    "                    content=content.replace(chr(u),'\"')\n",
    "                for (x,y) in toreplacechars:\n",
    "                    content=content.replace(chr(x),chr(y))\n",
    "                data = {'line': line_pk, 'transcription': target_tr_level, 'content': content}\n",
    "                if line_pk in target_line_pk_list:\n",
    "                    t_url=(root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+str(line['pk'])+'/')\n",
    "                    res_t=requests.get(t_url,headers=headers).json()\n",
    "                    print('put',res_t['content'])\n",
    "                    res_t['content']=content\n",
    "                    print(t_url)\n",
    "                    data=json.dumps(res_t, ensure_ascii=False).encode('utf8')\n",
    "                    res_put = requests.put(t_url, data=data, headers=headers)\n",
    "                    #print('put',res_put.content)\n",
    "                else:\n",
    "                    res_post = requests.post(target_tr_url, data=json.dumps(data), headers=headers)\n",
    "                    #print(n,part,m,res_post.content)\n",
    "            if not(res['next']):\n",
    "                break\n",
    "    print('done simplifying hebrew')\n",
    "\n",
    "def delete_text_in_main_regions_except_line_type(doc_nu,part_list,tr_level,allowed_line_types):\n",
    "\n",
    "    for n,part_pk in enumerate(part_list):\n",
    "        print(n,part_pk)\n",
    "        #get all main regions\n",
    "        regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "        main_region_list=[r.get('pk') for r in regions['results'] if r['typology']==2]\n",
    "\n",
    "        #get all lines of this part that are in the main columns and of the default type and delete their content\n",
    "        page_nu=0\n",
    "        del_lines_pk_list=[]\n",
    "        lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "        for line in lines:\n",
    "          if (not(line['typology'] in allowed_line_types)) and (line['region'] in main_region_list):\n",
    "            del_lines_pk_list.append(line['pk'])\n",
    "        transcriptions = get_page_transcription(doc_pk,part_pk,tr_level)\n",
    "        delete_tr_list = list()\n",
    "        for tr_line in transcriptions:\n",
    "          if tr_line['line'] in del_lines_pk_list:\n",
    "            delete_tr_list.append(tr_line['pk'])\n",
    "        bulk_delete_transcriptions(doc_pk,part_pk,delete_tr_list)\n",
    "\n",
    "def loop_document_restrict_extreme_lines_all_regions(doc_pk,fix_line_height,start_item=0):\n",
    "\n",
    "  part_pk_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_pk)\n",
    "  for part_pk in part_pk_list[start_item:]:\n",
    "    print('----------------------------------------------------')\n",
    "    print('resegmenting part :',part_pk)\n",
    "    region_list=get_region_pk_list(doc_pk,part_pk)\n",
    "    main_region_list=get_main_region_pk_list(doc_pk,part_pk)\n",
    "    average_line_height_in_main_regions=[]\n",
    "    for region_pk in main_region_list:\n",
    "      average_line_height_in_main_regions.append(restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,fix_line_height))\n",
    "    if len(main_region_list)>0:\n",
    "      global_average_line_height=mean(average_line_height_in_main_regions)\n",
    "    else:\n",
    "      global_average_line_height=fix_line_height\n",
    "    non_main_regions=set(region_list)-set(main_region_list)\n",
    "    for region_pk in non_main_regions:\n",
    "      line_height=restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,global_average_line_height)\n",
    "\n",
    "  print('done')\n",
    "\n",
    "def create_normalized_polygons_around_lines(doc_nu,splitfactor=0.6,safetydistance=5,basic_line_height=120):\n",
    "\n",
    "    part_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_nu)\n",
    "\n",
    "    for n,part in enumerate(part_list):\n",
    "        parts_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/')\n",
    "        res = requests.get(parts_url,headers=headers).json()\n",
    "        pagewidth=res['image']['size'][0]\n",
    "        pageheight=res['image']['size'][1]\n",
    "        print(parts_url)\n",
    "        ## first loop: calculate average line height and distance\n",
    "        regionList = [region for region in res['regions']]\n",
    "        average_line_height_list=[]\n",
    "        average_line_height_not_null=[]\n",
    "        for m,region in enumerate(regionList):\n",
    "            baselineCoordsList = [line['baseline'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
    "            midpointList = [[(line[0][0]+line[-1][0])/2,(line[0][1]+line[-1][1])/2] for line in baselineCoordsList]\n",
    "            lineBegList = [line_intersection([[0,0],[0,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
    "            lineEndList = [line_intersection([[pagewidth,0],[pagewidth,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
    "            d_list=[]\n",
    "            for n,midpoint in enumerate(midpointList):\n",
    "                if n>0:\n",
    "                    p1=array(lineBegList[n-1])\n",
    "                    p2=array(lineEndList[n-1])\n",
    "                    p3=array(midpoint)\n",
    "                    d_list.append(cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
    "                elif len(midpointList)>1:\n",
    "                    p1=array(lineBegList[n+1])\n",
    "                    p2=array(lineEndList[n+1])\n",
    "                    p3=array(midpoint)\n",
    "                    d_list.append(-(cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
    "            if not(d_list):\n",
    "                print('one line region')\n",
    "                average_line_height_list.append(0)\n",
    "            else:\n",
    "                average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
    "                average_line_height_list.append(average_line_height)\n",
    "                average_line_height_not_null.append(average_line_height)\n",
    "        print(average_line_height_list)\n",
    "        print(average_line_height_not_null)\n",
    "        if not(average_line_height_not_null):\n",
    "            global_average_line_height=basic_line_height\n",
    "            print('bad average line height')\n",
    "        else:\n",
    "            global_average_line_height=int(round(sum(average_line_height_not_null)/len(average_line_height_not_null),0))\n",
    "            print(global_average_line_height)\n",
    "\n",
    "        for m,region in enumerate(regionList):\n",
    "            if average_line_height_list[m]==0:\n",
    "                this_line_height=global_average_line_height\n",
    "            else:\n",
    "                this_line_height=average_line_height_list[m]\n",
    "            baselineCoordsList = [line['baseline'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
    "            baselinePKList = [line['pk'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
    "            for n,line in enumerate(baselineCoordsList):\n",
    "                old_pt=line[0]\n",
    "                for o,pt in enumerate(line[1:]):\n",
    "                    #print('region '+str(m)+', line '+str(n)+', point '+str(o)+str(old_pt)+':'+str(pt))\n",
    "                    angle=get_angle([old_pt,pt])\n",
    "                    if o==0:\n",
    "                        inside_above=False\n",
    "                        inside_below=False\n",
    "                        new_pt_above=rotateNP((old_pt[0],old_pt[1]-this_line_height*splitfactor),origin=tuple(old_pt),degrees=angle)\n",
    "                        new_pt_below=rotateNP((old_pt[0],old_pt[1]+this_line_height*(1-splitfactor)),origin=tuple(old_pt),degrees=angle)\n",
    "                        #print(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "                        new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "                        new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "                        #print(new_pt_above)\n",
    "                        polygon_above=[[int(coordinate) for coordinate in new_pt_above]]\n",
    "                        polygon_below=[[int(coordinate) for coordinate in new_pt_below]]\n",
    "                    new_pt_above=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]-this_line_height*splitfactor),origin=tuple(pt),degrees=angle)]\n",
    "                    new_pt_below=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]+this_line_height*(1-splitfactor)),origin=tuple(pt),degrees=angle)]\n",
    "                    new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "                    new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "                    if o>0:\n",
    "                        if inside_above and not(o==len(line[1:])): # if point is inside and is not the last point, # delete previous point, dont insert next point either # and insert instead the intersection of (p-2:p-1) and (p:p+1)\n",
    "                            replace_pt=list(line_intersection([polygon_above[-3],polygon_above[-2]],[polygon_above[-1],new_pt_above]))\n",
    "                            del polygon_above[-2:-1]\n",
    "                            polygon_above.append(replace_pt)\n",
    "                        if inside_below:\n",
    "                            replace_pt=list(line_intersection([polygon_below[2],polygon_below[1]],[polygon_below[0],new_pt_below]))\n",
    "                            del polygon_below[0:1]\n",
    "                            polygon_below.insert(0,replace_pt)\n",
    "                        pol=Polygon(polygon_above+polygon_below)\n",
    "                        inside_above=pol.contains(Point(new_pt_above))\n",
    "                        inside_below=pol.contains(Point(new_pt_below))\n",
    "                    polygon_above.append(new_pt_above) # if point is outside add it to boundary\n",
    "                    polygon_below.insert(0,new_pt_below) # if point is outside add it to boundary\n",
    "                    old_pt=pt\n",
    "                if inside_above:\n",
    "                    del polygon_above[-1] # if point is inside and is last point delete it\n",
    "                if inside_below:\n",
    "                    del polygon_below[0] # if point is inside and is last point delete it\n",
    "                polygon_boundary=polygon_above+polygon_below\n",
    "                line_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/lines/'+str(baselinePKList[n])+'/')\n",
    "                resline = requests.get(line_url,headers=headers).json()\n",
    "                resline['mask']=polygon_boundary\n",
    "                data=json.dumps(resline)\n",
    "                rput = requests.put(line_url,headers=headers,data=data)\n",
    "                print(m,rput)\n",
    "    print('done')\n",
    "\n",
    "def find_lines_without_baseline(doc_pk,part_pk,transcription_levels = []):\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  bad_lines = list()\n",
    "  for line in lines:\n",
    "    if not(line['baseline']==None):\n",
    "      if len(line['baseline'])<1:\n",
    "        bad_lines.append(line)\n",
    "    else:\n",
    "      bad_lines.append(line)\n",
    "  if not(transcription_levels==[]):\n",
    "    transcriptions = get_page_transcription(doc_pk,part_pk,transcription_levels)\n",
    "  return bad_lines\n",
    "\n",
    "def merge2levels_in_third(doc_pk,part_pk,level1,level2,level3):\n",
    "  lines_without_master_transcription,proportion,empty_lines=get_lines_without_transcription_level(doc_pk,part_pk,level1)\n",
    "  if proportion>0.3:\n",
    "    lines_without_level2_transcription,proportion_level2,empty_lines=get_lines_without_transcription_level(doc_pk,part_pk,level2)\n",
    "    if proportion_verso<0.3:\n",
    "      print('from level2 to master on part ',part_pk)\n",
    "      transfer_txt2level(doc_pk,part_pk,level2,level3)\n",
    "    else:\n",
    "      print('from level1 to master on part ',part_pk)\n",
    "      transfer_txt2level(doc_pk,part_pk,level1,level3)\n",
    "  else:\n",
    "    print('master exists for part ',part_pk)\n",
    "\n",
    "def change_all_regions2main(dok_pk):\n",
    "  part_pk_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_pk)\n",
    "  for n,part_pk in enumerate(part_pk_list):\n",
    "    region_list=get_region_pk_list(doc_pk,part_pk)\n",
    "    for region_pk in region_list:\n",
    "      regions_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/blocks/'+str(region_pk)+'/'\n",
    "      res=requests.get(regions_url,headers=headers).json()\n",
    "      print(n,res['typology'])\n",
    "      if not(res['typology']==2):\n",
    "        res['typology']=2\n",
    "        data=json.dumps(res)\n",
    "        res2=requests.put(regions_url,headers=headers,data=data)\n",
    "        print(regions_url)\n",
    "        print(res2)\n",
    "\n",
    "def plot_polygon(Polygon):\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  plt.plot(*Polygon.exterior.xy)\n",
    "  #plt.plot(*maskSimple.exterior.xy)\n",
    "\n",
    "def move_line_up_down(baseline,distance,down,pageheight,pagewidth):\n",
    "  #down=1\n",
    "  #up=-1\n",
    "  safety=5\n",
    "  angle=get_angle(baseline)\n",
    "  # 0=horizontal\n",
    "  if abs(angle)<45:\n",
    "    if baseline[0][0]<baseline[-1][0]:\n",
    "      vectorx,vectory=get_vector(angle+90*down,distance) #(LTR)\n",
    "    else:\n",
    "      vectorx,vectory=get_vector(angle+90*down,distance) #(RTL) (upside down)\n",
    "  else:\n",
    "    if baseline[0][1]<baseline[-1][1]:\n",
    "      vectorx,vectory=get_vector(angle+90*down,distance) #(vertical up)\n",
    "    else:\n",
    "      vectorx,vectory=get_vector(angle+90*down,distance) #(vertical down)\n",
    "  new_baseline=[[max(safety,min(pagewidth-safety,p[0]+vectorx)),max(safety,min(pageheight-safety,p[1]+vectory))] for p in baseline]\n",
    "  return(new_baseline)\n",
    "\n",
    "def extend_single_line(baseline,distance,do_left,do_right,pageheight,pagewidth):\n",
    "  angle=get_angle(baseline)\n",
    "  # 0=horizontal\n",
    "  vectorx,vectory=get_vector(angle,distance)\n",
    "  p1=baseline[0]\n",
    "  pz=baseline[-1]\n",
    "  p0=[]\n",
    "  pend=[]\n",
    "  if abs(angle)<45:\n",
    "    if baseline[0][0]<baseline[-1][0]:\n",
    "      if do_left:\n",
    "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
    "      if do_right:\n",
    "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
    "    else:\n",
    "      if do_left:\n",
    "        p0=[[max(1,min(pagewidth,p1[0]+vectorx)),max(1,min(pageheight,p1[1]+vectory))]]\n",
    "      if do_right:\n",
    "        pend=[[max(1,min(pagewidth,pz[0]-vectorx)),max(1,min(pageheight,pz[1]-vectory))]]\n",
    "  else:\n",
    "    if baseline[0][1]<baseline[-1][1]:\n",
    "      if do_left:\n",
    "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
    "      if do_right:\n",
    "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
    "    else:\n",
    "      if do_left:\n",
    "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
    "      if do_right:\n",
    "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
    "  if not(p0==[]):\n",
    "    baseline=baseline[1:]\n",
    "  if not(pend==[]):\n",
    "    baseline=baseline[:-1]\n",
    "  new_baseline=p0+baseline+pend\n",
    "  return(new_baseline)\n",
    "\n",
    "\n",
    "def move_all_lines_of_part_up_down(doc_pk,part_pk,distance=10,down=1,repoly=True,default2none=True):\n",
    "  if default2none:\n",
    "    default_line_type_pks = get_default_line_type_pks(doc_pk)\n",
    "  lines=get_all_lines_of_part(doc_pk,part_pk)\n",
    "  #[[line['pk'],line['order'],line['baseline'],line['typology'],line['region']] for line in data['results']]\n",
    "  pageheight,pagewidth=get_page_height_width(doc_pk,part_pk)\n",
    "  print(part_pk)\n",
    "  for line in lines:\n",
    "    baseline=line['baseline']\n",
    "    line_pk=line['pk']\n",
    "    line['baseline']=move_line_up_down(baseline,distance,down,pageheight,pagewidth)\n",
    "    if default2none:\n",
    "      if line['typology'] in default_line_type_pks:\n",
    "        line['typology'] = None\n",
    "  bulk_update_lines(doc_pk,part_pk,lines)\n",
    "  if repoly:\n",
    "    r = repolygonize(doc_pk,part_pk, print_status= False)\n",
    "\n",
    "\n",
    "def calculate_regions_for_unlinked_lines(doc_nu,part,margin_type,paratext_type):\n",
    "# calculate regions for unlinked lines by taking the minima and maxima of their boundaries\n",
    "    from shapely.ops import cascaded_union\n",
    "\n",
    "    parts_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/')\n",
    "    print(parts_url)\n",
    "    res = requests.get(parts_url,headers=headers).json()\n",
    "    regpoly_list=[]\n",
    "    line_links_list=[]\n",
    "\n",
    "    for line_n,lines in enumerate(res['lines']):\n",
    "        if lines['region']==None:\n",
    "            line_array=array(lines['mask'])\n",
    "            try:\n",
    "                x1 = min(line_array[:,0])\n",
    "                y1 = min(line_array[:,1])\n",
    "                x2 = max(line_array[:,0])\n",
    "                y2 = max(line_array[:,1])\n",
    "                poly = Polygon([[x1,y1],[x2,y1],[x2,y2],[x1,y2]])\n",
    "                if len(regpoly_list)==0:\n",
    "                    regpoly_list.append(poly)\n",
    "                    line_links_list=[[line_n]]\n",
    "                else:\n",
    "                    no_join_found = True\n",
    "                    for n,poly1 in enumerate(regpoly_list):\n",
    "                        if poly.intersects(poly1): #poly overlaps with poly1 :\n",
    "                    # merge: https://deparkes.co.uk/2015/02/28/how-to-merge-polygons-in-python/\n",
    "                            polygons = [poly1, poly]\n",
    "                            union_poly = cascaded_union(polygons)\n",
    "                    # replace poly1 by union_poly\n",
    "                            regpoly_list[n] = union_poly\n",
    "                            poly = union_poly\n",
    "                            line_links_list[n].append(line_n)\n",
    "                            no_join_found = False\n",
    "                    if no_join_found:\n",
    "                        regpoly_list.append(poly)\n",
    "                        line_links_list.append([line_n])\n",
    "            except:\n",
    "                print('error at:',str(part))\n",
    "    blocks_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/'\n",
    "    data=requests.get(blocks_url,headers=headers).json()\n",
    "    existing_regions=data['regions']\n",
    "    post_url = root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+ '/blocks/'\n",
    "    max_region_n=-1\n",
    "    if len(existing_regions)==0:\n",
    "        max_region_size=0\n",
    "        for region_n,region in enumerate(regpoly_list):\n",
    "            this_region_area=region.area\n",
    "            if this_region_area>max_region_size:\n",
    "                max_region_size=this_region_area\n",
    "                max_region_n=region_n\n",
    "        maincol=regpoly_list[max_region_n]\n",
    "        maincol_point_list = [[int(float(j)) for j in i] for i in list(maincol.exterior.coords)]\n",
    "        block_json={'document_part': part,'box': maincol_point_list,'typology': 2}\n",
    "        r = requests.post(post_url,headers=headers, data=json.dumps(block_json))\n",
    "        y_main_min = min(array(maincol_point_list)[:,1])\n",
    "        y_main_max = max(array(maincol_point_list)[:,1])\n",
    "    else:\n",
    "        y_main_min=10000\n",
    "        y_main_max=0\n",
    "        for region in existing_regions:\n",
    "            if len(region['box'])<3:\n",
    "                print('delete '+str(region['pk']))\n",
    "                delete_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+ '/blocks/'+str(region['pk']) + '/'\n",
    "                r = requests.delete(delete_url,headers=headers)\n",
    "                print(r.status_code)\n",
    "            else:\n",
    "                maincol=Polygon(region['box'])\n",
    "                y1 = min(array(region['box'])[:,1])\n",
    "                if y1<y_main_min:\n",
    "                    y_main_min=y1\n",
    "                y2 = max(array(region['box'])[:,1])\n",
    "                if y2>y_main_max:\n",
    "                    y_main_max=y2\n",
    "    if max_region_n>-1:\n",
    "        del regpoly_list[max_region_n] # KICK MAINCOL out of REGPOLYLIST\n",
    "    for region_n,poly in enumerate(regpoly_list):\n",
    "        poly_point_list = [[int(float(j)) for j in i] for i in list(poly.exterior.coords)]\n",
    "        y_poly = mean(array(poly_point_list)[:,1])\n",
    "        if (y_poly<y_main_min) or (y_poly>y_main_max):\n",
    "            region_type=paratext_type\n",
    "        else:\n",
    "            region_type=margin_type\n",
    "        block_json={'document_part': part,'box': poly_point_list,'typology': region_type}\n",
    "        r = requests.post(post_url,headers=headers, data=json.dumps(block_json))\n",
    "    reorder_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/recalculate_ordering/'\n",
    "    reorder_res = requests.post(reorder_url,headers=headers)\n",
    "\n",
    "def fast_merge_stuttering_lines(doc_pk, part_pk, regions_pks_list, delta_y):\n",
    "    distance_matrix_y = create_distance_matrix_y(doc_pk, part_pk, regions_pks_list)\n",
    "    lines = identify_group_of_lines_with_small_distances(doc_pk, part_pk, regions_pks_list, distance_matrix_y, delta_y)\n",
    "\n",
    "    for line in lines:\n",
    "        print(f\"lignes à fusionner:{line}\") # for debug\n",
    "        merge_lines(doc_pk,part_pk,line)\n",
    "\n",
    "    new_region_data = create_region_for_corrected_segmentation(doc_pk, part_pk, regions_pks_list)\n",
    "    link_merged_lines_to_new_region(doc_pk, part_pk, new_region_data)\n",
    "    reorder(doc_pk,part_pk,print_status = True)\n",
    "    repolygonize(doc_pk,part_pk, print_status= True)\n",
    "\n",
    "    print(\"The text no longer stutters!\")\n",
    "\n",
    "def create_distance_matrix_y(doc_pk, part_pk, regions_pks_list):\n",
    "    # Now takes as argument a list of regions region_pk = [region_pk1, region_pk2, etc. ]\n",
    "    # Merge the region line dictionary lists\n",
    "\n",
    "    lines_from_selected_regions = [] # List containing all the lines of all the regions selected in regions_pks_list\n",
    "    for region_pk in regions_pks_list:\n",
    "        lines_from_single_region = get_all_lines_of_region(doc_pk, part_pk, region_pk)\n",
    "        lines_from_selected_regions += lines_from_single_region\n",
    "\n",
    "    # Retrieve the coordinates of the midpoints of the lines in the region\n",
    "    lines = lines_from_selected_regions\n",
    "    coordinates = []\n",
    "    for line in lines:\n",
    "        line_mid_point = get_line_mid_point(line)\n",
    "        coordinates.append(line_mid_point)\n",
    "\n",
    "    # Calculating the y distance between two points\n",
    "    def distance_y(point1, point2):\n",
    "        return abs(point1[1] - point2[1])\n",
    "\n",
    "    # Construction of the matrix of interline distances projected onto the y-axis\n",
    "    distance_matrix_y = []\n",
    "    for i in range(len(coordinates)):\n",
    "        line_dist_y = [distance_y(coordinates[i], coordinates[j]) if i != j else sys.maxsize for j in range(len(coordinates))] # je reprends ici la sémantique du code d'eSc\n",
    "        distance_matrix_y.append(line_dist_y)\n",
    "    print(distance_matrix_y) # pour debug\n",
    "\n",
    "    return distance_matrix_y\n",
    "\n",
    "def link_merged_lines_to_new_region(doc_pk, part_pk, new_region_data):\n",
    "\n",
    "    # Fonction pour vérifier si un point (x, y) est à l'intérieur d'un polygone\n",
    "    def point_inside_polygon(x, y, poly):\n",
    "        n = len(poly)\n",
    "        inside = False\n",
    "        p1x, p1y = poly[0]\n",
    "        for i in range(n + 1):\n",
    "            p2x, p2y = poly[i % n]\n",
    "            if y > min(p1y, p2y):\n",
    "                if y <= max(p1y, p2y):\n",
    "                    if x <= max(p1x, p2x):\n",
    "                        if p1y != p2y:\n",
    "                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                        if p1x == p2x or x <= xinters:\n",
    "                            inside = not inside\n",
    "            p1x, p1y = p2x, p2y\n",
    "        return inside\n",
    "\n",
    "    # List of dictionaries for all lines in the part document\n",
    "    lines_list = get_all_lines_of_part(doc_pk, part_pk)\n",
    "\n",
    "    #Information about the new region \"new_region_data\n",
    "    region_pk = new_region_data['pk']\n",
    "    print(region_pk)\n",
    "    region_box = new_region_data['box']\n",
    "\n",
    "    # Initialise the line_data_update dictionary before the loop, which will contain the line data to be updated.\n",
    "    line_data_update = {}\n",
    "\n",
    "    # Browse all lines and assign the region if the mask is inside the region\n",
    "    for line in lines_list:\n",
    "        is_inside_region = all(point_inside_polygon(x, y, region_box) for x, y in line['mask'])\n",
    "        if is_inside_region:\n",
    "            line['region'] = region_pk\n",
    "            line_data_update = {'region': line['region']}\n",
    "        update_url = get_specific_line_url(doc_pk, part_pk, line['pk'])\n",
    "        r=requests.patch(update_url,headers=headers,data=json.dumps(line_data_update))\n",
    "\n",
    "\n",
    "    # Display updated lines\n",
    "    # for line in lines_list:\n",
    "    # print(line)\n",
    "\n",
    "\n",
    "def create_region_for_corrected_segmentation(doc_pk, part_pk, regions_pks_list):\n",
    "\n",
    "    # Retrieve the coordinates of the masks for the lines in each region of regions_pks_list\n",
    "    # and concatenate them.\n",
    "    # mask_coordinates = [mask_coordinates1, mask_coordinates2, ...]\n",
    "\n",
    "    lines_mask_coordinates = []\n",
    "    lines_url=get_lines_url(doc_pk,part_pk)\n",
    "    lines=loop_through_itempages_and_get_all(lines_url)\n",
    "    for line in lines:\n",
    "        if line['pk'] in regions_pks_list:\n",
    "            mask=line['mask']\n",
    "            lines_mask_coordinates.append(mask)\n",
    "\n",
    "    # Retrieve the coordinates of regions from regions_pks_list\n",
    "    regions_coordinates = []\n",
    "    all_regions_of_part = get_all_regions_of_part(doc_pk, part_pk)\n",
    "\n",
    "    for region in all_regions_of_part:\n",
    "        # if the region's region_pk is in regions_pks_list\n",
    "        # then its co-ordinates are added to the list of region co-ordinates regions_coordinates\n",
    "        if region['pk'] in regions_pks_list:\n",
    "            regions_coordinates.append(region['box'])\n",
    "    print(f\"coordonnées des régions de regions_pks_list: {regions_coordinates}\")\n",
    "\n",
    "    # Concatenate\n",
    "    poly_coordinates = regions_coordinates + lines_mask_coordinates\n",
    "\n",
    "    # Calculation of the convexHull of this zone\n",
    "\n",
    "    # Convert the nested list into a NumPy array\n",
    "    points = numpy.array([point for poly in poly_coordinates for point in poly])\n",
    "\n",
    "    # Calculation of convex hull\n",
    "    hull = ConvexHull(points)\n",
    "\n",
    "    # Obtain the indices of the vertices of the hull convex\n",
    "    convex_hull_indices = hull.vertices\n",
    "\n",
    "    # Obtain the coordinates of the convex hull points\n",
    "    convex_hull_points = points[convex_hull_indices]\n",
    "\n",
    "    # Convert the list of lists into a list of lists\n",
    "    convex_hull_points_lists = [list(point) for point in convex_hull_points]\n",
    "\n",
    "    # Passing int32 to int so that the json request can succeed.\n",
    "    convex_hull_points_lists = [[int(x), int(y)] for x, y in convex_hull_points_lists]\n",
    "\n",
    "    # Abolition of the old regions\n",
    "    for region_pk in regions_pks_list:\n",
    "        delete_region(doc_pk,part_pk,region_pk)\n",
    "\n",
    "    # Creation of the new region\n",
    "\n",
    "    region_url = get_regions_url(doc_pk,part_pk)\n",
    "    reg_polygon = convex_hull_points_lists\n",
    "    regiontype_pk = None\n",
    "\n",
    "    data = {'document_part':part_pk,'box':reg_polygon,'typology':regiontype_pk}\n",
    "    r = requests.post(url=region_url,headers=headers,data=json.dumps(data))\n",
    "    new_region_data = r.json()\n",
    "\n",
    "    return new_region_data\n",
    "\n",
    "\n",
    "# 2. Identify groups of lines whose vertical distance (y) between their midpoints is less than the delta_y interval\n",
    "# Identify and record the references of these lines in a dictionary.\n",
    "def identify_group_of_lines_with_small_distances(doc_pk, part_pk, regions_pks_list, distance_matrix_y, delta_y):\n",
    "\n",
    "    # Creation of a dictionary of the lines of the regions selected in regions_pks_list\n",
    "    lines_from_selected_regions = [] # List containing all the lines of all the regions selected in regions_pks_list\n",
    "    for region_pk in regions_pks_list:\n",
    "        lines_from_single_region = get_all_lines_of_region(doc_pk, part_pk, region_pk)\n",
    "        lines_from_selected_regions += lines_from_single_region\n",
    "\n",
    "    line_dict = lines_from_selected_regions\n",
    "\n",
    "    # Creation of the list hosting the row group references to be merged\n",
    "    # I want to format this data so that it can be easily passed to the API endpoint merge.\n",
    "    # [{'lines\": ['pk_number1', 'pk_number2', 'pk_number3']}, {'lines\": ['pk_number5', 'pk_number6']}, ... ]\n",
    "    group_of_lines_with_small_distances = []\n",
    "\n",
    "    # Identify and import groups of lines to be merged.\n",
    "    for i in range(len(distance_matrix_y)):\n",
    "        lines_to_merge = []\n",
    "        lines_to_merge.append(str(line_dict[i]['pk'])) # I import the first line i, which will be used for comparison\n",
    "\n",
    "        for j in range(i + 1, len(distance_matrix_y)): # Evaluation of the distance y between line i and all the others.\n",
    "            if distance_matrix_y[i][j] <= delta_y:     # If this distance is less than the delta_y interval, I import the line\n",
    "                lines_to_merge.append(str(line_dict[j]['pk'])) # in a \"line_to_merge\" dictionary\n",
    "        current_group = {'lines': lines_to_merge} # Creation of the group dictionary in the form {'lines': lines_to_merge}\n",
    "        group_of_lines_with_small_distances.append(current_group)\n",
    "\n",
    "    # Filter the list of dictionaries of lines to be merged, to remove solitary lines.\n",
    "    data_filtered = []\n",
    "    for entry in group_of_lines_with_small_distances:\n",
    "        if len(entry['lines']) >= 2:\n",
    "            data_filtered.append(entry)\n",
    "\n",
    "    # Delete duplicates\n",
    "    data_no_duplicates = [{'lines': list(set(d['lines']))} for d in data_filtered]\n",
    "\n",
    "    group_of_lines_with_small_distances = data_no_duplicates\n",
    "    return group_of_lines_with_small_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qvCvJredQSYj"
   },
   "outputs": [],
   "source": [
    "# @title more complex\n",
    "\n",
    "def get_delta_y_with_angle_and_x(angle,x):\n",
    "    return x * math.tan(angle * math.pi / 180)\n",
    "\n",
    "def sort_sublists_by_element(sub_li,n):\n",
    "      return(sorted(sub_li, key=lambda x: x[n]))\n",
    "\n",
    "def extend_lines2region_boundaries(doc_pk,part_pk):\n",
    "#if True:\n",
    "\n",
    "\n",
    "\n",
    "  page_height,page_width = get_page_height_width(doc_pk,part_pk)\n",
    "  regions = get_all_regions_of_part(doc_pk,part_pk)\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  new_lines=list()\n",
    "  for region in regions:\n",
    "    region['Circumference']=LineString(region['box'])\n",
    "\n",
    "  for line in lines:\n",
    "    if line['baseline'][0][0] > line['baseline'][-1][0]:\n",
    "      invert = True\n",
    "      line['baseline'].reverse()\n",
    "    else:\n",
    "      invert = False\n",
    "    this_region = search(line['region'],regions)\n",
    "    line_left_angle = get_angle([line['baseline'][0],line['baseline'][1]])\n",
    "    left_delta_y = get_delta_y_with_angle_and_x(line_left_angle,line['baseline'][0][0])\n",
    "    line_right_angle = get_angle([line['baseline'][-2],line['baseline'][-1]])\n",
    "    right_delta_y = get_delta_y_with_angle_and_x(line_right_angle,page_width-line['baseline'][-1][0])\n",
    "    new_p_left = [1,line['baseline'][0][1]+left_delta_y]\n",
    "    new_p_right = [page_width,line['baseline'][-1][1]+right_delta_y]\n",
    "    extended_line = LineString([new_p_left]+line['baseline']+[new_p_right])\n",
    "    #print()\n",
    "    #print(line['pk'],line['order'])\n",
    "    #print('beg:',line_left_angle,left_delta_y)\n",
    "    #print('end :',line_right_angle,right_delta_y)\n",
    "    #print('region boundary',this_region['Circumference'])\n",
    "    #print('extended_line:',extended_line)\n",
    "\n",
    "    intersection_points = intersection(extended_line, this_region['Circumference'])\n",
    "    new_points = sort_sublists_by_element([[int(round(coordinate[0],0)) for coordinate in p.xy] for p in intersection_points.geoms],0)\n",
    "    new_p_left = new_points[0]\n",
    "    new_p_right = new_points[-1]\n",
    "    #print('nps',new_points)\n",
    "    #print('np left',new_p_left)\n",
    "    #print('np right',new_p_right)\n",
    "    #print('old bl',line['baseline'])\n",
    "    new_baseline = [new_p_left]+line['baseline']+[new_p_right]\n",
    "    if invert:\n",
    "      new_baseline.reverse()\n",
    "    new_line = {'pk':line['pk'],'baseline':new_baseline}\n",
    "    new_lines.append(new_line)\n",
    "  bulk_update_lines(doc_pk,part_pk,new_lines)\n",
    "  repolygonize(doc_pk,part_pk)\n",
    "\n",
    "def get_ascender_descender_polys(line_x,direction = 'RTL'):\n",
    "  def poly_height(poly,baseline):\n",
    "    baseline_length = get_baseline_length(baseline)\n",
    "    if baseline_length > 0:\n",
    "      return poly.area/baseline_length\n",
    "    else:\n",
    "      return None\n",
    "  found = [False,False]\n",
    "  best_dist1 = 10000000\n",
    "  best_dist2 = 10000000\n",
    "  for i,p in enumerate(line_x['mask']):\n",
    "    this_dist1 = dist2points(line_x['baseline'][0][0],line_x['baseline'][0][1],p[0],p[1])\n",
    "    this_dist2 = dist2points(line_x['baseline'][-1][0],line_x['baseline'][-1][1],p[0],p[1])\n",
    "    if p == line_x['baseline'][0]:\n",
    "      found[0] = True\n",
    "      best_dist1 = 0\n",
    "    elif p == line_x['baseline'][-1]:\n",
    "      found[1] = True\n",
    "      best_dist = 0\n",
    "    else:\n",
    "      if this_dist1<best_dist1:\n",
    "        best_dist1 = this_dist1\n",
    "        best_i1 = i\n",
    "      elif this_dist2<best_dist2:\n",
    "        best_dist2 = this_dist2\n",
    "        best_i2 = i\n",
    "  if not(found[0]):\n",
    "    line_x['baseline'][0]=line_x['mask'][best_i1]\n",
    "  if not(found[1]):\n",
    "    line_x['baseline'][-1]=line_x['mask'][best_i2]\n",
    "  #if not(found[0]) or not(found[1]):\n",
    "    #print(found)\n",
    "    #print(line_x['mask'])\n",
    "    #print(line_x['baseline'])\n",
    "\n",
    "  polys = poly_split(Polygon(line_x['mask']), LineString(line_x['baseline']))\n",
    "  #print(polys)\n",
    "  poly1,poly2 = [poly for poly in polys.geoms]\n",
    "  xx, yy = poly1.boundary.coords.xy\n",
    "  poly1_points = [[p[0],p[1]] for p in zip(xx,yy)]\n",
    "  orientation = poly1_points[0]==line_x['baseline'][0]\n",
    "  if direction=='RTL':\n",
    "    orientation=not(orientation)\n",
    "  poly1_height = round(poly_height(poly1,line_x['baseline']),1)\n",
    "  poly2_height = round(poly_height(poly2,line_x['baseline']),1)\n",
    "  if orientation:\n",
    "    return poly1, poly2,poly1_height,poly2_height\n",
    "  else:\n",
    "    return poly2,poly1,poly2_height,poly1_height\n",
    "\n",
    "def calculate_average_line_distance(lines):\n",
    "\n",
    "  average_line_height_list=[]\n",
    "  midpointList = [get_line_mid_point(line) for line in lines]\n",
    "  #print(midpointList)\n",
    "  lineBegList = [get_line_beg_point(line) for line in lines]\n",
    "  lineEndList = [get_line_end_point(line) for line in lines]\n",
    "  d_list=[]\n",
    "  for n,midpoint in enumerate(midpointList):\n",
    "    if n>0:\n",
    "      p1=array(lineBegList[n-1])\n",
    "      p2=array(lineEndList[n-1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append(-cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
    "    elif len(midpointList)>1:\n",
    "      p1=array(lineBegList[n+1])\n",
    "      p2=array(lineEndList[n+1])\n",
    "      p3=array(midpoint)\n",
    "      d_list.append((cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
    "    average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
    "    average_line_height_list.append(average_line_height)\n",
    "    #print(average_line_height)\n",
    "  if average_line_height_list:\n",
    "    return int(round(mean(average_line_height_list)))\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "def create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    "    doc_pk,part_pk,method='fix',regiontypelist=[],linetypelist=[],\n",
    "    include_none_linetype=True,ascender=12,descender=24, safetydistance=5):\n",
    "# possible methods:\n",
    "# 'fix': fixed values for the distance above (ascender) and below (descender) the baseline.\n",
    "# 'average_line_dist': creates ascender polygons by multiplying the average line distance with the value in \"ascenders\" and analogously descender polys with \"descenders\"\n",
    "# 'average_line_height': calculates first the average current polygon height and adds the values in the ascender and descender parameters\n",
    "# proportion should be between 0 and 1\n",
    "  if not(method in ['fix','average_line_dist','average_line_height']):\n",
    "    print('unknown method type')\n",
    "    kaputt # fail\n",
    "  if include_none_linetype:\n",
    "    check_none=None\n",
    "  else:\n",
    "    check_none=False\n",
    "  pageheight,pagewidth=get_page_height_width(doc_pk,part_pk)\n",
    "  lines = get_all_lines_of_part(doc_pk,part_pk)\n",
    "  regions=get_all_regions_of_part(doc_pk,part_pk)\n",
    "  if regiontypelist == []: ## empty list means take all regions regardless of regiontype\n",
    "    regiontypelist = set([region['typology'] for region in regions])\n",
    "  if linetypelist ==[]: ## empty list means take all lines regardless of linetype\n",
    "    linetypelist = set([line['typology'] for line in lines])\n",
    "  selected_region_pks = [region['pk'] for region in regions if region['typology'] in regiontypelist]\n",
    "  selected_lines = [line for line in lines if ((line['typology'] in linetypelist) or (line['typology']==check_none)) and line['region'] in selected_region_pks]\n",
    "\n",
    "  if method == 'average_line_dist':\n",
    "    for region in regions:\n",
    "      if region['typology'] in regiontypelist:\n",
    "        region['line_dist'] = calculate_average_line_distance(selected_lines)\n",
    "        region['ascender'] = ascender * region['line_dist']\n",
    "        region['descender'] = descender * region['line_dist']\n",
    "      else:\n",
    "        region['line_dist'] = None\n",
    "        region['ascender'] = None\n",
    "        region['descender'] = None\n",
    "  elif method == 'average_line_height':\n",
    "    for region in regions:\n",
    "      if region['typology'] in regiontypelist:\n",
    "        ascenders = list()\n",
    "        descenders = list()\n",
    "        for line in selected_lines:\n",
    "          Poly1,Poly2,a,d = get_ascender_descender_polys(line)\n",
    "          ascenders.append(a)\n",
    "          descenders.append(d)\n",
    "        region['ascender'] = mean(ascenders) + ascender\n",
    "        region['descender'] = mean(descenders) + descender\n",
    "\n",
    "  for line in selected_lines:\n",
    "    baseline = line['baseline']\n",
    "    old_pt=baseline[0]\n",
    "\n",
    "    if method in ['average_line_dist','average_line_height']:\n",
    "      region_pk = line['region']\n",
    "      ascender = search(region_pk,regions)['ascender']\n",
    "      descender = search(region_pk,regions)['descender']\n",
    "    if (ascender == None) or (descender == None):\n",
    "      print('no ascender or descender value for line',line['pk'])\n",
    "      continue\n",
    "    for o,pt in enumerate(baseline[1:]):\n",
    "      angle=get_angle([old_pt,pt])\n",
    "      if o==0:\n",
    "        inside_above=False\n",
    "        inside_below=False\n",
    "        new_pt_above=rotateNP((old_pt[0],old_pt[1]-ascender),origin=tuple(old_pt),degrees=angle)\n",
    "        new_pt_below=rotateNP((old_pt[0],old_pt[1]+descender),origin=tuple(old_pt),degrees=angle)\n",
    "        new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "        new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "        polygon_above=[[int(coordinate) for coordinate in new_pt_above]]\n",
    "        polygon_below=[[int(coordinate) for coordinate in new_pt_below]]\n",
    "      new_pt_above=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]-ascender),origin=tuple(pt),degrees=angle)]\n",
    "      new_pt_below=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]+descender),origin=tuple(pt),degrees=angle)]\n",
    "      new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
    "      new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
    "      if o>0:\n",
    "        # if point is inside new mask and is not the last point, delete previous point, dont insert next point either but insert instead the intersection of (p-2:p-1) and (p:p+1)\n",
    "        if inside_above and not(o==len(line[1:])):\n",
    "          replace_pt=list(line_intersection([polygon_above[-3],polygon_above[-2]],[polygon_above[-1],new_pt_above]))\n",
    "          del polygon_above[-2:-1]\n",
    "          polygon_above.append(replace_pt)\n",
    "        if inside_below:\n",
    "          replace_pt=list(line_intersection([polygon_below[2],polygon_below[1]],[polygon_below[0],new_pt_below]))\n",
    "          del polygon_below[0:1]\n",
    "          polygon_below.insert(0,replace_pt)\n",
    "        pol=Polygon(polygon_above+polygon_below)\n",
    "        inside_above=pol.contains(Point(new_pt_above))\n",
    "        inside_below=pol.contains(Point(new_pt_below))\n",
    "      polygon_above.append(new_pt_above) # if point is outside add it to boundary\n",
    "      polygon_below.insert(0,new_pt_below) # if point is outside add it to boundary\n",
    "      old_pt=pt\n",
    "    if inside_above:\n",
    "      del polygon_above[-1] # if point is inside and is last point delete it\n",
    "    if inside_below:\n",
    "      del polygon_below[0] # if point is inside and is last point delete it\n",
    "    polygon_boundary=[baseline[0]]+polygon_above+[baseline[-1]]+polygon_below\n",
    "    line['mask']=polygon_boundary\n",
    "  return bulk_update_lines(doc_pk,part_pk,selected_lines)\n",
    "#create_normalized_line_polygons_with_fixed_ascenders_descenders_4line_and_regiontypology(\n",
    " #   doc_pk,part_pk,method='average_line_height',regiontypelist=[],linetypelist=[],\n",
    "  #  include_none_linetype=True,ascender=-30,descender=-10, safetydistance=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qnvdLgBZOCXD"
   },
   "outputs": [],
   "source": [
    "# @title normalize_existing_transcription_in_manual\n",
    "def normalize_existing_transcription_in_manual(doc_pk):\n",
    "\n",
    "  #tr_level = create_transcription_levels(doc_pk,'master')\n",
    "  part_pks = [part['pk'] for part in get_all_parts(doc_pk)]\n",
    "  manual_tr_level = get_specific_tr_level_pk(doc_pk,'manual')\n",
    "  chars = get_set_of_all_characters_in_transcription_level(doc_pk,part_pks,manual_tr_level)\n",
    "  for c in chars:\n",
    "    print(c,ord(c))\n",
    "  master_level = create_transcription_levels(doc_pk,'master')\n",
    "  print('master_level:',master_level)\n",
    "\n",
    "\n",
    "  chars2delete = '̊'\n",
    "  chars2replace = '{}<>'\n",
    "  replacementchars = '⟦⟧()'\n",
    "  replacement_tuples = [(old_c,new_c) for old_c,new_c in zip(chars2replace,replacementchars)]\n",
    "  chars_in_brackets = re.compile(\"(\\[[א-ת' :]*\\])\")\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  for part in parts:\n",
    "    part_pk = part['pk']\n",
    "    trs = get_page_transcription(doc_pk,part_pk,manual_tr_level)\n",
    "    new_trs = list()\n",
    "    for tr in trs:\n",
    "      linetxt = tr['content']\n",
    "      new_tr = dict()\n",
    "      if not(linetxt==''):\n",
    "        #print(linetxt)\n",
    "        #step 1: delete char 778 (circellus)\n",
    "        for c in chars2delete:\n",
    "          linetxt = linetxt.replace(c,'')\n",
    "        #step 2: switch all {} to ⟦⟧ !!! WE NEED A DISTINCTION BETWEEN DELETION AND LACUNA\n",
    "        #step 3: switch all <> to ()\n",
    "        for old_c,new_c in replacement_tuples:\n",
    "          linetxt = linetxt.replace(old_c,new_c)\n",
    "        #step 4: replace ':' with ' :'\n",
    "        linetxt = linetxt.replace(':',' :')\n",
    "        #step 5: delete all chars in brackets\n",
    "        linetxt = re.sub(chars_in_brackets,'[ ]',linetxt)\n",
    "        #step 6: delete opening bracket at line beginnings and end brackets and line\n",
    "        if linetxt[0:2] == '[ ' and len(linetxt)>3:\n",
    "          linetxt = linetxt[2:]\n",
    "        if linetxt[-3:-1] == '[ ' and len(linetxt)>3:\n",
    "          linetxt = linetxt[:-2]\n",
    "        new_tr['content'] = linetxt\n",
    "      else:\n",
    "        new_tr['content'] = ''\n",
    "      new_tr['line'] = tr['line']\n",
    "      new_tr['transcription'] = master_level\n",
    "      new_trs.append(new_tr)\n",
    "    r = bulk_create_transcriptions(doc_pk,part_pk,new_trs)\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RoIPx_UAGZFc"
   },
   "outputs": [],
   "source": [
    "# @title metadata_reading_writing\n",
    "\n",
    "def convert_eScr_metadata2dsbe_metadata_dict(doc_pk,part_pk,eScr_metadata_dict_list):\n",
    "  dsbe_metadata_dict_list=[]\n",
    "  for item in eScr_metadata_dict_list:\n",
    "    #print(item)\n",
    "    dsbe_metadata_dict = {'pk':item['pk'],'doc_pk':doc_pk,'part_pk':part_pk,'keyname':item['key']['name'],'cidoc_id':item['key']['cidoc_id'],'value':item['value']}\n",
    "    dsbe_metadata_dict_list.append(dsbe_metadata_dict)\n",
    "  return dsbe_metadata_dict_list\n",
    "\n",
    "def update_part_metadata(doc_pk,part_pk,eScr_metadata):\n",
    "  metadata_url = get_metadata_url(doc_pk,part_pk)\n",
    "  print(metadata_url)\n",
    "  r = update_item(metadata_url,eScr_metadata)\n",
    "  return r\n",
    "\n",
    "def update_dsbe_metadata_dict2eScr(dsbe_metadata_dict_list):\n",
    "  doc_pks=set()\n",
    "  for item in dsbe_metadata_dict_list:\n",
    "    doc_pks.add(item['doc_pk'])\n",
    "  for doc_pk in doc_pks:\n",
    "    metadata_this_doc = search_any('doc_pk',doc_pk,dsbe_metadata_dict_list)\n",
    "    part_pks=set()\n",
    "    for item in metadata_this_doc:\n",
    "      part_pks.add(item['part_pk'])\n",
    "    for part_pk in part_pks:\n",
    "      metadata_this_part=search_any('part_pk',part_pk,metadata_this_doc)\n",
    "      eScr_metadata=[]\n",
    "      for dsbe_item in metadata_this_part:\n",
    "        eScr_item = {'pk':item['pk'],'key':{'name':item['keyname'],'cidoc_id':item['cidoc_id']},'value':item['value']}\n",
    "        eScr_metadata.append(eScr_item)\n",
    "      r=update_part_metadata(doc_pk,part_pk,eScr_metadata)\n",
    "      print(r.content)\n",
    "\n",
    "def create_diverse_metadata(dsbe_metadata_dict_list):\n",
    "  for item in dsbe_metadata_dict_list:\n",
    "    doc_pk = item['doc_pk']\n",
    "    part_pk = item['part_pk']\n",
    "    eScr_item = {'key':{'name':item['keyname'],'cidoc_id':item['cidoc_id']},'value':item['value']}\n",
    "    r=create_part_metadata(doc_pk,part_pk,eScr_item)\n",
    "    print(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8q7Ln1tlXeI"
   },
   "outputs": [],
   "source": [
    "\n",
    "servername= '' # Add the instance name which you set in the first cell. It will fetch your connections credentials for your account.\n",
    "root_url,headers,headersbrief=get_serverinfo(servername,serverconnections)\n",
    "doc_url=get_documents_url()\n",
    "print(doc_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5JAzUcsoLso"
   },
   "source": [
    "This opens the same data directly as json. But you can get this directly with requests.get as we did before for sefaria, can't we? Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vO1aM7cHoKaj"
   },
   "outputs": [],
   "source": [
    "r=requests.get(url=doc_url)\n",
    "docs_dict = json.loads(r.content)\n",
    "print(docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCkMdMMD_BIa"
   },
   "source": [
    "Oops - this did not work. Why not? Your access to eScriptorium is limited to your data. Otherwise anybody could start to do anything with the data of anybody else.\n",
    "\n",
    "If you access eScriptorium, you need to login with your username and password. Both of them are encoded in your token. So we need to submit that together with our request. This is done in the `headers` parameter that we configured in the beginning. So lets retry with the correct permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK4KTJ3d_liY"
   },
   "outputs": [],
   "source": [
    "r=requests.get(url=doc_url,headers=headers)\n",
    "docs_dict = json.loads(r.content)\n",
    "print('This is the full data returned - a dictionary:')\n",
    "print(docs_dict)\n",
    "print()\n",
    "print('This is the loop through the keys of the dictionary:')\n",
    "for key in docs_dict:\n",
    "  print(key,type(docs_dict[key]),docs_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnKoSqLD8Jgv"
   },
   "source": [
    "Results is a `list`. Each of your documents is an entry in this list, so let's loop through the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jMZGfYz6rGO"
   },
   "outputs": [],
   "source": [
    "for i,doc in enumerate(docs_dict['results']):\n",
    "  print(i,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHj5yqL6IaiV"
   },
   "source": [
    "`doc_dict` contains the json you received from eScriptorium into a dictionary that you can query like any other dictionary.\n",
    "\n",
    "E.g. get a list of the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0nunmFJIoFu"
   },
   "outputs": [],
   "source": [
    "for key in docs_dict:\n",
    "  print(key,'type:',type(docs_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-lRa7_a0-Nv"
   },
   "outputs": [],
   "source": [
    "docs_dict['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXYnniBpI9RJ"
   },
   "source": [
    "There are pages like this not only for documents, but also for parts (images), regions, lines, transcriptions, the different ontology items (regiontypes, linetypes, annotations etc). They are all built in the same way.\n",
    "\n",
    "*   `count` gives the total number of elements. 41 in my case means that I have 41 documents.\n",
    "*   `next` indicates the url where you can get the next 10 elements if you have more than 10. If not, it says `None`.\n",
    "*   `previous` indicates the url where you can get the previous 10 elements if there are any. If not, it says `None`.\n",
    "*   `results` is a list with the information for each of the 10 documents stored as another dictionary. Let's look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6rOrEdMJ9-E"
   },
   "outputs": [],
   "source": [
    "doc = docs_dict['results'][0]\n",
    "get_basic_info(doc['pk'])\n",
    "print(doc)\n",
    "print()\n",
    "print('And here are the keys in the document dictionary, their variable types and contents')\n",
    "for key in doc:\n",
    "  print(key.ljust(20,' '),str(type(doc[key])).ljust(20,' '),doc[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQ7U7rYbPI9r"
   },
   "outputs": [],
   "source": [
    "doc_pk=3221\n",
    "parts=get_all_parts(doc_pk)\n",
    "for part in parts[:5]:\n",
    "  print(part)\n",
    "  associate_lines_with_existing_regions_and_reorder(doc_pk,part['pk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yhhROakZrx6"
   },
   "source": [
    "You can get information about anything concerning parts (=images). The most important for us are the pk (=primary key, i.e. the ID in the database that you need to do anything with anything related to this part, e.g. the regions, lines or transcriptions).\n",
    "\n",
    "For beginners the most important keys are:\n",
    "* pk\n",
    "* filename (the string for the image name, which you can not modify)\n",
    "* image (a dictionary containing among others the height and width in pixels)\n",
    "* order (a integer for the order in the overall document, which you can also\n",
    "modify)\n",
    "\n",
    "For more advanced users there is also:\n",
    "* name (a string for what is normally shown as \"Element N\", which you can modify to what you like, e.g. foliation : \"fol. 46b\")\n",
    "* comments (a string where you can write anything you want that will appear in the part metadata panel)\n",
    "* max_avg_confidence (a float [= a number giving the average precision for the automatic model if this feature is turned on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GB7-iB0gPyH0"
   },
   "outputs": [],
   "source": [
    "for key in part:\n",
    "  print(key,type(part[key]),part[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bukJNEiwdONJ"
   },
   "source": [
    "If you have both the pk of the document AND the pk of the part you can access the regions.\n",
    "\n",
    "There are currently 5 keys in the region dictionary\n",
    "* pk - primary key (ID)\n",
    "* document part - the pk of the part to which the regions belongs\n",
    "* external_id - a string which you can modify to give the region a readable name, e.g. fol_46b_region_001\n",
    "* order - an integer describing the order of regions on this part\n",
    "* box - a list of points describing the polygon of the region\n",
    "* typology - a pk for the ontology of regiontypes\n",
    "\n",
    "All fields except pk are writable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s036DOtrcgrs"
   },
   "outputs": [],
   "source": [
    "print(doc_pk,part['pk'])\n",
    "regions = get_all_regions_of_part(doc_pk,part['pk'])\n",
    "print('There are',len(regions),'region(s) stored in a list of dictionaries that looks as follows.')\n",
    "for region in regions:\n",
    "  print(region)\n",
    "print()\n",
    "print('The first region has the following keys, valuetypes and variable content.')\n",
    "for key in regions[0]:\n",
    "  print(key,type(regions[0][key]),regions[0][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTkVS8jWfTAd"
   },
   "source": [
    "If you have both the pk of the document AND the pk of the part you can access also the lines.\n",
    "\n",
    "There are currently 8 keys in the lines dictionary\n",
    "* pk - primary key (ID)\n",
    "* document part - the pk of the part to which the line belongs\n",
    "* external_id - a string which you can modify to give the line a readable name, e.g. fol_46b_line_001\n",
    "* order - an integer describing the order of lines on this part\n",
    "* region - the pk of the region with which this line has been linked (\"None\" if it is unlinked)\n",
    "* baseline - a list of points describing the baseline of the line\n",
    "* mask - a list of points describing the polygon around the line\n",
    "* typology - a pk for the ontology of linetypes\n",
    "\n",
    "All fields except pk are writable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fsPrwF4dMn3"
   },
   "outputs": [],
   "source": [
    "print(doc_pk,part['pk'])\n",
    "lines = get_all_lines_of_part(doc_pk,part['pk'])\n",
    "print('There are',len(lines),'lines(s) stored in a list of dictionaries that looks as follows.')\n",
    "print(len(lines))\n",
    "for line in lines:\n",
    "  print(line)\n",
    "print()\n",
    "print('The first line has the following keys, valuetypes and variable content.')\n",
    "for key in lines[0]:\n",
    "  print(key,type(lines[0][key]),lines[0][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVKJ7qEfcx_I"
   },
   "source": [
    "As there can be many transcriptions to one line, transcriptions are different from lines.\n",
    "\n",
    "You can get a list of existing transcription levels via the function \"get_transcription_levels\" with the parameter of the document primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0Ilfm6fqYjK"
   },
   "outputs": [],
   "source": [
    "tr_levels = get_transcription_levels(doc_pk)\n",
    "for tr_level in tr_levels:\n",
    "  print(tr_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wQyILd-sxcp"
   },
   "source": [
    "You can access ALL transcriptions of one \"part\" just as you accessed regions and lines. However, this will give you ALL transcriptions, i.e. all transcriptions of all transcription levels if you have several transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFhS875jcxYD"
   },
   "outputs": [],
   "source": [
    "part_pk=part['pk']\n",
    "tr_url = get_part_transcriptions_base_url(doc_pk,part_pk)\n",
    "transcriptions=get_all_page_transcriptions(doc_pk,part_pk)\n",
    "print('a list of all transcriptions:')\n",
    "for transcription in transcriptions:\n",
    "  print(transcription)\n",
    "\n",
    "print()\n",
    "print(' a list of keys in one transcription dictionary')\n",
    "for key in transcriptions[0]:\n",
    "  print(key,type(transcriptions[0][key]),transcriptions[0][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHjsTU0to0ha"
   },
   "source": [
    "Of the 9 keys in the dictionary, four are important for beginners:\n",
    "\n",
    "* pk [an integer for the ID of the line-transcription]\n",
    "* line [an integer for the ID of the line to which this transcription belongs, i.e. the geometrical data with baseline, mask-polygon etc]\n",
    "* transcription [an integer for the ID of the transcription LEVEL, i.e. \"manual\" or some automatically created transcription. The list of transcription levels can be queried at:\n",
    "* content [the text of the line-transcription]\n",
    "\n",
    "For geeks also the graphs are very interesting as they contain the geometrical information for each automatically transcribed letter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjtwsYE5s_Nh"
   },
   "source": [
    "if you want to filter these transcriptions and just show the transcriptions of a specific level you can either do it yourself or use a function I already created for that, but you first need to get the primary key (ID) of the transcription level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oovpSH2bHk0B"
   },
   "outputs": [],
   "source": [
    "# replace some unicode by some other\n",
    "tr_level_name = 'WHATEVERNAME of a transcription level you have used in your document'\n",
    "doc_pks = [XXX] # Set XXX to the doc_pk of your document\n",
    "replacements = [('׳',\"'\"),('’',\"'\")] # first value in each tuple is the unicode to be replaced. the second is the replacement.\n",
    "\n",
    "for doc_pk in doc_pks:\n",
    "  master_tr_level = get_transcription_level_pk_by_name(doc_pk,tr_level_name)\n",
    "  parts = get_all_parts(doc_pk)\n",
    "  for i,part in enumerate(parts):\n",
    "    part_pk = part['pk']\n",
    "    print(part_pk,end = ',')\n",
    "    if i % 10 == 0:\n",
    "      print()\n",
    "    trs = get_page_transcription(doc_pk,part_pk,master_tr_level)\n",
    "    updated_trs = []\n",
    "    for tr in trs:\n",
    "      for replacement in replacements:\n",
    "        if replacement[0] in tr['content']:\n",
    "          tr['content'] = tr['content'].replace(replacement[0],replacement[1])\n",
    "          updated_trs.append(tr)\n",
    "    if updated_trs != []:\n",
    "\n",
    "      r = bulk_update_transcriptions(doc_pk,part_pk,updated_trs)\n",
    "      print('trs updated',len(updated_trs),r)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
